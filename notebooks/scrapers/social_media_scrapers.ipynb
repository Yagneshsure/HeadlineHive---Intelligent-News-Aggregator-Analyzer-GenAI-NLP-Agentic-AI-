{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed2954c",
   "metadata": {},
   "source": [
    "# Twitter scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eb5bc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22AI%20OR%20Artificial%20Intelligence%20since%3A2025-08-15%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D: blocked (404)\n",
      "4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22AI%20OR%20Artificial%20Intelligence%20since%3A2025-08-15%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up.\n",
      "Errors: blocked (404), blocked (404), blocked (404), blocked (404)\n"
     ]
    },
    {
     "ename": "ScraperException",
     "evalue": "4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22AI%20OR%20Artificial%20Intelligence%20since%3A2025-08-15%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mScraperException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(tweets)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m df_twitter \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_twitter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAI OR Artificial Intelligence since:2025-08-15\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_twitter\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m, in \u001b[0;36mscrape_twitter\u001b[1;34m(query, limit)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mscrape_twitter\u001b[39m(query, limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m      5\u001b[0m     tweets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, tweet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sntwitter\u001b[38;5;241m.\u001b[39mTwitterSearchScraper(query)\u001b[38;5;241m.\u001b[39mget_items()):\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m limit:\n\u001b[0;32m      8\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\snscrape\\modules\\twitter.py:1763\u001b[0m, in \u001b[0;36mTwitterSearchScraper.get_items\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1760\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m'\u001b[39m: variables, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m: features}\n\u001b[0;32m   1761\u001b[0m paginationParams \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m'\u001b[39m: paginationVariables, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m: features}\n\u001b[1;32m-> 1763\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_api_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline\u001b[39m\u001b[38;5;124m'\u001b[39m, _TwitterAPIType\u001b[38;5;241m.\u001b[39mGRAPHQL, params, paginationParams, cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor, instructionsPath \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_by_raw_query\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_timeline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m   1764\u001b[0m \t\u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graphql_timeline_instructions_to_tweets(obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_by_raw_query\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_timeline\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeline\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\snscrape\\modules\\twitter.py:915\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._iter_api_data\u001b[1;34m(self, endpoint, apiType, params, paginationParams, cursor, direction, instructionsPath)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetrieving scroll page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcursor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 915\u001b[0m \tobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_api_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreqParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    916\u001b[0m \t\u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    918\u001b[0m \t\u001b[38;5;66;03m# No data format test, just a hard and loud crash if anything's wrong :-)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\snscrape\\modules\\twitter.py:886\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._get_api_data\u001b[1;34m(self, endpoint, apiType, params, instructionsPath)\u001b[0m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m apiType \u001b[38;5;129;01mis\u001b[39;00m _TwitterAPIType\u001b[38;5;241m.\u001b[39mGRAPHQL:\n\u001b[0;32m    885\u001b[0m \tparams \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39murlencode({k: json\u001b[38;5;241m.\u001b[39mdumps(v, separators \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems()}, quote_via \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39mquote)\n\u001b[1;32m--> 886\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apiHeaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponseOkCallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfunctools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_api_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39m_snscrapeObj\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\snscrape\\base.py:275\u001b[0m, in \u001b[0;36mScraper._get\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 275\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\snscrape\\base.py:271\u001b[0m, in \u001b[0;36mScraper._request\u001b[1;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[0m\n\u001b[0;32m    269\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39mfatal(msg)\n\u001b[0;32m    270\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39mfatal(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mErrors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(errors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 271\u001b[0m \t\u001b[38;5;28;01mraise\u001b[39;00m ScraperException(msg)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReached unreachable code\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mScraperException\u001b[0m: 4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22AI%20OR%20Artificial%20Intelligence%20since%3A2025-08-15%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up."
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_twitter(query, limit=50):\n",
    "    tweets = []\n",
    "    for i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
    "        if i >= limit:\n",
    "            break\n",
    "        tweets.append({\n",
    "            \"date\": tweet.date,\n",
    "            \"id\": tweet.id,\n",
    "            \"content\": tweet.content,\n",
    "            \"username\": tweet.user.username,\n",
    "            \"retweets\": tweet.retweetCount,\n",
    "            \"likes\": tweet.likeCount,\n",
    "        })\n",
    "    return pd.DataFrame(tweets)\n",
    "\n",
    "# Example usage\n",
    "df_twitter = scrape_twitter(\"AI OR Artificial Intelligence since:2025-08-15\")\n",
    "print(df_twitter.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82987048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snscrape"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached snscrape-0.7.0.20230622-py3-none-any.whl (74 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Collecting lxml\n",
      "  Using cached lxml-6.0.0-cp310-cp310-win_amd64.whl (4.0 MB)\n",
      "Collecting requests[socks]\n",
      "  Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Collecting typing-extensions>=4.0.0\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Using cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.4.3-cp310-cp310-win_amd64.whl (107 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, soupsieve, PySocks, lxml, idna, filelock, charset_normalizer, certifi, requests, beautifulsoup4, snscrape\n",
      "Successfully installed PySocks-1.7.1 beautifulsoup4-4.13.4 certifi-2025.8.3 charset_normalizer-3.4.3 filelock-3.19.1 idna-3.10 lxml-6.0.0 requests-2.32.5 snscrape-0.7.0.20230622 soupsieve-2.7 typing-extensions-4.14.1 urllib3-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install snscrape\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe0843b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FileFinder' object has no attribute 'find_module'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msnscrape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtwitter\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msntwitter\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([tweet\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m sntwitter\u001b[38;5;241m.\u001b[39mTwitterSearchScraper(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mget_items()])\n",
      "File \u001b[1;32mc:\\Users\\yagne\\miniconda3\\Lib\\site-packages\\snscrape\\modules\\__init__.py:17\u001b[0m\n\u001b[0;32m     13\u001b[0m \t\tmodule \u001b[38;5;241m=\u001b[39m importer\u001b[38;5;241m.\u001b[39mfind_module(moduleName)\u001b[38;5;241m.\u001b[39mload_module(moduleName)\n\u001b[0;32m     14\u001b[0m \t\t\u001b[38;5;28mglobals\u001b[39m()[moduleNameWithoutPrefix] \u001b[38;5;241m=\u001b[39m module\n\u001b[1;32m---> 17\u001b[0m \u001b[43m_import_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yagne\\miniconda3\\Lib\\site-packages\\snscrape\\modules\\__init__.py:13\u001b[0m, in \u001b[0;36m_import_modules\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m moduleNameWithoutPrefix \u001b[38;5;241m=\u001b[39m moduleName[prefixLen:]\n\u001b[0;32m     12\u001b[0m __all__\u001b[38;5;241m.\u001b[39mappend(moduleNameWithoutPrefix)\n\u001b[1;32m---> 13\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimporter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_module\u001b[49m(moduleName)\u001b[38;5;241m.\u001b[39mload_module(moduleName)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mglobals\u001b[39m()[moduleNameWithoutPrefix] \u001b[38;5;241m=\u001b[39m module\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FileFinder' object has no attribute 'find_module'"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([tweet.content for tweet in sntwitter.TwitterSearchScraper(\"AI\").get_items()])\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef307b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89be22de",
   "metadata": {},
   "source": [
    "# Reddit Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d0e87c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Politics from r/politics...\n",
      "Scraping Economy from r/economics...\n",
      "Scraping Health from r/health...\n",
      "Scraping Sport from r/sports...\n",
      "Scraping Technology from r/technology...\n",
      "Scraping Entertainment from r/movies...\n",
      "Scraping Society from r/worldnews...\n",
      "Saved scraping results to reddit_data.csv\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"xeV3sLrkl4xGyM-ScCBkSA\",\n",
    "    client_secret=\"sGrJQoAWabMp3hVcAXTxocV3gkCXoQ\",\n",
    "    user_agent=\"HeadlineHive\"\n",
    ")\n",
    "\n",
    "def scrape_reddit(subreddit, limit=100):\n",
    "    posts = []\n",
    "    for post in reddit.subreddit(subreddit).hot(limit=limit):\n",
    "        posts.append({\n",
    "            \"title\": post.title,\n",
    "            \"text\": post.selftext,\n",
    "            \"score\": post.score,\n",
    "            \"subreddit\": subreddit\n",
    "        })\n",
    "    return pd.DataFrame(posts)\n",
    "\n",
    "subs = {\n",
    "    \"Politics\": \"politics\",\n",
    "    \"Economy\": \"economics\",\n",
    "    \"Health\": \"health\",\n",
    "    \"Sport\": \"sports\",\n",
    "    \"Technology\": \"technology\",\n",
    "    \"Entertainment\": \"movies\",\n",
    "    \"Society\": \"worldnews\"\n",
    "}\n",
    "\n",
    "all_reddit = pd.DataFrame()\n",
    "for cat, sub in subs.items():\n",
    "    print(f\"Scraping {cat} from r/{sub}...\")\n",
    "    df = scrape_reddit(sub, limit=200)\n",
    "    df[\"category\"] = cat\n",
    "    all_reddit = pd.concat([all_reddit, df], ignore_index=True)\n",
    "\n",
    "all_reddit.to_csv(\"reddit_data.csv\", index=False)\n",
    "print(\"Saved scraping results to reddit_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc3073b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Reddit data collection with multiple methods...\n",
      "\n",
      "==================================================\n",
      "🔍 Method 1: Searching r/politics for 'politics'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 100 posts...\n",
      "Collected 200 posts...\n",
      "✅ Method 1 Success: 234 posts\n",
      "\n",
      "==================================================\n",
      "🔥 Method 2: Getting ['hot', 'new', 'top'] posts from r/politics...\n",
      "Fetching hot posts...\n",
      "Got 200 hot posts\n",
      "Fetching new posts...\n",
      "Got 200 new posts\n",
      "Fetching top posts...\n",
      "Got 200 top posts\n",
      "✅ Method 2 Success: 600 posts\n",
      "After keyword filtering: 44 posts\n",
      "\n",
      "==================================================\n",
      "📡 Method 3: Using Reddit JSON API for r/politics...\n",
      "Collected 25 posts via JSON API...\n",
      "Collected 50 posts via JSON API...\n",
      "Collected 75 posts via JSON API...\n",
      "Collected 100 posts via JSON API...\n",
      "Collected 125 posts via JSON API...\n",
      "Collected 150 posts via JSON API...\n",
      "Collected 175 posts via JSON API...\n",
      "Collected 200 posts via JSON API...\n",
      "✅ Method 3 Success: 200 posts\n",
      "\n",
      "==================================================\n",
      "🎯 Method 4: Collecting from multiple subreddits...\n",
      "Processing r/politics...\n",
      "🔍 Method 1: Searching r/politics for 'politics'...\n",
      "Collected 100 posts...\n",
      "Collected 200 posts...\n",
      "Got 200 posts from r/politics\n",
      "Processing r/PoliticalDiscussion...\n",
      "🔍 Method 1: Searching r/PoliticalDiscussion for 'politics'...\n",
      "Collected 100 posts...\n",
      "Collected 200 posts...\n",
      "Got 200 posts from r/PoliticalDiscussion\n",
      "Processing r/neutralpolitics...\n",
      "🔍 Method 1: Searching r/neutralpolitics for 'politics'...\n",
      "Collected 100 posts...\n",
      "Collected 200 posts...\n",
      "Got 200 posts from r/neutralpolitics\n",
      "Processing r/Ask_Politics...\n",
      "🔍 Method 1: Searching r/Ask_Politics for 'politics'...\n",
      "Collected 100 posts...\n",
      "Collected 200 posts...\n",
      "Got 200 posts from r/Ask_Politics\n",
      "✅ Method 4 Success: 800 posts from multiple subreddits\n",
      "\n",
      "🎉 FINAL RESULT: 1051 unique posts collected!\n",
      "Date range: 2012-06-01 05:28:02 to 2025-08-21 12:06:17\n",
      "💾 Data saved to: reddit_politics_data_20250821_174542.csv\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas.io.formats.string'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 275\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;66;03m# Show sample\u001b[39;00m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSample data:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 275\u001b[0m     \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauthor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcreated_utc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m❌ All methods failed. This might be due to:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\pandas\\core\\frame.py:1214\u001b[0m, in \u001b[0;36m__repr__\u001b[1;34m(self)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\pandas\\core\\frame.py:1394\u001b[0m, in \u001b[0;36mto_string\u001b[1;34m(self, buf, columns, col_space, header, index, na_rep, formatters, float_format, sparsify, index_names, justify, max_rows, max_cols, show_dimensions, decimal, line_width, min_rows, max_colwidth, encoding)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\pandas\\io\\formats\\format.py:959\u001b[0m, in \u001b[0;36mto_string\u001b[1;34m(self, buf, encoding, line_width)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas.io.formats.string'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import praw\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"xeV3sLrkl4xGyM-ScCBkSA\",\n",
    "    client_secret=\"sGrJQoAWabMp3hVcAXTxocV3gkCXoQ\",\n",
    "    user_agent=\"HeadlineHive\"\n",
    ")\n",
    "\n",
    "def method_1_praw_search(subreddit_name, query, limit=1000, time_filter='all'):\n",
    "    \"\"\"\n",
    "    Method 1: PRAW Search (Most Reliable)\n",
    "    Uses Reddit's official API through PRAW\n",
    "    \"\"\"\n",
    "    print(f\"🔍 Method 1: Searching r/{subreddit_name} for '{query}'...\")\n",
    "    \n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "    \n",
    "    try:\n",
    "        # Search posts\n",
    "        for submission in subreddit.search(query, limit=limit, time_filter=time_filter):\n",
    "            posts.append({\n",
    "                \"id\": submission.id,\n",
    "                \"title\": submission.title,\n",
    "                \"selftext\": submission.selftext,\n",
    "                \"author\": str(submission.author) if submission.author else \"[deleted]\",\n",
    "                \"subreddit\": submission.subreddit.display_name,\n",
    "                \"created_utc\": pd.to_datetime(submission.created_utc, unit='s'),\n",
    "                \"url\": submission.url,\n",
    "                \"score\": submission.score,\n",
    "                \"num_comments\": submission.num_comments,\n",
    "                \"upvote_ratio\": submission.upvote_ratio,\n",
    "                \"is_self\": submission.is_self,\n",
    "                \"permalink\": f\"https://reddit.com{submission.permalink}\"\n",
    "            })\n",
    "            \n",
    "            if len(posts) % 100 == 0:\n",
    "                print(f\"Collected {len(posts)} posts...\")\n",
    "                time.sleep(0.1)  # Be gentle with API\n",
    "                \n",
    "        return pd.DataFrame(posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"PRAW search error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def method_2_praw_hot_new_top(subreddit_name, categories=['hot', 'new', 'top'], limit_per_category=500):\n",
    "    \"\"\"\n",
    "    Method 2: PRAW Hot/New/Top Posts\n",
    "    Gets recent popular posts from specific categories\n",
    "    \"\"\"\n",
    "    print(f\"🔥 Method 2: Getting {categories} posts from r/{subreddit_name}...\")\n",
    "    \n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    all_posts = []\n",
    "    \n",
    "    for category in categories:\n",
    "        print(f\"Fetching {category} posts...\")\n",
    "        posts = []\n",
    "        \n",
    "        try:\n",
    "            if category == 'hot':\n",
    "                submissions = subreddit.hot(limit=limit_per_category)\n",
    "            elif category == 'new':\n",
    "                submissions = subreddit.new(limit=limit_per_category)\n",
    "            elif category == 'top':\n",
    "                submissions = subreddit.top(time_filter='month', limit=limit_per_category)\n",
    "            elif category == 'rising':\n",
    "                submissions = subreddit.rising(limit=limit_per_category)\n",
    "            \n",
    "            for submission in submissions:\n",
    "                posts.append({\n",
    "                    \"id\": submission.id,\n",
    "                    \"title\": submission.title,\n",
    "                    \"selftext\": submission.selftext,\n",
    "                    \"author\": str(submission.author) if submission.author else \"[deleted]\",\n",
    "                    \"subreddit\": submission.subreddit.display_name,\n",
    "                    \"created_utc\": pd.to_datetime(submission.created_utc, unit='s'),\n",
    "                    \"url\": submission.url,\n",
    "                    \"score\": submission.score,\n",
    "                    \"num_comments\": submission.num_comments,\n",
    "                    \"upvote_ratio\": submission.upvote_ratio,\n",
    "                    \"category\": category,\n",
    "                    \"permalink\": f\"https://reddit.com{submission.permalink}\"\n",
    "                })\n",
    "                \n",
    "            print(f\"Got {len(posts)} {category} posts\")\n",
    "            all_posts.extend(posts)\n",
    "            time.sleep(1)  # Rate limiting\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {category} posts: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(all_posts)\n",
    "\n",
    "def method_3_reddit_json_api(subreddit_name, category='hot', limit=100):\n",
    "    \"\"\"\n",
    "    Method 3: Direct Reddit JSON API\n",
    "    Uses Reddit's JSON endpoints (no authentication required)\n",
    "    \"\"\"\n",
    "    print(f\"📡 Method 3: Using Reddit JSON API for r/{subreddit_name}...\")\n",
    "    \n",
    "    base_url = f\"https://www.reddit.com/r/{subreddit_name}/{category}.json\"\n",
    "    posts = []\n",
    "    after = None\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'HeadlineHive/1.0'\n",
    "    }\n",
    "    \n",
    "    while len(posts) < limit:\n",
    "        params = {'limit': 25}\n",
    "        if after:\n",
    "            params['after'] = after\n",
    "            \n",
    "        try:\n",
    "            response = requests.get(base_url, params=params, headers=headers)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                children = data['data']['children']\n",
    "                \n",
    "                if not children:\n",
    "                    break\n",
    "                \n",
    "                for child in children:\n",
    "                    post = child['data']\n",
    "                    posts.append({\n",
    "                        \"id\": post.get('id'),\n",
    "                        \"title\": post.get('title'),\n",
    "                        \"selftext\": post.get('selftext'),\n",
    "                        \"author\": post.get('author'),\n",
    "                        \"subreddit\": post.get('subreddit'),\n",
    "                        \"created_utc\": pd.to_datetime(post.get('created_utc'), unit='s'),\n",
    "                        \"url\": post.get('url'),\n",
    "                        \"score\": post.get('score'),\n",
    "                        \"num_comments\": post.get('num_comments'),\n",
    "                        \"upvote_ratio\": post.get('upvote_ratio'),\n",
    "                        \"permalink\": f\"https://reddit.com{post.get('permalink')}\"\n",
    "                    })\n",
    "                \n",
    "                after = data['data']['after']\n",
    "                print(f\"Collected {len(posts)} posts via JSON API...\")\n",
    "                \n",
    "                if not after:  # No more pages\n",
    "                    break\n",
    "                    \n",
    "                time.sleep(1)  # Rate limiting\n",
    "                \n",
    "            else:\n",
    "                print(f\"HTTP Error: {response.status_code}\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"JSON API error: {e}\")\n",
    "            break\n",
    "    \n",
    "    return pd.DataFrame(posts[:limit])\n",
    "\n",
    "def method_4_multiple_subreddits(subreddit_list, query, method='search'):\n",
    "    \"\"\"\n",
    "    Method 4: Collect from multiple subreddits\n",
    "    \"\"\"\n",
    "    print(f\"🎯 Method 4: Collecting from multiple subreddits...\")\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for sub in subreddit_list:\n",
    "        print(f\"Processing r/{sub}...\")\n",
    "        try:\n",
    "            if method == 'search':\n",
    "                df = method_1_praw_search(sub, query, limit=200)\n",
    "            elif method == 'hot':\n",
    "                df = method_2_praw_hot_new_top(sub, ['hot'], limit_per_category=200)\n",
    "                \n",
    "            if not df.empty:\n",
    "                all_data.append(df)\n",
    "                print(f\"Got {len(df)} posts from r/{sub}\")\n",
    "            \n",
    "            time.sleep(2)  # Be respectful between subreddits\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with r/{sub}: {e}\")\n",
    "    \n",
    "    if all_data:\n",
    "        return pd.concat(all_data, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def filter_by_keywords(df, keywords, column='title'):\n",
    "    \"\"\"\n",
    "    Filter dataframe by keywords in title or text\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    pattern = '|'.join(keywords)\n",
    "    mask = df[column].str.contains(pattern, case=False, na=False)\n",
    "    \n",
    "    if 'selftext' in df.columns:\n",
    "        selftext_mask = df['selftext'].str.contains(pattern, case=False, na=False)\n",
    "        mask = mask | selftext_mask\n",
    "    \n",
    "    return df[mask]\n",
    "\n",
    "# Example usage with multiple fallback methods\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Starting Reddit data collection with multiple methods...\\n\")\n",
    "    \n",
    "    # Try Method 1: PRAW Search (most reliable for keyword-based collection)\n",
    "    print(\"=\" * 50)\n",
    "    df1 = method_1_praw_search('politics', 'politics', limit=500)\n",
    "    \n",
    "    if not df1.empty:\n",
    "        print(f\"✅ Method 1 Success: {len(df1)} posts\")\n",
    "    else:\n",
    "        print(\"❌ Method 1 failed\")\n",
    "    \n",
    "    # Try Method 2: Get hot/new/top posts from politics subreddit\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    df2 = method_2_praw_hot_new_top('politics', ['hot', 'new', 'top'], limit_per_category=200)\n",
    "    \n",
    "    if not df2.empty:\n",
    "        print(f\"✅ Method 2 Success: {len(df2)} posts\")\n",
    "        # Filter for politics-related content\n",
    "        df2_filtered = filter_by_keywords(df2, ['politics', 'political', 'election', 'government', 'congress'])\n",
    "        print(f\"After keyword filtering: {len(df2_filtered)} posts\")\n",
    "    else:\n",
    "        print(\"❌ Method 2 failed\")\n",
    "    \n",
    "    # Try Method 3: Reddit JSON API\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    df3 = method_3_reddit_json_api('politics', 'hot', limit=200)\n",
    "    \n",
    "    if not df3.empty:\n",
    "        print(f\"✅ Method 3 Success: {len(df3)} posts\")\n",
    "    else:\n",
    "        print(\"❌ Method 3 failed\")\n",
    "    \n",
    "    # Try Method 4: Multiple subreddits\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    political_subs = ['politics', 'PoliticalDiscussion', 'neutralpolitics', 'Ask_Politics']\n",
    "    df4 = method_4_multiple_subreddits(political_subs, 'politics', method='search')\n",
    "    \n",
    "    if not df4.empty:\n",
    "        print(f\"✅ Method 4 Success: {len(df4)} posts from multiple subreddits\")\n",
    "    else:\n",
    "        print(\"❌ Method 4 failed\")\n",
    "    \n",
    "    # Combine all successful methods\n",
    "    successful_dfs = [df for df in [df1, df2_filtered if 'df2_filtered' in locals() else df2, df3, df4] if not df.empty]\n",
    "    \n",
    "    if successful_dfs:\n",
    "        final_df = pd.concat(successful_dfs, ignore_index=True)\n",
    "        # Remove duplicates based on post ID\n",
    "        final_df = final_df.drop_duplicates(subset=['id'], keep='first')\n",
    "        \n",
    "        print(f\"\\n🎉 FINAL RESULT: {len(final_df)} unique posts collected!\")\n",
    "        print(f\"Date range: {final_df['created_utc'].min()} to {final_df['created_utc'].max()}\")\n",
    "        \n",
    "        # Save results\n",
    "        output_file = f\"reddit_politics_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        final_df.to_csv(output_file, index=False)\n",
    "        print(f\"💾 Data saved to: {output_file}\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(\"\\nSample data:\")\n",
    "        print(final_df[['title', 'author', 'score', 'created_utc']].head())\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n❌ All methods failed. This might be due to:\")\n",
    "        print(\"- Reddit API rate limiting\")\n",
    "        print(\"- Network connectivity issues\") \n",
    "        print(\"- Reddit credentials issues\")\n",
    "        print(\"- Subreddit restrictions\")\n",
    "        \n",
    "        # Show troubleshooting steps\n",
    "        print(\"\\n🔧 Troubleshooting steps:\")\n",
    "        print(\"1. Check your Reddit API credentials\")\n",
    "        print(\"2. Try again in a few minutes (rate limiting)\")\n",
    "        print(\"3. Test with a smaller limit\")\n",
    "        print(\"4. Check if the subreddit exists and is public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8182e1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Simple Reddit Data Collection\n",
      "==================================================\n",
      "Testing Reddit connection...\n",
      "✅ Connected! Test subreddit: python\n",
      "\n",
      "📍 Method 1: Searching for politics posts...\n",
      "🔍 Collecting posts from r/politics...\n",
      "Searching for: 'politics'\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "Collected 150 posts...\n",
      "Collected 200 posts...\n",
      "✅ Collected 234 politics posts\n",
      "✅ Data saved to reddit_politics_20250821_174915.csv\n",
      "✅ JSON data saved to reddit_politics_20250821_174915.json\n",
      "\n",
      "📋 Sample of 3 posts:\n",
      "============================================================\n",
      "Post 1:\n",
      "  Title: DOJ Opens Door To Stripping Citizenship Over Politics...\n",
      "  Author: marji80\n",
      "  Score: 33598\n",
      "  Comments: 3300\n",
      "  Created: 2025-07-02 19:42:01\n",
      "  URL: https://talkingpointsmemo.com/news/doj-opens-door-to-strippi...\n",
      "----------------------------------------\n",
      "Post 2:\n",
      "  Title: Everybody Hates Trump Now | Six months after Trump bragged about a “historic” re...\n",
      "  Author: Aggravating_Money992\n",
      "  Score: 17373\n",
      "  Comments: 1238\n",
      "  Created: 2025-07-31 16:41:20\n",
      "  URL: https://newrepublic.com/article/198624/everybody-hates-trump...\n",
      "----------------------------------------\n",
      "Post 3:\n",
      "  Title: Tim Walz: Trump Will Start Arresting Political Opponents...\n",
      "  Author: thedailybeast\n",
      "  Score: 58945\n",
      "  Comments: 4112\n",
      "  Created: 2025-03-20 20:30:01\n",
      "  URL: https://www.thedailybeast.com/tim-walz-trump-will-start-arre...\n",
      "----------------------------------------\n",
      "\n",
      "📊 Data Summary:\n",
      "Total posts: 234\n",
      "Average score: 22926.1\n",
      "Max score: 129734\n",
      "Unique authors: 172\n",
      "\n",
      "📍 Method 2: Trying other political subreddits...\n",
      "Trying r/PoliticalDiscussion...\n",
      "🔍 Collecting posts from r/PoliticalDiscussion...\n",
      "Getting hot posts...\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "✅ Added 100 posts from r/PoliticalDiscussion\n",
      "Trying r/Ask_Politics...\n",
      "🔍 Collecting posts from r/Ask_Politics...\n",
      "Getting hot posts...\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "✅ Added 100 posts from r/Ask_Politics\n",
      "Trying r/neutralpolitics...\n",
      "🔍 Collecting posts from r/neutralpolitics...\n",
      "Getting hot posts...\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "✅ Added 100 posts from r/neutralpolitics\n",
      "\n",
      "🎉 FINAL TOTAL: 534 posts collected!\n",
      "After removing duplicates: 534 unique posts\n",
      "✅ Data saved to reddit_political_data_final_20250821_174930.csv\n",
      "✅ JSON data saved to reddit_political_data_final_20250821_174930.json\n",
      "\n",
      "🎯 Data collection complete!\n",
      "CSV file: reddit_political_data_final_20250821_174930.csv\n",
      "JSON file: reddit_political_data_final_20250821_174930.json\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"xeV3sLrkl4xGyM-ScCBkSA\",\n",
    "    client_secret=\"sGrJQoAWabMp3hVcAXTxocV3gkCXoQ\",\n",
    "    user_agent=\"HeadlineHive\"\n",
    ")\n",
    "\n",
    "def collect_reddit_posts_simple(subreddit_name, search_query=None, limit=500):\n",
    "    \"\"\"\n",
    "    Simple Reddit data collection without pandas dependencies\n",
    "    \"\"\"\n",
    "    print(f\"🔍 Collecting posts from r/{subreddit_name}...\")\n",
    "    \n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "    \n",
    "    try:\n",
    "        if search_query:\n",
    "            print(f\"Searching for: '{search_query}'\")\n",
    "            submissions = subreddit.search(search_query, limit=limit)\n",
    "        else:\n",
    "            print(\"Getting hot posts...\")\n",
    "            submissions = subreddit.hot(limit=limit)\n",
    "        \n",
    "        for i, submission in enumerate(submissions):\n",
    "            try:\n",
    "                post_data = {\n",
    "                    'id': submission.id,\n",
    "                    'title': submission.title,\n",
    "                    'text': submission.selftext,\n",
    "                    'author': str(submission.author) if submission.author else '[deleted]',\n",
    "                    'subreddit': submission.subreddit.display_name,\n",
    "                    'score': submission.score,\n",
    "                    'comments': submission.num_comments,\n",
    "                    'url': submission.url,\n",
    "                    'created': datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'permalink': f\"https://reddit.com{submission.permalink}\"\n",
    "                }\n",
    "                posts.append(post_data)\n",
    "                \n",
    "                if (i + 1) % 50 == 0:\n",
    "                    print(f\"Collected {i + 1} posts...\")\n",
    "                    time.sleep(0.5)  # Be nice to the API\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing post {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return posts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting posts: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_to_csv(posts, filename):\n",
    "    \"\"\"Save posts to CSV file\"\"\"\n",
    "    if not posts:\n",
    "        print(\"No posts to save!\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        fieldnames = posts[0].keys()\n",
    "        \n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for post in posts:\n",
    "                # Clean text fields to avoid CSV issues\n",
    "                clean_post = {}\n",
    "                for key, value in post.items():\n",
    "                    if isinstance(value, str):\n",
    "                        # Remove problematic characters\n",
    "                        clean_post[key] = value.replace('\\n', ' ').replace('\\r', ' ')[:1000]\n",
    "                    else:\n",
    "                        clean_post[key] = value\n",
    "                writer.writerow(clean_post)\n",
    "        \n",
    "        print(f\"✅ Data saved to {filename}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving CSV: {e}\")\n",
    "        return False\n",
    "\n",
    "def save_to_json(posts, filename):\n",
    "    \"\"\"Save posts to JSON file\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(posts, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"✅ JSON data saved to {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving JSON: {e}\")\n",
    "        return False\n",
    "\n",
    "def display_sample(posts, num_samples=3):\n",
    "    \"\"\"Display sample posts without pandas\"\"\"\n",
    "    if not posts:\n",
    "        print(\"No posts to display\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n📋 Sample of {min(num_samples, len(posts))} posts:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, post in enumerate(posts[:num_samples]):\n",
    "        print(f\"Post {i+1}:\")\n",
    "        print(f\"  Title: {post['title'][:80]}...\")\n",
    "        print(f\"  Author: {post['author']}\")\n",
    "        print(f\"  Score: {post['score']}\")\n",
    "        print(f\"  Comments: {post['comments']}\")\n",
    "        print(f\"  Created: {post['created']}\")\n",
    "        print(f\"  URL: {post['url'][:60]}...\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Simple Reddit Data Collection\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test Reddit connection\n",
    "    try:\n",
    "        print(\"Testing Reddit connection...\")\n",
    "        test_sub = reddit.subreddit('python')\n",
    "        print(f\"✅ Connected! Test subreddit: {test_sub.display_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Reddit connection failed: {e}\")\n",
    "        print(\"Check your Reddit API credentials!\")\n",
    "        exit()\n",
    "    \n",
    "    # Method 1: Search for politics posts\n",
    "    print(\"\\n📍 Method 1: Searching for politics posts...\")\n",
    "    politics_posts = collect_reddit_posts_simple('politics', 'politics', limit=300)\n",
    "    \n",
    "    if politics_posts:\n",
    "        print(f\"✅ Collected {len(politics_posts)} politics posts\")\n",
    "        \n",
    "        # Save data\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        csv_file = f\"reddit_politics_{timestamp}.csv\"\n",
    "        json_file = f\"reddit_politics_{timestamp}.json\"\n",
    "        \n",
    "        save_to_csv(politics_posts, csv_file)\n",
    "        save_to_json(politics_posts, json_file)\n",
    "        \n",
    "        # Display sample\n",
    "        display_sample(politics_posts)\n",
    "        \n",
    "        # Basic stats\n",
    "        print(f\"\\n📊 Data Summary:\")\n",
    "        print(f\"Total posts: {len(politics_posts)}\")\n",
    "        \n",
    "        scores = [post['score'] for post in politics_posts]\n",
    "        if scores:\n",
    "            print(f\"Average score: {sum(scores) / len(scores):.1f}\")\n",
    "            print(f\"Max score: {max(scores)}\")\n",
    "        \n",
    "        authors = [post['author'] for post in politics_posts if post['author'] != '[deleted]']\n",
    "        unique_authors = len(set(authors))\n",
    "        print(f\"Unique authors: {unique_authors}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ Failed to collect politics posts\")\n",
    "    \n",
    "    # Method 2: Try other political subreddits\n",
    "    print(\"\\n📍 Method 2: Trying other political subreddits...\")\n",
    "    other_subs = ['PoliticalDiscussion', 'Ask_Politics', 'neutralpolitics']\n",
    "    all_posts = politics_posts.copy() if politics_posts else []\n",
    "    \n",
    "    for sub in other_subs:\n",
    "        print(f\"Trying r/{sub}...\")\n",
    "        try:\n",
    "            sub_posts = collect_reddit_posts_simple(sub, None, limit=100)\n",
    "            if sub_posts:\n",
    "                all_posts.extend(sub_posts)\n",
    "                print(f\"✅ Added {len(sub_posts)} posts from r/{sub}\")\n",
    "            time.sleep(2)  # Rate limiting\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed r/{sub}: {e}\")\n",
    "    \n",
    "    # Final results\n",
    "    if all_posts:\n",
    "        print(f\"\\n🎉 FINAL TOTAL: {len(all_posts)} posts collected!\")\n",
    "        \n",
    "        # Remove duplicates manually\n",
    "        seen_ids = set()\n",
    "        unique_posts = []\n",
    "        for post in all_posts:\n",
    "            if post['id'] not in seen_ids:\n",
    "                unique_posts.append(post)\n",
    "                seen_ids.add(post['id'])\n",
    "        \n",
    "        print(f\"After removing duplicates: {len(unique_posts)} unique posts\")\n",
    "        \n",
    "        # Save final dataset\n",
    "        final_csv = f\"reddit_political_data_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        final_json = f\"reddit_political_data_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        \n",
    "        save_to_csv(unique_posts, final_csv)\n",
    "        save_to_json(unique_posts, final_json)\n",
    "        \n",
    "        print(\"\\n🎯 Data collection complete!\")\n",
    "        print(f\"CSV file: {final_csv}\")\n",
    "        print(f\"JSON file: {final_json}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n❌ No data collected. Possible issues:\")\n",
    "        print(\"- Reddit API credentials invalid\")\n",
    "        print(\"- Network connectivity problems\")\n",
    "        print(\"- Rate limiting (try again later)\")\n",
    "        print(\"- Subreddit access restrictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "853c1956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Simple Reddit Data Collection\n",
      "==================================================\n",
      "Testing Reddit connection...\n",
      "✅ Connected! Test subreddit: python\n",
      "\n",
      "📍 Method 1: Searching for politics posts...\n",
      "🔍 Collecting posts from r/politics...\n",
      "Searching for: 'politics'\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "Collected 150 posts...\n",
      "Collected 200 posts...\n",
      "✅ Collected 234 politics posts\n",
      "✅ JSON data saved to reddit_politics_20250821_180555.json\n",
      "\n",
      "📋 Sample of 3 posts:\n",
      "============================================================\n",
      "Post 1:\n",
      "  Title: DOJ Opens Door To Stripping Citizenship Over Politics...\n",
      "  Author: marji80\n",
      "  Score: 33606\n",
      "  Comments: 3300\n",
      "  Created: 2025-07-02 19:42:01\n",
      "  URL: https://talkingpointsmemo.com/news/doj-opens-door-to-strippi...\n",
      "----------------------------------------\n",
      "Post 2:\n",
      "  Title: Everybody Hates Trump Now | Six months after Trump bragged about a “historic” re...\n",
      "  Author: Aggravating_Money992\n",
      "  Score: 17373\n",
      "  Comments: 1238\n",
      "  Created: 2025-07-31 16:41:20\n",
      "  URL: https://newrepublic.com/article/198624/everybody-hates-trump...\n",
      "----------------------------------------\n",
      "Post 3:\n",
      "  Title: Tim Walz: Trump Will Start Arresting Political Opponents...\n",
      "  Author: thedailybeast\n",
      "  Score: 58943\n",
      "  Comments: 4112\n",
      "  Created: 2025-03-20 20:30:01\n",
      "  URL: https://www.thedailybeast.com/tim-walz-trump-will-start-arre...\n",
      "----------------------------------------\n",
      "\n",
      "📊 Data Summary:\n",
      "Total posts: 234\n",
      "Average score: 22925.9\n",
      "Max score: 129732\n",
      "Unique authors: 172\n",
      "\n",
      "📍 Method 2: Trying other political subreddits...\n",
      "Trying r/PoliticalDiscussion...\n",
      "🔍 Collecting posts from r/PoliticalDiscussion...\n",
      "Getting hot posts...\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "✅ Added 100 posts from r/PoliticalDiscussion\n",
      "Trying r/Ask_Politics...\n",
      "🔍 Collecting posts from r/Ask_Politics...\n",
      "Getting hot posts...\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "✅ Added 100 posts from r/Ask_Politics\n",
      "Trying r/neutralpolitics...\n",
      "🔍 Collecting posts from r/neutralpolitics...\n",
      "Getting hot posts...\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "✅ Added 100 posts from r/neutralpolitics\n",
      "\n",
      "🎉 FINAL TOTAL: 534 posts collected!\n",
      "After removing duplicates: 534 unique posts\n",
      "✅ JSON data saved to reddit_political_data_final_20250821_180610.json\n",
      "\n",
      "🎯 Data collection complete!\n",
      "JSON file: reddit_political_data_final_20250821_180610.json\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"xeV3sLrkl4xGyM-ScCBkSA\",\n",
    "    client_secret=\"sGrJQoAWabMp3hVcAXTxocV3gkCXoQ\",\n",
    "    user_agent=\"HeadlineHive\"\n",
    ")\n",
    "\n",
    "def collect_reddit_posts_simple(subreddit_name, search_query=None, limit=500):\n",
    "    \"\"\"\n",
    "    Simple Reddit data collection without pandas dependencies\n",
    "    \"\"\"\n",
    "    print(f\"🔍 Collecting posts from r/{subreddit_name}...\")\n",
    "    \n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "    \n",
    "    try:\n",
    "        if search_query:\n",
    "            print(f\"Searching for: '{search_query}'\")\n",
    "            submissions = subreddit.search(search_query, limit=limit)\n",
    "        else:\n",
    "            print(\"Getting hot posts...\")\n",
    "            submissions = subreddit.hot(limit=limit)\n",
    "        \n",
    "        for i, submission in enumerate(submissions):\n",
    "            try:\n",
    "                post_data = {\n",
    "                    'id': submission.id,\n",
    "                    'title': submission.title,\n",
    "                    'text': submission.selftext,\n",
    "                    'author': str(submission.author) if submission.author else '[deleted]',\n",
    "                    'subreddit': submission.subreddit.display_name,\n",
    "                    'score': submission.score,\n",
    "                    'comments': submission.num_comments,\n",
    "                    'url': submission.url,\n",
    "                    'created': datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'permalink': f\"https://reddit.com{submission.permalink}\"\n",
    "                }\n",
    "                posts.append(post_data)\n",
    "                \n",
    "                if (i + 1) % 50 == 0:\n",
    "                    print(f\"Collected {i + 1} posts...\")\n",
    "                    time.sleep(0.5)  # Be nice to the API\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing post {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return posts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting posts: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_to_csv(posts, filename):\n",
    "    \"\"\"Save posts to CSV file\"\"\"\n",
    "    if not posts:\n",
    "        print(\"No posts to save!\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        fieldnames = posts[0].keys()\n",
    "        \n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for post in posts:\n",
    "                # Clean text fields to avoid CSV issues\n",
    "                clean_post = {}\n",
    "                for key, value in post.items():\n",
    "                    if isinstance(value, str):\n",
    "                        # Remove problematic characters\n",
    "                        clean_post[key] = value.replace('\\n', ' ').replace('\\r', ' ')[:1000]\n",
    "                    else:\n",
    "                        clean_post[key] = value\n",
    "                writer.writerow(clean_post)\n",
    "        \n",
    "        print(f\"✅ Data saved to {filename}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving CSV: {e}\")\n",
    "        return False\n",
    "\n",
    "def save_to_json(posts, filename):\n",
    "    \"\"\"Save posts to JSON file\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(posts, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"✅ JSON data saved to {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving JSON: {e}\")\n",
    "        return False\n",
    "\n",
    "def display_sample(posts, num_samples=3):\n",
    "    \"\"\"Display sample posts without pandas\"\"\"\n",
    "    if not posts:\n",
    "        print(\"No posts to display\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n📋 Sample of {min(num_samples, len(posts))} posts:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, post in enumerate(posts[:num_samples]):\n",
    "        print(f\"Post {i+1}:\")\n",
    "        print(f\"  Title: {post['title'][:80]}...\")\n",
    "        print(f\"  Author: {post['author']}\")\n",
    "        print(f\"  Score: {post['score']}\")\n",
    "        print(f\"  Comments: {post['comments']}\")\n",
    "        print(f\"  Created: {post['created']}\")\n",
    "        print(f\"  URL: {post['url'][:60]}...\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Simple Reddit Data Collection\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test Reddit connection\n",
    "    try:\n",
    "        print(\"Testing Reddit connection...\")\n",
    "        test_sub = reddit.subreddit('python')\n",
    "        print(f\"✅ Connected! Test subreddit: {test_sub.display_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Reddit connection failed: {e}\")\n",
    "        print(\"Check your Reddit API credentials!\")\n",
    "        exit()\n",
    "    \n",
    "    # Method 1: Search for politics posts\n",
    "    print(\"\\n📍 Method 1: Searching for politics posts...\")\n",
    "    politics_posts = collect_reddit_posts_simple('politics', 'politics', limit=300)\n",
    "    \n",
    "    if politics_posts:\n",
    "        print(f\"✅ Collected {len(politics_posts)} politics posts\")\n",
    "        \n",
    "        # Save data (JSON only)\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        json_file = f\"reddit_politics_{timestamp}.json\"\n",
    "        \n",
    "        save_to_json(politics_posts, json_file)\n",
    "        \n",
    "        # Display sample\n",
    "        display_sample(politics_posts)\n",
    "        \n",
    "        # Basic stats\n",
    "        print(f\"\\n📊 Data Summary:\")\n",
    "        print(f\"Total posts: {len(politics_posts)}\")\n",
    "        \n",
    "        scores = [post['score'] for post in politics_posts]\n",
    "        if scores:\n",
    "            print(f\"Average score: {sum(scores) / len(scores):.1f}\")\n",
    "            print(f\"Max score: {max(scores)}\")\n",
    "        \n",
    "        authors = [post['author'] for post in politics_posts if post['author'] != '[deleted]']\n",
    "        unique_authors = len(set(authors))\n",
    "        print(f\"Unique authors: {unique_authors}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ Failed to collect politics posts\")\n",
    "    \n",
    "    # Method 2: Try other political subreddits\n",
    "    print(\"\\n📍 Method 2: Trying other political subreddits...\")\n",
    "    other_subs = ['PoliticalDiscussion', 'Ask_Politics', 'neutralpolitics']\n",
    "    all_posts = politics_posts.copy() if politics_posts else []\n",
    "    \n",
    "    for sub in other_subs:\n",
    "        print(f\"Trying r/{sub}...\")\n",
    "        try:\n",
    "            sub_posts = collect_reddit_posts_simple(sub, None, limit=100)\n",
    "            if sub_posts:\n",
    "                all_posts.extend(sub_posts)\n",
    "                print(f\"✅ Added {len(sub_posts)} posts from r/{sub}\")\n",
    "            time.sleep(2)  # Rate limiting\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed r/{sub}: {e}\")\n",
    "    \n",
    "    # Final results\n",
    "    if all_posts:\n",
    "        print(f\"\\n🎉 FINAL TOTAL: {len(all_posts)} posts collected!\")\n",
    "        \n",
    "        # Remove duplicates manually\n",
    "        seen_ids = set()\n",
    "        unique_posts = []\n",
    "        for post in all_posts:\n",
    "            if post['id'] not in seen_ids:\n",
    "                unique_posts.append(post)\n",
    "                seen_ids.add(post['id'])\n",
    "        \n",
    "        print(f\"After removing duplicates: {len(unique_posts)} unique posts\")\n",
    "        \n",
    "        # Save final dataset (JSON only)\n",
    "        final_json = f\"reddit_political_data_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        \n",
    "        save_to_json(unique_posts, final_json)\n",
    "        \n",
    "        print(\"\\n🎯 Data collection complete!\")\n",
    "        print(f\"JSON file: {final_json}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n❌ No data collected. Possible issues:\")\n",
    "        print(\"- Reddit API credentials invalid\")\n",
    "        print(\"- Network connectivity problems\")\n",
    "        print(\"- Rate limiting (try again later)\")\n",
    "        print(\"- Subreddit access restrictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b78ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Reddit Data Collection - Single JSON Output\n",
      "============================================================\n",
      "🔌 Testing Reddit connection...\n",
      "✅ Connected! Test subreddit: python\n",
      "\n",
      "🎯 Starting comprehensive data collection...\n",
      "\n",
      "📍 Processing r/politics...\n",
      "🔍 Method: SEARCH from r/politics\n",
      "   Query: 'politics'\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "   Processed 125 posts...\n",
      "   Processed 150 posts...\n",
      "   Processed 175 posts...\n",
      "   Processed 200 posts...\n",
      "✅ Collected 200 posts using search\n",
      "   Added 200 posts from search\n",
      "🔍 Method: HOT from r/politics\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "✅ Collected 100 posts using hot\n",
      "   Added 100 posts from hot\n",
      "🔍 Method: NEW from r/politics\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "✅ Collected 100 posts using new\n",
      "   Added 100 posts from new\n",
      "\n",
      "📍 Processing r/PoliticalDiscussion...\n",
      "🔍 Method: SEARCH from r/PoliticalDiscussion\n",
      "   Query: 'politics'\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "   Processed 125 posts...\n",
      "   Processed 150 posts...\n",
      "   Processed 175 posts...\n",
      "   Processed 200 posts...\n",
      "✅ Collected 200 posts using search\n",
      "   Added 200 posts from search\n",
      "🔍 Method: HOT from r/PoliticalDiscussion\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "✅ Collected 100 posts using hot\n",
      "   Added 100 posts from hot\n",
      "🔍 Method: NEW from r/PoliticalDiscussion\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "✅ Collected 100 posts using new\n",
      "   Added 100 posts from new\n",
      "\n",
      "📍 Processing r/Ask_Politics...\n",
      "🔍 Method: SEARCH from r/Ask_Politics\n",
      "   Query: 'politics'\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "   Processed 125 posts...\n",
      "   Processed 150 posts...\n",
      "   Processed 175 posts...\n",
      "   Processed 200 posts...\n",
      "✅ Collected 200 posts using search\n",
      "   Added 200 posts from search\n",
      "🔍 Method: HOT from r/Ask_Politics\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "✅ Collected 100 posts using hot\n",
      "   Added 100 posts from hot\n",
      "🔍 Method: NEW from r/Ask_Politics\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "✅ Collected 100 posts using new\n",
      "   Added 100 posts from new\n",
      "\n",
      "📍 Processing r/neutralpolitics...\n",
      "🔍 Method: SEARCH from r/neutralpolitics\n",
      "   Query: 'politics'\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "   Processed 125 posts...\n",
      "   Processed 150 posts...\n",
      "   Processed 175 posts...\n",
      "   Processed 200 posts...\n",
      "✅ Collected 200 posts using search\n",
      "   Added 200 posts from search\n",
      "🔍 Method: HOT from r/neutralpolitics\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "✅ Collected 100 posts using hot\n",
      "   Added 100 posts from hot\n",
      "🔍 Method: NEW from r/neutralpolitics\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "✅ Collected 100 posts using new\n",
      "   Added 100 posts from new\n",
      "\n",
      "📊 Processing 1600 total posts...\n",
      "After removing duplicates: 1219 unique posts\n",
      "Posts with text content: 868\n",
      "Posts without text content: 351\n",
      "✅ All data saved to: reddit_political_data_complete_20250821_181052.json\n",
      "   Total posts: 1219\n",
      "\n",
      "📋 Sample of 5 posts:\n",
      "================================================================================\n",
      "\n",
      "Post 1:\n",
      "  ID: 1lpwwbr\n",
      "  Title: DOJ Opens Door To Stripping Citizenship Over Politics...\n",
      "  Author: marji80\n",
      "  Subreddit: r/politics\n",
      "  Score: 33601 | Comments: 3300\n",
      "  Method: search\n",
      "  Is Self Post: False\n",
      "  Text: Link post: https://talkingpointsmemo.com/news/doj-opens-door-to-stripping-citizenship-over-politics\n",
      "  Created: 2025-07-02 19:42:01\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Post 2:\n",
      "  ID: 1mdyoyp\n",
      "  Title: Everybody Hates Trump Now | Six months after Trump bragged about a “historic” realignment, voters fr...\n",
      "  Author: Aggravating_Money992\n",
      "  Subreddit: r/politics\n",
      "  Score: 17367 | Comments: 1238\n",
      "  Method: search\n",
      "  Is Self Post: False\n",
      "  Text: Link post: https://newrepublic.com/article/198624/everybody-hates-trump-approval-rating-polls\n",
      "  Created: 2025-07-31 16:41:20\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Post 3:\n",
      "  ID: 1jfqvem\n",
      "  Title: Tim Walz: Trump Will Start Arresting Political Opponents...\n",
      "  Author: thedailybeast\n",
      "  Subreddit: r/politics\n",
      "  Score: 58941 | Comments: 4112\n",
      "  Method: search\n",
      "  Is Self Post: False\n",
      "  Text: Link post: https://www.thedailybeast.com/tim-walz-trump-will-start-arresting-political-opponents/\n",
      "  Created: 2025-03-20 20:30:01\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Post 4:\n",
      "  ID: 1m323ju\n",
      "  Title: Politics, not Performance Killed ‘The Late Show’ with Stephen Colbert...\n",
      "  Author: AlexandrTheTolerable\n",
      "  Subreddit: r/politics\n",
      "  Score: 12306 | Comments: 845\n",
      "  Method: search\n",
      "  Is Self Post: False\n",
      "  Text: Link post: https://www.rollingstone.com/tv-movies/tv-movie-features/late-show-with-stephen-colbert-ending-analysis-1235388587/\n",
      "  Created: 2025-07-18 18:47:27\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Post 5:\n",
      "  ID: 1k4pnor\n",
      "  Title: Musk wants to leave politics because he’s tired of ‘attacks’ from the left: report...\n",
      "  Author: BreakfastTop6899\n",
      "  Subreddit: r/politics\n",
      "  Score: 13091 | Comments: 1823\n",
      "  Method: search\n",
      "  Is Self Post: False\n",
      "  Text: Link post: https://www.independent.co.uk/news/world/americas/us-politics/elon-musk-donald-trump-doge-b2736753.html\n",
      "  Created: 2025-04-22 03:17:46\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📈 Final Statistics:\n",
      "Total unique posts: 1219\n",
      "Posts with actual text: 868\n",
      "Subreddits covered: 4\n",
      "Methods used: {'new', 'hot', 'search'}\n",
      "Average text length: 1136 characters\n",
      "Longest text: 16093 characters\n",
      "\n",
      "🎉 Complete dataset saved to: reddit_political_data_complete_20250821_181052.json\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"xeV3sLrkl4xGyM-ScCBkSA\",\n",
    "    client_secret=\"sGrJQoAWabMp3hVcAXTxocV3gkCXoQ\",\n",
    "    user_agent=\"HeadlineHive\"\n",
    ")\n",
    "\n",
    "def collect_reddit_posts(subreddit_name, search_query=None, limit=500, method=\"search\"):\n",
    "    \"\"\"\n",
    "    Collect Reddit data with proper text extraction\n",
    "    \"\"\"\n",
    "    print(f\"🔍 Method: {method.upper()} from r/{subreddit_name}\")\n",
    "    if search_query:\n",
    "        print(f\"   Query: '{search_query}'\")\n",
    "    \n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "    \n",
    "    try:\n",
    "        # Choose method\n",
    "        if method == \"search\" and search_query:\n",
    "            submissions = subreddit.search(search_query, limit=limit, time_filter='all')\n",
    "        elif method == \"hot\":\n",
    "            submissions = subreddit.hot(limit=limit)\n",
    "        elif method == \"new\":\n",
    "            submissions = subreddit.new(limit=limit)\n",
    "        elif method == \"top\":\n",
    "            submissions = subreddit.top(limit=limit, time_filter='month')\n",
    "        else:\n",
    "            print(f\"Unknown method: {method}\")\n",
    "            return []\n",
    "        \n",
    "        for i, submission in enumerate(submissions):\n",
    "            try:\n",
    "                # Extract text content properly\n",
    "                text_content = \"\"\n",
    "                \n",
    "                # For self posts (text posts)\n",
    "                if submission.is_self and submission.selftext:\n",
    "                    text_content = submission.selftext.strip()\n",
    "                \n",
    "                # For link posts, we might want the URL or title as content\n",
    "                elif not submission.is_self:\n",
    "                    text_content = f\"Link post: {submission.url}\"\n",
    "                \n",
    "                # Sometimes selftext exists but is empty/whitespace\n",
    "                if not text_content and hasattr(submission, 'selftext'):\n",
    "                    if submission.selftext and submission.selftext.strip():\n",
    "                        text_content = submission.selftext.strip()\n",
    "                \n",
    "                # If still no content, use title as fallback\n",
    "                if not text_content:\n",
    "                    text_content = f\"[Title only] {submission.title}\"\n",
    "                \n",
    "                post_data = {\n",
    "                    'id': submission.id,\n",
    "                    'title': submission.title if submission.title else \"No title\",\n",
    "                    'text': text_content,\n",
    "                    'author': str(submission.author) if submission.author else '[deleted]',\n",
    "                    'subreddit': submission.subreddit.display_name,\n",
    "                    'score': submission.score if submission.score else 0,\n",
    "                    'upvote_ratio': getattr(submission, 'upvote_ratio', 0),\n",
    "                    'comments': submission.num_comments if submission.num_comments else 0,\n",
    "                    'url': submission.url if submission.url else \"\",\n",
    "                    'is_self': submission.is_self,\n",
    "                    'created': datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'permalink': f\"https://reddit.com{submission.permalink}\",\n",
    "                    'method_collected': method,\n",
    "                    'flair': submission.link_flair_text if hasattr(submission, 'link_flair_text') else None,\n",
    "                    'nsfw': submission.over_18 if hasattr(submission, 'over_18') else False\n",
    "                }\n",
    "                \n",
    "                posts.append(post_data)\n",
    "                \n",
    "                if (i + 1) % 25 == 0:\n",
    "                    print(f\"   Processed {i + 1} posts...\")\n",
    "                    time.sleep(0.3)  # Rate limiting\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   Error processing post {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"✅ Collected {len(posts)} posts using {method}\")\n",
    "        return posts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with {method} method: {e}\")\n",
    "        return []\n",
    "\n",
    "def remove_duplicates(all_posts):\n",
    "    \"\"\"Remove duplicate posts based on ID\"\"\"\n",
    "    seen_ids = set()\n",
    "    unique_posts = []\n",
    "    \n",
    "    for post in all_posts:\n",
    "        if post['id'] not in seen_ids:\n",
    "            unique_posts.append(post)\n",
    "            seen_ids.add(post['id'])\n",
    "    \n",
    "    return unique_posts\n",
    "\n",
    "def save_single_json(posts, filename):\n",
    "    \"\"\"Save all posts to a single JSON file\"\"\"\n",
    "    if not posts:\n",
    "        print(\"❌ No posts to save!\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Create comprehensive data structure\n",
    "        output_data = {\n",
    "            \"metadata\": {\n",
    "                \"total_posts\": len(posts),\n",
    "                \"collection_date\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"methods_used\": list(set([post.get('method_collected', 'unknown') for post in posts])),\n",
    "                \"subreddits\": list(set([post['subreddit'] for post in posts])),\n",
    "                \"date_range\": {\n",
    "                    \"earliest\": min([post['created'] for post in posts]),\n",
    "                    \"latest\": max([post['created'] for post in posts])\n",
    "                }\n",
    "            },\n",
    "            \"posts\": posts\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"✅ All data saved to: {filename}\")\n",
    "        print(f\"   Total posts: {len(posts)}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving JSON: {e}\")\n",
    "        return False\n",
    "\n",
    "def display_sample_with_text(posts, num_samples=3):\n",
    "    \"\"\"Display sample posts showing text content\"\"\"\n",
    "    if not posts:\n",
    "        print(\"No posts to display\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n📋 Sample of {min(num_samples, len(posts))} posts:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, post in enumerate(posts[:num_samples]):\n",
    "        print(f\"\\nPost {i+1}:\")\n",
    "        print(f\"  ID: {post['id']}\")\n",
    "        print(f\"  Title: {post['title'][:100]}...\")\n",
    "        print(f\"  Author: {post['author']}\")\n",
    "        print(f\"  Subreddit: r/{post['subreddit']}\")\n",
    "        print(f\"  Score: {post['score']} | Comments: {post['comments']}\")\n",
    "        print(f\"  Method: {post['method_collected']}\")\n",
    "        print(f\"  Is Self Post: {post['is_self']}\")\n",
    "        print(f\"  Text Preview: {post['text'][:200]}...\" if len(post['text']) > 200 else f\"  Text: {post['text']}\")\n",
    "        print(f\"  Created: {post['created']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Main execution - Single JSON output with both methods\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Reddit Data Collection - Single JSON Output\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test Reddit connection\n",
    "    try:\n",
    "        print(\"🔌 Testing Reddit connection...\")\n",
    "        test_sub = reddit.subreddit('python')\n",
    "        print(f\"✅ Connected! Test subreddit: {test_sub.display_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Reddit connection failed: {e}\")\n",
    "        print(\"Check your Reddit API credentials!\")\n",
    "        exit()\n",
    "    \n",
    "    # Collect from multiple sources using both methods\n",
    "    all_posts = []\n",
    "    \n",
    "    # Configuration\n",
    "    subreddits_to_check = ['politics', 'PoliticalDiscussion', 'Ask_Politics', 'neutralpolitics']\n",
    "    methods_to_try = ['search', 'hot', 'new']\n",
    "    \n",
    "    print(\"\\n🎯 Starting comprehensive data collection...\")\n",
    "    \n",
    "    # Method 1: Search for politics-related content\n",
    "    for subreddit in subreddits_to_check:\n",
    "        print(f\"\\n📍 Processing r/{subreddit}...\")\n",
    "        \n",
    "        # Try search method first\n",
    "        if 'search' in methods_to_try:\n",
    "            search_posts = collect_reddit_posts(subreddit, 'politics', limit=200, method=\"search\")\n",
    "            if search_posts:\n",
    "                all_posts.extend(search_posts)\n",
    "                print(f\"   Added {len(search_posts)} posts from search\")\n",
    "        \n",
    "        # Try hot posts method\n",
    "        if 'hot' in methods_to_try:\n",
    "            time.sleep(1)  # Rate limiting\n",
    "            hot_posts = collect_reddit_posts(subreddit, None, limit=100, method=\"hot\")\n",
    "            if hot_posts:\n",
    "                all_posts.extend(hot_posts)\n",
    "                print(f\"   Added {len(hot_posts)} posts from hot\")\n",
    "        \n",
    "        # Try new posts method\n",
    "        if 'new' in methods_to_try:\n",
    "            time.sleep(1)  # Rate limiting\n",
    "            new_posts = collect_reddit_posts(subreddit, None, limit=100, method=\"new\")\n",
    "            if new_posts:\n",
    "                all_posts.extend(new_posts)\n",
    "                print(f\"   Added {len(new_posts)} posts from new\")\n",
    "        \n",
    "        time.sleep(2)  # Rate limiting between subreddits\n",
    "    \n",
    "    # Process and save results\n",
    "    if all_posts:\n",
    "        print(f\"\\n📊 Processing {len(all_posts)} total posts...\")\n",
    "        \n",
    "        # Remove duplicates\n",
    "        unique_posts = remove_duplicates(all_posts)\n",
    "        print(f\"After removing duplicates: {len(unique_posts)} unique posts\")\n",
    "        \n",
    "        # Filter out posts with no meaningful text content if desired\n",
    "        posts_with_text = []\n",
    "        posts_without_text = []\n",
    "        \n",
    "        for post in unique_posts:\n",
    "            if post['text'] and not post['text'].startswith('[Title only]') and not post['text'].startswith('Link post:'):\n",
    "                posts_with_text.append(post)\n",
    "            else:\n",
    "                posts_without_text.append(post)\n",
    "        \n",
    "        print(f\"Posts with text content: {len(posts_with_text)}\")\n",
    "        print(f\"Posts without text content: {len(posts_without_text)}\")\n",
    "        \n",
    "        # Save single JSON file with all data\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        final_json = f\"reddit_political_data_complete_{timestamp}.json\"\n",
    "        \n",
    "        save_single_json(unique_posts, final_json)\n",
    "        \n",
    "        # Display sample with text content\n",
    "        display_sample_with_text(unique_posts, 5)\n",
    "        \n",
    "        # Show statistics\n",
    "        print(f\"\\n📈 Final Statistics:\")\n",
    "        print(f\"Total unique posts: {len(unique_posts)}\")\n",
    "        print(f\"Posts with actual text: {len(posts_with_text)}\")\n",
    "        print(f\"Subreddits covered: {len(set([post['subreddit'] for post in unique_posts]))}\")\n",
    "        print(f\"Methods used: {set([post['method_collected'] for post in unique_posts])}\")\n",
    "        \n",
    "        # Show text content statistics\n",
    "        text_lengths = [len(post['text']) for post in posts_with_text]\n",
    "        if text_lengths:\n",
    "            print(f\"Average text length: {sum(text_lengths) / len(text_lengths):.0f} characters\")\n",
    "            print(f\"Longest text: {max(text_lengths)} characters\")\n",
    "        \n",
    "        print(f\"\\n🎉 Complete dataset saved to: {final_json}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n❌ No data collected. Possible issues:\")\n",
    "        print(\"- Reddit API credentials invalid\")\n",
    "        print(\"- Network connectivity problems\") \n",
    "        print(\"- Rate limiting (try again later)\")\n",
    "        print(\"- Subreddit access restrictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a2e3a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 REDDIT COMPREHENSIVE DATA COLLECTION\n",
      "17 Categories - Separate JSON Files\n",
      "================================================================================\n",
      "🔌 Testing Reddit connection...\n",
      "✅ Connected! Test subreddit: python\n",
      "\n",
      "📋 CATEGORIES TO COLLECT (17):\n",
      "   1. Politics\n",
      "   2. Economy Business Finance\n",
      "   3. Science Technology\n",
      "   4. Health\n",
      "   5. Sport\n",
      "   6. Arts Culture Entertainment Media\n",
      "   7. Society\n",
      "   8. Human Interest\n",
      "   9. Crime Law Justice\n",
      "  10. Conflict War Peace\n",
      "  11. Disaster Accident Emergency\n",
      "  12. Environment\n",
      "  13. Education\n",
      "  14. Labour\n",
      "  15. Lifestyle Leisure\n",
      "  16. Weather\n",
      "  17. Religion\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 1/17: POLITICS\n",
      "================================================================================\n",
      "\n",
      "🎯 === COLLECTING POLITICS DATA ===\n",
      "\n",
      "  📍 Processing r/politics...\n",
      "  🔍 Method: SEARCH from r/politics\n",
      "     Query: 'politics'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'politics'\n",
      "  🔍 Method: SEARCH from r/politics\n",
      "     Query: 'election'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'election'\n",
      "  🔍 Method: SEARCH from r/politics\n",
      "     Query: 'government'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'government'\n",
      "  ✅ r/politics: 300 posts total\n",
      "\n",
      "  📍 Processing r/PoliticalDiscussion...\n",
      "  🔍 Method: SEARCH from r/PoliticalDiscussion\n",
      "     Query: 'politics'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'politics'\n",
      "  🔍 Method: SEARCH from r/PoliticalDiscussion\n",
      "     Query: 'election'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'election'\n",
      "  🔍 Method: SEARCH from r/PoliticalDiscussion\n",
      "     Query: 'government'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'government'\n",
      "  ✅ r/PoliticalDiscussion: 300 posts total\n",
      "\n",
      "  📍 Processing r/Ask_Politics...\n",
      "  🔍 Method: SEARCH from r/Ask_Politics\n",
      "     Query: 'politics'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 89 posts using search\n",
      "     Added 89 posts from search: 'politics'\n",
      "  🔍 Method: SEARCH from r/Ask_Politics\n",
      "     Query: 'election'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 74 posts using search\n",
      "     Added 74 posts from search: 'election'\n",
      "  🔍 Method: SEARCH from r/Ask_Politics\n",
      "     Query: 'government'\n",
      "     ✅ Collected 30 posts using search\n",
      "     Added 30 posts from search: 'government'\n",
      "  🔍 Method: HOT from r/Ask_Politics\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/Ask_Politics\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/Ask_Politics: 353 posts total\n",
      "\n",
      "  📍 Processing r/neutralpolitics...\n",
      "  🔍 Method: SEARCH from r/neutralpolitics\n",
      "     Query: 'politics'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 60 posts using search\n",
      "     Added 60 posts from search: 'politics'\n",
      "  🔍 Method: SEARCH from r/neutralpolitics\n",
      "     Query: 'election'\n",
      "     ✅ Collected 36 posts using search\n",
      "     Added 36 posts from search: 'election'\n",
      "  🔍 Method: SEARCH from r/neutralpolitics\n",
      "     Query: 'government'\n",
      "     ✅ Collected 33 posts using search\n",
      "     Added 33 posts from search: 'government'\n",
      "  🔍 Method: HOT from r/neutralpolitics\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/neutralpolitics\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/neutralpolitics: 289 posts total\n",
      "\n",
      "  📍 Processing r/worldpolitics...\n",
      "  🔍 Method: SEARCH from r/worldpolitics\n",
      "     Query: 'politics'\n",
      "     ✅ Collected 0 posts using search\n",
      "  🔍 Method: SEARCH from r/worldpolitics\n",
      "     Query: 'election'\n",
      "     ✅ Collected 0 posts using search\n",
      "  🔍 Method: SEARCH from r/worldpolitics\n",
      "     Query: 'government'\n",
      "     ✅ Collected 0 posts using search\n",
      "  🔍 Method: HOT from r/worldpolitics\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/worldpolitics\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/worldpolitics: 160 posts total\n",
      "\n",
      "  📊 Removed 446 duplicates\n",
      "  ✅ Politics data saved to: reddit_data/Politics_20250822_235911.json\n",
      "     Posts: 956\n",
      "     Subreddits: 5\n",
      "\n",
      "  📊 POLITICS SUMMARY:\n",
      "     Total posts: 956\n",
      "     Posts with text: 581\n",
      "     Text ratio: 60.8%\n",
      "     Subreddits: 5\n",
      "     Average score: 4875.5\n",
      "\n",
      "  ⏳ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 2/17: ECONOMY BUSINESS FINANCE\n",
      "================================================================================\n",
      "\n",
      "🎯 === COLLECTING ECONOMY BUSINESS FINANCE DATA ===\n",
      "\n",
      "  📍 Processing r/Economics...\n",
      "  🔍 Method: SEARCH from r/Economics\n",
      "     Query: 'economy'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'economy'\n",
      "  🔍 Method: SEARCH from r/Economics\n",
      "     Query: 'business'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'business'\n",
      "  🔍 Method: SEARCH from r/Economics\n",
      "     Query: 'finance'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 66 posts using search\n",
      "     Added 66 posts from search: 'finance'\n",
      "  ✅ r/Economics: 266 posts total\n",
      "\n",
      "  📍 Processing r/business...\n",
      "  🔍 Method: SEARCH from r/business\n",
      "     Query: 'economy'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 62 posts using search\n",
      "     Added 62 posts from search: 'economy'\n",
      "  🔍 Method: SEARCH from r/business\n",
      "     Query: 'business'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'business'\n",
      "  🔍 Method: SEARCH from r/business\n",
      "     Query: 'finance'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'finance'\n",
      "  ✅ r/business: 262 posts total\n",
      "\n",
      "  📍 Processing r/stocks...\n",
      "  🔍 Method: SEARCH from r/stocks\n",
      "     Query: 'economy'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'economy'\n",
      "  🔍 Method: SEARCH from r/stocks\n",
      "     Query: 'business'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'business'\n",
      "  🔍 Method: SEARCH from r/stocks\n",
      "     Query: 'finance'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'finance'\n",
      "  ✅ r/stocks: 300 posts total\n",
      "\n",
      "  📍 Processing r/investing...\n",
      "  🔍 Method: SEARCH from r/investing\n",
      "     Query: 'economy'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'economy'\n",
      "  🔍 Method: SEARCH from r/investing\n",
      "     Query: 'business'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'business'\n",
      "  🔍 Method: SEARCH from r/investing\n",
      "     Query: 'finance'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'finance'\n",
      "  ✅ r/investing: 300 posts total\n",
      "\n",
      "  📍 Processing r/personalfinance...\n",
      "  🔍 Method: SEARCH from r/personalfinance\n",
      "     Query: 'economy'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'economy'\n",
      "  🔍 Method: SEARCH from r/personalfinance\n",
      "     Query: 'business'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'business'\n",
      "  🔍 Method: SEARCH from r/personalfinance\n",
      "     Query: 'finance'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'finance'\n",
      "  ✅ r/personalfinance: 300 posts total\n",
      "\n",
      "  📍 Processing r/economy...\n",
      "  🔍 Method: SEARCH from r/economy\n",
      "     Query: 'economy'\n",
      "     ✅ Collected 0 posts using search\n",
      "  🔍 Method: SEARCH from r/economy\n",
      "     Query: 'business'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'business'\n",
      "  🔍 Method: SEARCH from r/economy\n",
      "     Query: 'finance'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'finance'\n",
      "  🔍 Method: HOT from r/economy\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ✅ r/economy: 280 posts total\n",
      "\n",
      "  📍 Processing r/finance...\n",
      "  🔍 Method: SEARCH from r/finance\n",
      "     Query: 'economy'\n",
      "     ✅ Collected 10 posts using search\n",
      "     Added 10 posts from search: 'economy'\n",
      "  🔍 Method: SEARCH from r/finance\n",
      "     Query: 'business'\n",
      "     ✅ Collected 6 posts using search\n",
      "     Added 6 posts from search: 'business'\n",
      "  🔍 Method: SEARCH from r/finance\n",
      "     Query: 'finance'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'finance'\n",
      "  🔍 Method: HOT from r/finance\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/finance\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/finance: 276 posts total\n",
      "\n",
      "  📊 Removed 194 duplicates\n",
      "  ✅ Economy_Business_Finance data saved to: reddit_data/Economy_Business_Finance_20250823_000119.json\n",
      "     Posts: 1790\n",
      "     Subreddits: 7\n",
      "\n",
      "  📊 ECONOMY BUSINESS FINANCE SUMMARY:\n",
      "     Total posts: 1790\n",
      "     Posts with text: 1093\n",
      "     Text ratio: 61.1%\n",
      "     Subreddits: 7\n",
      "     Average score: 1062.8\n",
      "\n",
      "  ⏳ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 3/17: SCIENCE TECHNOLOGY\n",
      "================================================================================\n",
      "\n",
      "🎯 === COLLECTING SCIENCE TECHNOLOGY DATA ===\n",
      "\n",
      "  📍 Processing r/technology...\n",
      "  🔍 Method: SEARCH from r/technology\n",
      "     Query: 'technology'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'technology'\n",
      "  🔍 Method: SEARCH from r/technology\n",
      "     Query: 'science'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'science'\n",
      "  ✅ r/technology: 200 posts total\n",
      "\n",
      "  📍 Processing r/science...\n",
      "  🔍 Method: SEARCH from r/science\n",
      "     Query: 'technology'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 76 posts using search\n",
      "     Added 76 posts from search: 'technology'\n",
      "  🔍 Method: SEARCH from r/science\n",
      "     Query: 'science'\n",
      "     ✅ Collected 0 posts using search\n",
      "  🔍 Method: SEARCH from r/science\n",
      "     Query: 'innovation'\n",
      "     ✅ Collected 40 posts using search\n",
      "     Added 40 posts from search: 'innovation'\n",
      "  🔍 Method: HOT from r/science\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ✅ r/science: 196 posts total\n",
      "\n",
      "  📍 Processing r/gadgets...\n",
      "  🔍 Method: SEARCH from r/gadgets\n",
      "     Query: 'technology'\n",
      "     ✅ Collected 8 posts using search\n",
      "     Added 8 posts from search: 'technology'\n",
      "  🔍 Method: SEARCH from r/gadgets\n",
      "     Query: 'science'\n",
      "     ✅ Collected 7 posts using search\n",
      "     Added 7 posts from search: 'science'\n",
      "  🔍 Method: SEARCH from r/gadgets\n",
      "     Query: 'innovation'\n",
      "     ✅ Collected 11 posts using search\n",
      "     Added 11 posts from search: 'innovation'\n",
      "  🔍 Method: HOT from r/gadgets\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/gadgets\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/gadgets: 186 posts total\n",
      "\n",
      "  📍 Processing r/tech...\n",
      "  🔍 Method: SEARCH from r/tech\n",
      "     Query: 'technology'\n",
      "     ✅ Collected 26 posts using search\n",
      "     Added 26 posts from search: 'technology'\n",
      "  🔍 Method: SEARCH from r/tech\n",
      "     Query: 'science'\n",
      "     ✅ Collected 40 posts using search\n",
      "     Added 40 posts from search: 'science'\n",
      "  🔍 Method: SEARCH from r/tech\n",
      "     Query: 'innovation'\n",
      "     ✅ Collected 15 posts using search\n",
      "     Added 15 posts from search: 'innovation'\n",
      "  🔍 Method: HOT from r/tech\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/tech\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/tech: 241 posts total\n",
      "\n",
      "  📍 Processing r/MachineLearning...\n",
      "  🔍 Method: SEARCH from r/MachineLearning\n",
      "     Query: 'technology'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 57 posts using search\n",
      "     Added 57 posts from search: 'technology'\n",
      "  🔍 Method: SEARCH from r/MachineLearning\n",
      "     Query: 'science'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'science'\n",
      "  🔍 Method: SEARCH from r/MachineLearning\n",
      "     Query: 'innovation'\n",
      "     ✅ Collected 0 posts using search\n",
      "  🔍 Method: HOT from r/MachineLearning\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ✅ r/MachineLearning: 237 posts total\n",
      "\n",
      "  📍 Processing r/artificial...\n",
      "  🔍 Method: SEARCH from r/artificial\n",
      "     Query: 'technology'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'technology'\n",
      "  🔍 Method: SEARCH from r/artificial\n",
      "     Query: 'science'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'science'\n",
      "  ✅ r/artificial: 200 posts total\n",
      "\n",
      "  📍 Processing r/programming...\n",
      "  🔍 Method: SEARCH from r/programming\n",
      "     Query: 'technology'\n",
      "     ✅ Collected 39 posts using search\n",
      "     Added 39 posts from search: 'technology'\n",
      "  🔍 Method: SEARCH from r/programming\n",
      "     Query: 'science'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 62 posts using search\n",
      "     Added 62 posts from search: 'science'\n",
      "  🔍 Method: SEARCH from r/programming\n",
      "     Query: 'innovation'\n",
      "     ✅ Collected 32 posts using search\n",
      "     Added 32 posts from search: 'innovation'\n",
      "  🔍 Method: HOT from r/programming\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ✅ r/programming: 213 posts total\n",
      "\n",
      "  📍 Processing r/computers...\n",
      "  🔍 Method: SEARCH from r/computers\n",
      "     Query: 'technology'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'technology'\n",
      "  🔍 Method: SEARCH from r/computers\n",
      "     Query: 'science'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'science'\n",
      "  ✅ r/computers: 200 posts total\n",
      "\n",
      "  📊 Removed 195 duplicates\n",
      "  ✅ Science_Technology data saved to: reddit_data/Science_Technology_20250823_000331.json\n",
      "     Posts: 1478\n",
      "     Subreddits: 8\n",
      "\n",
      "  📊 SCIENCE TECHNOLOGY SUMMARY:\n",
      "     Total posts: 1478\n",
      "     Posts with text: 407\n",
      "     Text ratio: 27.5%\n",
      "     Subreddits: 8\n",
      "     Average score: 2318.2\n",
      "\n",
      "  ⏳ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 4/17: HEALTH\n",
      "================================================================================\n",
      "\n",
      "🎯 === COLLECTING HEALTH DATA ===\n",
      "\n",
      "  📍 Processing r/Health...\n",
      "  🔍 Method: SEARCH from r/Health\n",
      "     Query: 'health'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'health'\n",
      "  🔍 Method: SEARCH from r/Health\n",
      "     Query: 'medicine'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 61 posts using search\n",
      "     Added 61 posts from search: 'medicine'\n",
      "  🔍 Method: SEARCH from r/Health\n",
      "     Query: 'medical'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'medical'\n",
      "  ✅ r/Health: 261 posts total\n",
      "\n",
      "  📍 Processing r/medicine...\n",
      "  🔍 Method: SEARCH from r/medicine\n",
      "     Query: 'health'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'health'\n",
      "  🔍 Method: SEARCH from r/medicine\n",
      "     Query: 'medicine'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'medicine'\n",
      "  🔍 Method: SEARCH from r/medicine\n",
      "     Query: 'medical'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'medical'\n",
      "  ✅ r/medicine: 300 posts total\n",
      "\n",
      "  📍 Processing r/medical...\n",
      "  🔍 Method: SEARCH from r/medical\n",
      "     Query: 'health'\n",
      "     ✅ Collected 0 posts using search\n",
      "  🔍 Method: SEARCH from r/medical\n",
      "     Query: 'medicine'\n",
      "     ✅ Collected 0 posts using search\n",
      "  🔍 Method: SEARCH from r/medical\n",
      "     Query: 'medical'\n",
      "     ✅ Collected 0 posts using search\n",
      "  🔍 Method: HOT from r/medical\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/medical\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/medical: 160 posts total\n",
      "\n",
      "  📍 Processing r/healthcare...\n",
      "  🔍 Method: SEARCH from r/healthcare\n",
      "     Query: 'health'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'health'\n",
      "  🔍 Method: SEARCH from r/healthcare\n",
      "     Query: 'medicine'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 90 posts using search\n",
      "     Added 90 posts from search: 'medicine'\n",
      "  🔍 Method: SEARCH from r/healthcare\n",
      "     Query: 'medical'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'medical'\n",
      "  ✅ r/healthcare: 290 posts total\n",
      "\n",
      "  📍 Processing r/nutrition...\n",
      "  🔍 Method: SEARCH from r/nutrition\n",
      "     Query: 'health'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'health'\n",
      "  🔍 Method: SEARCH from r/nutrition\n",
      "     Query: 'medicine'\n",
      "     ✅ Collected 29 posts using search\n",
      "     Added 29 posts from search: 'medicine'\n",
      "  🔍 Method: SEARCH from r/nutrition\n",
      "     Query: 'medical'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 66 posts using search\n",
      "     Added 66 posts from search: 'medical'\n",
      "  🔍 Method: HOT from r/nutrition\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ✅ r/nutrition: 275 posts total\n",
      "\n",
      "  📍 Processing r/fitness...\n",
      "  🔍 Method: SEARCH from r/fitness\n",
      "     Query: 'health'\n",
      "     ✅ Collected 0 posts using search\n",
      "  🔍 Method: SEARCH from r/fitness\n",
      "     Query: 'medicine'\n",
      "     ✅ Collected 2 posts using search\n",
      "     Added 2 posts from search: 'medicine'\n",
      "  🔍 Method: SEARCH from r/fitness\n",
      "     Query: 'medical'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'medical'\n",
      "  🔍 Method: HOT from r/fitness\n",
      "     ✅ Collected 42 posts using hot\n",
      "     Added 42 posts from hot\n",
      "  🔍 Method: NEW from r/fitness\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/fitness: 224 posts total\n",
      "\n",
      "  📍 Processing r/mentalhealth...\n",
      "  🔍 Method: SEARCH from r/mentalhealth\n",
      "     Query: 'health'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'health'\n",
      "  🔍 Method: SEARCH from r/mentalhealth\n",
      "     Query: 'medicine'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'medicine'\n",
      "  🔍 Method: SEARCH from r/mentalhealth\n",
      "     Query: 'medical'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'medical'\n",
      "  ✅ r/mentalhealth: 300 posts total\n",
      "\n",
      "  📊 Removed 205 duplicates\n",
      "  ✅ Health data saved to: reddit_data/Health_20250823_000541.json\n",
      "     Posts: 1605\n",
      "     Subreddits: 7\n",
      "\n",
      "  📊 HEALTH SUMMARY:\n",
      "     Total posts: 1605\n",
      "     Posts with text: 1165\n",
      "     Text ratio: 72.6%\n",
      "     Subreddits: 7\n",
      "     Average score: 167.8\n",
      "\n",
      "  ⏳ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 5/17: SPORT\n",
      "================================================================================\n",
      "\n",
      "🎯 === COLLECTING SPORT DATA ===\n",
      "\n",
      "  📍 Processing r/sports...\n",
      "  🔍 Method: SEARCH from r/sports\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  🔍 Method: SEARCH from r/sports\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'football'\n",
      "  ✅ r/sports: 200 posts total\n",
      "\n",
      "  📍 Processing r/nfl...\n",
      "  🔍 Method: SEARCH from r/nfl\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  🔍 Method: SEARCH from r/nfl\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'football'\n",
      "  ✅ r/nfl: 200 posts total\n",
      "\n",
      "  📍 Processing r/nba...\n",
      "  🔍 Method: SEARCH from r/nba\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  🔍 Method: SEARCH from r/nba\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'football'\n",
      "  ✅ r/nba: 200 posts total\n",
      "\n",
      "  📍 Processing r/soccer...\n",
      "  🔍 Method: SEARCH from r/soccer\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  🔍 Method: SEARCH from r/soccer\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'football'\n",
      "  ✅ r/soccer: 200 posts total\n",
      "\n",
      "  📍 Processing r/baseball...\n",
      "  🔍 Method: SEARCH from r/baseball\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  🔍 Method: SEARCH from r/baseball\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'football'\n",
      "  ✅ r/baseball: 200 posts total\n",
      "\n",
      "  📍 Processing r/hockey...\n",
      "  🔍 Method: SEARCH from r/hockey\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  🔍 Method: SEARCH from r/hockey\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'football'\n",
      "  ✅ r/hockey: 200 posts total\n",
      "\n",
      "  📍 Processing r/olympics...\n",
      "  🔍 Method: SEARCH from r/olympics\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  🔍 Method: SEARCH from r/olympics\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 58 posts using search\n",
      "     Added 58 posts from search: 'football'\n",
      "  🔍 Method: SEARCH from r/olympics\n",
      "     Query: 'basketball'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 53 posts using search\n",
      "     Added 53 posts from search: 'basketball'\n",
      "  ✅ r/olympics: 211 posts total\n",
      "\n",
      "  📍 Processing r/football...\n",
      "  🔍 Method: SEARCH from r/football\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  🔍 Method: SEARCH from r/football\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'football'\n",
      "  ✅ r/football: 200 posts total\n",
      "\n",
      "  📊 Removed 51 duplicates\n",
      "  ✅ Sport data saved to: reddit_data/Sport_20250823_000732.json\n",
      "     Posts: 1560\n",
      "     Subreddits: 8\n",
      "\n",
      "  📊 SPORT SUMMARY:\n",
      "     Total posts: 1560\n",
      "     Posts with text: 419\n",
      "     Text ratio: 26.9%\n",
      "     Subreddits: 8\n",
      "     Average score: 3217.7\n",
      "\n",
      "  ⏳ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 6/17: ARTS CULTURE ENTERTAINMENT MEDIA\n",
      "================================================================================\n",
      "\n",
      "🎯 === COLLECTING ARTS CULTURE ENTERTAINMENT MEDIA DATA ===\n",
      "\n",
      "  📍 Processing r/movies...\n",
      "  🔍 Method: SEARCH from r/movies\n",
      "     Query: 'movies'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'movies'\n",
      "  🔍 Method: SEARCH from r/movies\n",
      "     Query: 'music'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'music'\n",
      "  ✅ r/movies: 200 posts total\n",
      "\n",
      "  📍 Processing r/music...\n",
      "  🔍 Method: SEARCH from r/music\n",
      "     Query: 'movies'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'movies'\n",
      "  🔍 Method: SEARCH from r/music\n",
      "     Query: 'music'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'music'\n",
      "  ✅ r/music: 200 posts total\n",
      "\n",
      "  📍 Processing r/art...\n",
      "  🔍 Method: SEARCH from r/art\n",
      "     Query: 'movies'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 74 posts using search\n",
      "     Added 74 posts from search: 'movies'\n",
      "  🔍 Method: SEARCH from r/art\n",
      "     Query: 'music'\n",
      "     ✅ Collected 43 posts using search\n",
      "     Added 43 posts from search: 'music'\n",
      "  🔍 Method: SEARCH from r/art\n",
      "     Query: 'art'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'art'\n",
      "  ✅ r/art: 217 posts total\n",
      "\n",
      "  📍 Processing r/books...\n",
      "  🔍 Method: SEARCH from r/books\n",
      "     Query: 'movies'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'movies'\n",
      "  🔍 Method: SEARCH from r/books\n",
      "     Query: 'music'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 70 posts using search\n",
      "     Added 70 posts from search: 'music'\n",
      "  🔍 Method: SEARCH from r/books\n",
      "     Query: 'art'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'art'\n",
      "  ✅ r/books: 270 posts total\n",
      "\n",
      "  📍 Processing r/television...\n",
      "  🔍 Method: SEARCH from r/television\n",
      "     Query: 'movies'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'movies'\n",
      "  🔍 Method: SEARCH from r/television\n",
      "     Query: 'music'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'music'\n",
      "  ✅ r/television: 200 posts total\n",
      "\n",
      "  📍 Processing r/entertainment...\n",
      "  🔍 Method: SEARCH from r/entertainment\n",
      "     Query: 'movies'\n",
      "     ✅ Collected 0 posts using search\n",
      "  🔍 Method: SEARCH from r/entertainment\n",
      "     Query: 'music'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'music'\n",
      "  🔍 Method: SEARCH from r/entertainment\n",
      "     Query: 'art'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'art'\n",
      "  ✅ r/entertainment: 200 posts total\n",
      "\n",
      "  📍 Processing r/culture...\n",
      "  🔍 Method: SEARCH from r/culture\n",
      "     Query: 'movies'\n",
      "     ✅ Collected 13 posts using search\n",
      "     Added 13 posts from search: 'movies'\n",
      "  🔍 Method: SEARCH from r/culture\n",
      "     Query: 'music'\n",
      "     ✅ Collected 30 posts using search\n",
      "     Added 30 posts from search: 'music'\n",
      "  🔍 Method: SEARCH from r/culture\n",
      "     Query: 'art'\n",
      "     ✅ Collected 32 posts using search\n",
      "     Added 32 posts from search: 'art'\n",
      "  🔍 Method: HOT from r/culture\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/culture\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/culture: 235 posts total\n",
      "\n",
      "  📍 Processing r/media...\n",
      "  🔍 Method: SEARCH from r/media\n",
      "     Query: 'movies'\n",
      "     ❌ Error with search method: received 404 HTTP response\n",
      "  🔍 Method: SEARCH from r/media\n",
      "     Query: 'music'\n",
      "     ❌ Error with search method: received 404 HTTP response\n",
      "  🔍 Method: SEARCH from r/media\n",
      "     Query: 'art'\n",
      "     ❌ Error with search method: received 404 HTTP response\n",
      "  🔍 Method: HOT from r/media\n",
      "     ❌ Error with hot method: received 404 HTTP response\n",
      "  🔍 Method: NEW from r/media\n",
      "     ❌ Error with new method: received 404 HTTP response\n",
      "  ✅ r/media: 0 posts total\n",
      "\n",
      "  📊 Removed 121 duplicates\n",
      "  ✅ Arts_Culture_Entertainment_Media data saved to: reddit_data/Arts_Culture_Entertainment_Media_20250823_000935.json\n",
      "     Posts: 1401\n",
      "     Subreddits: 7\n",
      "\n",
      "  📊 ARTS CULTURE ENTERTAINMENT MEDIA SUMMARY:\n",
      "     Total posts: 1401\n",
      "     Posts with text: 550\n",
      "     Text ratio: 39.3%\n",
      "     Subreddits: 7\n",
      "     Average score: 2243.2\n",
      "\n",
      "  ⏳ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 7/17: SOCIETY\n",
      "================================================================================\n",
      "\n",
      "🎯 === COLLECTING SOCIETY DATA ===\n",
      "\n",
      "  📍 Processing r/sociology...\n",
      "  🔍 Method: SEARCH from r/sociology\n",
      "     Query: 'society'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'society'\n",
      "  🔍 Method: SEARCH from r/sociology\n",
      "     Query: 'social'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'social'\n",
      "  🔍 Method: SEARCH from r/sociology\n",
      "     Query: 'community'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 65 posts using search\n",
      "     Added 65 posts from search: 'community'\n",
      "  ✅ r/sociology: 265 posts total\n",
      "\n",
      "  📍 Processing r/society...\n",
      "  🔍 Method: SEARCH from r/society\n",
      "     Query: 'society'\n",
      "     ❌ Error with search method: received 403 HTTP response\n",
      "  🔍 Method: SEARCH from r/society\n",
      "     Query: 'social'\n",
      "     ❌ Error with search method: received 403 HTTP response\n",
      "  🔍 Method: SEARCH from r/society\n",
      "     Query: 'community'\n",
      "     ❌ Error with search method: received 403 HTTP response\n",
      "  🔍 Method: HOT from r/society\n",
      "     ❌ Error with hot method: received 403 HTTP response\n",
      "  🔍 Method: NEW from r/society\n",
      "     ❌ Error with new method: received 403 HTTP response\n",
      "  ✅ r/society: 0 posts total\n",
      "\n",
      "  📍 Processing r/social...\n",
      "  🔍 Method: SEARCH from r/social\n",
      "     Query: 'society'\n",
      "     ✅ Collected 0 posts using search\n",
      "  🔍 Method: SEARCH from r/social\n",
      "     Query: 'social'\n",
      "     ✅ Collected 0 posts using search\n",
      "  🔍 Method: SEARCH from r/social\n",
      "     Query: 'community'\n",
      "     ✅ Collected 0 posts using search\n",
      "  🔍 Method: HOT from r/social\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/social\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/social: 160 posts total\n",
      "\n",
      "  📍 Processing r/community...\n",
      "  🔍 Method: SEARCH from r/community\n",
      "     Query: 'society'\n",
      "     ✅ Collected 3 posts using search\n",
      "     Added 3 posts from search: 'society'\n",
      "  🔍 Method: SEARCH from r/community\n",
      "     Query: 'social'\n",
      "     ✅ Collected 19 posts using search\n",
      "     Added 19 posts from search: 'social'\n",
      "  🔍 Method: SEARCH from r/community\n",
      "     Query: 'community'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'community'\n",
      "  🔍 Method: HOT from r/community\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/community\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/community: 282 posts total\n",
      "\n",
      "  📍 Processing r/TrueReddit...\n",
      "  🔍 Method: SEARCH from r/TrueReddit\n",
      "     Query: 'society'\n",
      "     ✅ Collected 10 posts using search\n",
      "     Added 10 posts from search: 'society'\n",
      "  🔍 Method: SEARCH from r/TrueReddit\n",
      "     Query: 'social'\n",
      "     ✅ Collected 41 posts using search\n",
      "     Added 41 posts from search: 'social'\n",
      "  🔍 Method: SEARCH from r/TrueReddit\n",
      "     Query: 'community'\n",
      "     ✅ Collected 4 posts using search\n",
      "     Added 4 posts from search: 'community'\n",
      "  🔍 Method: HOT from r/TrueReddit\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/TrueReddit\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/TrueReddit: 215 posts total\n",
      "\n",
      "  📍 Processing r/news...\n",
      "  🔍 Method: SEARCH from r/news\n",
      "     Query: 'society'\n",
      "     ✅ Collected 26 posts using search\n",
      "     Added 26 posts from search: 'society'\n",
      "  🔍 Method: SEARCH from r/news\n",
      "     Query: 'social'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 81 posts using search\n",
      "     Added 81 posts from search: 'social'\n",
      "  🔍 Method: SEARCH from r/news\n",
      "     Query: 'community'\n",
      "     ✅ Collected 18 posts using search\n",
      "     Added 18 posts from search: 'community'\n",
      "  🔍 Method: HOT from r/news\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/news\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/news: 285 posts total\n",
      "\n",
      "  📊 Removed 357 duplicates\n",
      "  ✅ Society data saved to: reddit_data/Society_20250823_001122.json\n",
      "     Posts: 850\n",
      "     Subreddits: 5\n",
      "\n",
      "  📊 SOCIETY SUMMARY:\n",
      "     Total posts: 850\n",
      "     Posts with text: 339\n",
      "     Text ratio: 39.9%\n",
      "     Subreddits: 5\n",
      "     Average score: 2397.7\n",
      "\n",
      "  ⏳ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 8/17: HUMAN INTEREST\n",
      "================================================================================\n",
      "\n",
      "🎯 === COLLECTING HUMAN INTEREST DATA ===\n",
      "\n",
      "  📍 Processing r/UpliftingNews...\n",
      "  🔍 Method: SEARCH from r/UpliftingNews\n",
      "     Query: 'human interest'\n",
      "     ✅ Collected 0 posts using search\n",
      "  🔍 Method: SEARCH from r/UpliftingNews\n",
      "     Query: 'inspiring'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 50 posts using search\n",
      "     Added 50 posts from search: 'inspiring'\n",
      "  🔍 Method: SEARCH from r/UpliftingNews\n",
      "     Query: 'heartwarming'\n",
      "     ✅ Collected 26 posts using search\n",
      "     Added 26 posts from search: 'heartwarming'\n",
      "  🔍 Method: HOT from r/UpliftingNews\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/UpliftingNews\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/UpliftingNews: 236 posts total\n",
      "\n",
      "  📍 Processing r/MadeMeSmile...\n",
      "  🔍 Method: SEARCH from r/MadeMeSmile\n",
      "     Query: 'human interest'\n",
      "     ✅ Collected 6 posts using search\n",
      "     Added 6 posts from search: 'human interest'\n",
      "  🔍 Method: SEARCH from r/MadeMeSmile\n",
      "     Query: 'inspiring'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'inspiring'\n",
      "  🔍 Method: SEARCH from r/MadeMeSmile\n",
      "     Query: 'heartwarming'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'heartwarming'\n",
      "  🔍 Method: HOT from r/MadeMeSmile\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ✅ r/MadeMeSmile: 286 posts total\n",
      "\n",
      "  📍 Processing r/happy...\n",
      "  🔍 Method: SEARCH from r/happy\n",
      "     Query: 'human interest'\n",
      "     ✅ Collected 1 posts using search\n",
      "     Added 1 posts from search: 'human interest'\n",
      "  🔍 Method: SEARCH from r/happy\n",
      "     Query: 'inspiring'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 64 posts using search\n",
      "     Added 64 posts from search: 'inspiring'\n",
      "  🔍 Method: SEARCH from r/happy\n",
      "     Query: 'heartwarming'\n",
      "     ✅ Collected 9 posts using search\n",
      "     Added 9 posts from search: 'heartwarming'\n",
      "  🔍 Method: HOT from r/happy\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/happy\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/happy: 234 posts total\n",
      "\n",
      "  📍 Processing r/wholesome...\n",
      "  🔍 Method: SEARCH from r/wholesome\n",
      "     Query: 'human interest'\n",
      "     ✅ Collected 1 posts using search\n",
      "     Added 1 posts from search: 'human interest'\n",
      "  🔍 Method: SEARCH from r/wholesome\n",
      "     Query: 'inspiring'\n",
      "     ✅ Collected 21 posts using search\n",
      "     Added 21 posts from search: 'inspiring'\n",
      "  🔍 Method: SEARCH from r/wholesome\n",
      "     Query: 'heartwarming'\n",
      "     ✅ Collected 29 posts using search\n",
      "     Added 29 posts from search: 'heartwarming'\n",
      "  🔍 Method: HOT from r/wholesome\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/wholesome\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/wholesome: 211 posts total\n",
      "\n",
      "  📍 Processing r/HumansBeingBros...\n",
      "  🔍 Method: SEARCH from r/HumansBeingBros\n",
      "     Query: 'human interest'\n",
      "     ✅ Collected 0 posts using search\n",
      "  🔍 Method: SEARCH from r/HumansBeingBros\n",
      "     Query: 'inspiring'\n",
      "     ✅ Collected 5 posts using search\n",
      "     Added 5 posts from search: 'inspiring'\n",
      "  🔍 Method: SEARCH from r/HumansBeingBros\n",
      "     Query: 'heartwarming'\n",
      "     ✅ Collected 11 posts using search\n",
      "     Added 11 posts from search: 'heartwarming'\n",
      "  🔍 Method: HOT from r/HumansBeingBros\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/HumansBeingBros\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/HumansBeingBros: 176 posts total\n",
      "\n",
      "  📍 Processing r/todayilearned...\n",
      "  🔍 Method: SEARCH from r/todayilearned\n",
      "     Query: 'human interest'\n",
      "     ✅ Collected 10 posts using search\n",
      "     Added 10 posts from search: 'human interest'\n",
      "  🔍 Method: SEARCH from r/todayilearned\n",
      "     Query: 'inspiring'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'inspiring'\n",
      "  🔍 Method: SEARCH from r/todayilearned\n",
      "     Query: 'heartwarming'\n",
      "     ✅ Collected 5 posts using search\n",
      "     Added 5 posts from search: 'heartwarming'\n",
      "  🔍 Method: HOT from r/todayilearned\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/todayilearned\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/todayilearned: 275 posts total\n",
      "\n",
      "  📊 Removed 409 duplicates\n",
      "  ✅ Human_Interest data saved to: reddit_data/Human_Interest_20250823_001325.json\n",
      "     Posts: 1009\n",
      "     Subreddits: 6\n",
      "\n",
      "  📊 HUMAN INTEREST SUMMARY:\n",
      "     Total posts: 1009\n",
      "     Posts with text: 115\n",
      "     Text ratio: 11.4%\n",
      "     Subreddits: 6\n",
      "     Average score: 8879.6\n",
      "\n",
      "  ⏳ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 9/17: CRIME LAW JUSTICE\n",
      "================================================================================\n",
      "\n",
      "🎯 === COLLECTING CRIME LAW JUSTICE DATA ===\n",
      "\n",
      "  📍 Processing r/law...\n",
      "  🔍 Method: SEARCH from r/law\n",
      "     Query: 'crime'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'crime'\n",
      "  🔍 Method: SEARCH from r/law\n",
      "     Query: 'law'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'law'\n",
      "  🔍 Method: SEARCH from r/law\n",
      "     Query: 'justice'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'justice'\n",
      "  ✅ r/law: 300 posts total\n",
      "\n",
      "  📍 Processing r/legaladvice...\n",
      "  🔍 Method: SEARCH from r/legaladvice\n",
      "     Query: 'crime'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'crime'\n",
      "  🔍 Method: SEARCH from r/legaladvice\n",
      "     Query: 'law'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'law'\n",
      "  🔍 Method: SEARCH from r/legaladvice\n",
      "     Query: 'justice'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'justice'\n",
      "  ✅ r/legaladvice: 300 posts total\n",
      "\n",
      "  📍 Processing r/UnresolvedMysteries...\n",
      "  🔍 Method: SEARCH from r/UnresolvedMysteries\n",
      "     Query: 'crime'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'crime'\n",
      "  🔍 Method: SEARCH from r/UnresolvedMysteries\n",
      "     Query: 'law'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'law'\n",
      "  🔍 Method: SEARCH from r/UnresolvedMysteries\n",
      "     Query: 'justice'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'justice'\n",
      "  ✅ r/UnresolvedMysteries: 300 posts total\n",
      "\n",
      "  📍 Processing r/TrueCrime...\n",
      "  🔍 Method: SEARCH from r/TrueCrime\n",
      "     Query: 'crime'\n",
      "     ✅ Collected 32 posts using search\n",
      "     Added 32 posts from search: 'crime'\n",
      "  🔍 Method: SEARCH from r/TrueCrime\n",
      "     Query: 'law'\n",
      "     ✅ Collected 8 posts using search\n",
      "     Added 8 posts from search: 'law'\n",
      "  🔍 Method: SEARCH from r/TrueCrime\n",
      "     Query: 'justice'\n",
      "     ✅ Collected 8 posts using search\n",
      "     Added 8 posts from search: 'justice'\n",
      "  🔍 Method: HOT from r/TrueCrime\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/TrueCrime\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/TrueCrime: 208 posts total\n",
      "\n",
      "  📍 Processing r/justice...\n",
      "  🔍 Method: SEARCH from r/justice\n",
      "     Query: 'crime'\n",
      "     ❌ Error with search method: received 403 HTTP response\n",
      "  🔍 Method: SEARCH from r/justice\n",
      "     Query: 'law'\n",
      "     ❌ Error with search method: received 403 HTTP response\n",
      "  🔍 Method: SEARCH from r/justice\n",
      "     Query: 'justice'\n",
      "     ❌ Error with search method: received 403 HTTP response\n",
      "  🔍 Method: HOT from r/justice\n",
      "     ❌ Error with hot method: received 403 HTTP response\n",
      "  🔍 Method: NEW from r/justice\n",
      "     ❌ Error with new method: received 403 HTTP response\n",
      "  ✅ r/justice: 0 posts total\n",
      "\n",
      "  📍 Processing r/police...\n",
      "  🔍 Method: SEARCH from r/police\n",
      "     Query: 'crime'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'crime'\n",
      "  🔍 Method: SEARCH from r/police\n",
      "     Query: 'law'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/police/search/?q=law&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205552B70>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: SEARCH from r/police\n",
      "     Query: 'justice'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/police/search/?q=justice&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205490260>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: HOT from r/police\n",
      "     ❌ Error with hot method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/police/hot?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015204672300>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: NEW from r/police\n",
      "     ❌ Error with new method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/police/new?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152054932F0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  ✅ r/police: 100 posts total\n",
      "\n",
      "  📊 Removed 210 duplicates\n",
      "  ✅ Crime_Law_Justice data saved to: reddit_data/Crime_Law_Justice_20250823_001548.json\n",
      "     Posts: 998\n",
      "     Subreddits: 5\n",
      "\n",
      "  📊 CRIME LAW JUSTICE SUMMARY:\n",
      "     Total posts: 998\n",
      "     Posts with text: 675\n",
      "     Text ratio: 67.6%\n",
      "     Subreddits: 5\n",
      "     Average score: 4161.3\n",
      "\n",
      "  ⏳ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 10/17: CONFLICT WAR PEACE\n",
      "================================================================================\n",
      "\n",
      "🎯 === COLLECTING CONFLICT WAR PEACE DATA ===\n",
      "\n",
      "  📍 Processing r/worldnews...\n",
      "  🔍 Method: SEARCH from r/worldnews\n",
      "     Query: 'war'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/worldnews/search/?q=war&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205550500>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: SEARCH from r/worldnews\n",
      "     Query: 'conflict'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/worldnews/search/?q=conflict&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205552BA0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: SEARCH from r/worldnews\n",
      "     Query: 'military'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/worldnews/search/?q=military&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205553CE0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: HOT from r/worldnews\n",
      "     ❌ Error with hot method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/worldnews/hot?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205551AC0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: NEW from r/worldnews\n",
      "     ❌ Error with new method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/worldnews/new?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015204641A90>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  ✅ r/worldnews: 0 posts total\n",
      "\n",
      "  📍 Processing r/geopolitics...\n",
      "  🔍 Method: SEARCH from r/geopolitics\n",
      "     Query: 'war'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/geopolitics/search/?q=war&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205553830>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: SEARCH from r/geopolitics\n",
      "     Query: 'conflict'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/geopolitics/search/?q=conflict&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152053CDE80>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: SEARCH from r/geopolitics\n",
      "     Query: 'military'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/geopolitics/search/?q=military&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152055530E0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: HOT from r/geopolitics\n",
      "     ❌ Error with hot method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/geopolitics/hot?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205550F50>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: NEW from r/geopolitics\n",
      "     ❌ Error with new method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/geopolitics/new?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152053CD0A0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  ✅ r/geopolitics: 0 posts total\n",
      "\n",
      "  📍 Processing r/Military...\n",
      "  🔍 Method: SEARCH from r/Military\n",
      "     Query: 'war'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/Military/search/?q=war&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205551280>: Failed to resolve 'oauth.reddit.com' ([Errno 11002] getaddrinfo failed)\"))\n",
      "  🔍 Method: SEARCH from r/Military\n",
      "     Query: 'conflict'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/Military/search/?q=conflict&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152053CD520>: Failed to resolve 'oauth.reddit.com' ([Errno 11002] getaddrinfo failed)\"))\n",
      "  🔍 Method: SEARCH from r/Military\n",
      "     Query: 'military'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/Military/search/?q=military&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205550890>: Failed to resolve 'oauth.reddit.com' ([Errno 11002] getaddrinfo failed)\"))\n",
      "  🔍 Method: HOT from r/Military\n",
      "     ❌ Error with hot method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/Military/hot?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152053CFB00>: Failed to resolve 'oauth.reddit.com' ([Errno 11002] getaddrinfo failed)\"))\n",
      "  🔍 Method: NEW from r/Military\n",
      "     ❌ Error with new method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/Military/new?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152046719A0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  ✅ r/Military: 0 posts total\n",
      "\n",
      "  📍 Processing r/CredibleDefense...\n",
      "  🔍 Method: SEARCH from r/CredibleDefense\n",
      "     Query: 'war'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/CredibleDefense/search/?q=war&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205551E80>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: SEARCH from r/CredibleDefense\n",
      "     Query: 'conflict'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/CredibleDefense/search/?q=conflict&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152055B5B20>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: SEARCH from r/CredibleDefense\n",
      "     Query: 'military'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/CredibleDefense/search/?q=military&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205553140>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: HOT from r/CredibleDefense\n",
      "     ❌ Error with hot method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/CredibleDefense/hot?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152055B71A0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: NEW from r/CredibleDefense\n",
      "     ❌ Error with new method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/CredibleDefense/new?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152055B6F00>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  ✅ r/CredibleDefense: 0 posts total\n",
      "\n",
      "  📍 Processing r/conflict...\n",
      "  🔍 Method: SEARCH from r/conflict\n",
      "     Query: 'war'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/conflict/search/?q=war&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152055507A0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: SEARCH from r/conflict\n",
      "     Query: 'conflict'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/conflict/search/?q=conflict&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152055B41A0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: SEARCH from r/conflict\n",
      "     Query: 'military'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/conflict/search/?q=military&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152055B7320>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: HOT from r/conflict\n",
      "     ❌ Error with hot method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/conflict/hot?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152055517C0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: NEW from r/conflict\n",
      "     ❌ Error with new method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/conflict/new?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205493020>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  ✅ r/conflict: 0 posts total\n",
      "  ❌ No data collected for Conflict_War_Peace\n",
      "\n",
      "  ⏳ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 11/17: DISASTER ACCIDENT EMERGENCY\n",
      "================================================================================\n",
      "\n",
      "🎯 === COLLECTING DISASTER ACCIDENT EMERGENCY DATA ===\n",
      "\n",
      "  📍 Processing r/worldnews...\n",
      "  🔍 Method: SEARCH from r/worldnews\n",
      "     Query: 'disaster'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/worldnews/search/?q=disaster&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152053CF200>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: SEARCH from r/worldnews\n",
      "     Query: 'emergency'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/worldnews/search/?q=emergency&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152053CD2E0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: SEARCH from r/worldnews\n",
      "     Query: 'accident'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/worldnews/search/?q=accident&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015204672180>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: HOT from r/worldnews\n",
      "     ❌ Error with hot method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/worldnews/hot?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152053CE450>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: NEW from r/worldnews\n",
      "     ❌ Error with new method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/worldnews/new?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152055512B0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  ✅ r/worldnews: 0 posts total\n",
      "\n",
      "  📍 Processing r/news...\n",
      "  🔍 Method: SEARCH from r/news\n",
      "     Query: 'disaster'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/news/search/?q=disaster&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152053CDFD0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: SEARCH from r/news\n",
      "     Query: 'emergency'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/news/search/?q=emergency&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015204673620>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: SEARCH from r/news\n",
      "     Query: 'accident'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/news/search/?q=accident&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015204673620>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: HOT from r/news\n",
      "     ❌ Error with hot method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/news/hot?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152053CE510>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: NEW from r/news\n",
      "     ❌ Error with new method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/news/new?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152055501A0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  ✅ r/news: 0 posts total\n",
      "\n",
      "  📍 Processing r/CatastrophicFailure...\n",
      "  🔍 Method: SEARCH from r/CatastrophicFailure\n",
      "     Query: 'disaster'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/CatastrophicFailure/search/?q=disaster&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152053CEE40>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: SEARCH from r/CatastrophicFailure\n",
      "     Query: 'emergency'\n",
      "     ❌ Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/CatastrophicFailure/search/?q=emergency&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205550BF0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  🔍 Method: SEARCH from r/CatastrophicFailure\n",
      "     Query: 'accident'\n",
      "     ✅ Collected 34 posts using search\n",
      "     Added 34 posts from search: 'accident'\n",
      "  🔍 Method: HOT from r/CatastrophicFailure\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/CatastrophicFailure\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/CatastrophicFailure: 194 posts total\n",
      "\n",
      "  📍 Processing r/emergencyservices...\n",
      "  🔍 Method: SEARCH from r/emergencyservices\n",
      "     Query: 'disaster'\n",
      "     ✅ Collected 0 posts using search\n",
      "  🔍 Method: SEARCH from r/emergencyservices\n",
      "     Query: 'emergency'\n",
      "     ✅ Collected 2 posts using search\n",
      "     Added 2 posts from search: 'emergency'\n",
      "  🔍 Method: SEARCH from r/emergencyservices\n",
      "     Query: 'accident'\n",
      "     ✅ Collected 1 posts using search\n",
      "     Added 1 posts from search: 'accident'\n",
      "  🔍 Method: HOT from r/emergencyservices\n",
      "     ✅ Collected 22 posts using hot\n",
      "     Added 22 posts from hot\n",
      "  🔍 Method: NEW from r/emergencyservices\n",
      "     ✅ Collected 22 posts using new\n",
      "     Added 22 posts from new\n",
      "  ✅ r/emergencyservices: 47 posts total\n",
      "\n",
      "  📊 Removed 109 duplicates\n",
      "  ✅ Disaster_Accident_Emergency data saved to: reddit_data/Disaster_Accident_Emergency_20250823_002100.json\n",
      "     Posts: 132\n",
      "     Subreddits: 2\n",
      "\n",
      "  📊 DISASTER ACCIDENT EMERGENCY SUMMARY:\n",
      "     Total posts: 132\n",
      "     Posts with text: 15\n",
      "     Text ratio: 11.4%\n",
      "     Subreddits: 2\n",
      "     Average score: 949.8\n",
      "\n",
      "  ⏳ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 12/17: ENVIRONMENT\n",
      "================================================================================\n",
      "\n",
      "🎯 === COLLECTING ENVIRONMENT DATA ===\n",
      "\n",
      "  📍 Processing r/environment...\n",
      "  🔍 Method: SEARCH from r/environment\n",
      "     Query: 'environment'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'environment'\n",
      "  🔍 Method: SEARCH from r/environment\n",
      "     Query: 'climate change'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'climate change'\n",
      "  🔍 Method: SEARCH from r/environment\n",
      "     Query: 'sustainability'\n",
      "     ✅ Collected 47 posts using search\n",
      "     Added 47 posts from search: 'sustainability'\n",
      "  🔍 Method: HOT from r/environment\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ✅ r/environment: 327 posts total\n",
      "\n",
      "  📍 Processing r/climatechange...\n",
      "  🔍 Method: SEARCH from r/climatechange\n",
      "     Query: 'environment'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 77 posts using search\n",
      "     Added 77 posts from search: 'environment'\n",
      "  🔍 Method: SEARCH from r/climatechange\n",
      "     Query: 'climate change'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'climate change'\n",
      "  🔍 Method: SEARCH from r/climatechange\n",
      "     Query: 'sustainability'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 94 posts using search\n",
      "     Added 94 posts from search: 'sustainability'\n",
      "  ✅ r/climatechange: 271 posts total\n",
      "\n",
      "  📍 Processing r/sustainability...\n",
      "  🔍 Method: SEARCH from r/sustainability\n",
      "     Query: 'environment'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 61 posts using search\n",
      "     Added 61 posts from search: 'environment'\n",
      "  🔍 Method: SEARCH from r/sustainability\n",
      "     Query: 'climate change'\n",
      "     ✅ Collected 40 posts using search\n",
      "     Added 40 posts from search: 'climate change'\n",
      "  🔍 Method: SEARCH from r/sustainability\n",
      "     Query: 'sustainability'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sustainability'\n",
      "  🔍 Method: HOT from r/sustainability\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ✅ r/sustainability: 281 posts total\n",
      "\n",
      "  📍 Processing r/nature...\n",
      "  🔍 Method: SEARCH from r/nature\n",
      "     Query: 'environment'\n",
      "     ✅ Collected 10 posts using search\n",
      "     Added 10 posts from search: 'environment'\n",
      "  🔍 Method: SEARCH from r/nature\n",
      "     Query: 'climate change'\n",
      "     ✅ Collected 18 posts using search\n",
      "     Added 18 posts from search: 'climate change'\n",
      "  🔍 Method: SEARCH from r/nature\n",
      "     Query: 'sustainability'\n",
      "     ✅ Collected 5 posts using search\n",
      "     Added 5 posts from search: 'sustainability'\n",
      "  🔍 Method: HOT from r/nature\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/nature\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/nature: 193 posts total\n",
      "\n",
      "  📍 Processing r/ecology...\n",
      "  🔍 Method: SEARCH from r/ecology\n",
      "     Query: 'environment'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'environment'\n",
      "  🔍 Method: SEARCH from r/ecology\n",
      "     Query: 'climate change'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 50 posts using search\n",
      "     Added 50 posts from search: 'climate change'\n",
      "  🔍 Method: SEARCH from r/ecology\n",
      "     Query: 'sustainability'\n",
      "     ✅ Collected 46 posts using search\n",
      "     Added 46 posts from search: 'sustainability'\n",
      "  🔍 Method: HOT from r/ecology\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ✅ r/ecology: 276 posts total\n",
      "\n",
      "  📍 Processing r/green...\n",
      "  🔍 Method: SEARCH from r/green\n",
      "     Query: 'environment'\n",
      "     ✅ Collected 13 posts using search\n",
      "     Added 13 posts from search: 'environment'\n",
      "  🔍 Method: SEARCH from r/green\n",
      "     Query: 'climate change'\n",
      "     ✅ Collected 16 posts using search\n",
      "     Added 16 posts from search: 'climate change'\n",
      "  🔍 Method: SEARCH from r/green\n",
      "     Query: 'sustainability'\n",
      "     ✅ Collected 29 posts using search\n",
      "     Added 29 posts from search: 'sustainability'\n",
      "  🔍 Method: HOT from r/green\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/green\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/green: 218 posts total\n",
      "\n",
      "  📊 Removed 285 duplicates\n",
      "  ✅ Environment data saved to: reddit_data/Environment_20250823_002255.json\n",
      "     Posts: 1281\n",
      "     Subreddits: 6\n",
      "\n",
      "  📊 ENVIRONMENT SUMMARY:\n",
      "     Total posts: 1281\n",
      "     Posts with text: 519\n",
      "     Text ratio: 40.5%\n",
      "     Subreddits: 6\n",
      "     Average score: 351.0\n",
      "\n",
      "  ⏳ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 13/17: EDUCATION\n",
      "================================================================================\n",
      "\n",
      "🎯 === COLLECTING EDUCATION DATA ===\n",
      "\n",
      "  📍 Processing r/education...\n",
      "  🔍 Method: SEARCH from r/education\n",
      "     Query: 'education'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'education'\n",
      "  🔍 Method: SEARCH from r/education\n",
      "     Query: 'school'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'school'\n",
      "  🔍 Method: SEARCH from r/education\n",
      "     Query: 'university'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'university'\n",
      "  ✅ r/education: 300 posts total\n",
      "\n",
      "  📍 Processing r/teachers...\n",
      "  🔍 Method: SEARCH from r/teachers\n",
      "     Query: 'education'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'education'\n",
      "  🔍 Method: SEARCH from r/teachers\n",
      "     Query: 'school'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'school'\n",
      "  🔍 Method: SEARCH from r/teachers\n",
      "     Query: 'university'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'university'\n",
      "  ✅ r/teachers: 300 posts total\n",
      "\n",
      "  📍 Processing r/college...\n",
      "  🔍 Method: SEARCH from r/college\n",
      "     Query: 'education'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'education'\n",
      "  🔍 Method: SEARCH from r/college\n",
      "     Query: 'school'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'school'\n",
      "  🔍 Method: SEARCH from r/college\n",
      "     Query: 'university'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'university'\n",
      "  ✅ r/college: 300 posts total\n",
      "\n",
      "  📍 Processing r/university...\n",
      "  🔍 Method: SEARCH from r/university\n",
      "     Query: 'education'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'education'\n",
      "  🔍 Method: SEARCH from r/university\n",
      "     Query: 'school'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'school'\n",
      "  🔍 Method: SEARCH from r/university\n",
      "     Query: 'university'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'university'\n",
      "  ✅ r/university: 300 posts total\n",
      "\n",
      "  📍 Processing r/students...\n",
      "  🔍 Method: SEARCH from r/students\n",
      "     Query: 'education'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'education'\n",
      "  🔍 Method: SEARCH from r/students\n",
      "     Query: 'school'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'school'\n",
      "  🔍 Method: SEARCH from r/students\n",
      "     Query: 'university'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'university'\n",
      "  ✅ r/students: 300 posts total\n",
      "\n",
      "  📍 Processing r/Academia...\n",
      "  🔍 Method: SEARCH from r/Academia\n",
      "     Query: 'education'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'education'\n",
      "  🔍 Method: SEARCH from r/Academia\n",
      "     Query: 'school'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'school'\n",
      "  🔍 Method: SEARCH from r/Academia\n",
      "     Query: 'university'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'university'\n",
      "  ✅ r/Academia: 300 posts total\n",
      "\n",
      "  📊 Removed 135 duplicates\n",
      "  ✅ Education data saved to: reddit_data/Education_20250823_002444.json\n",
      "     Posts: 1665\n",
      "     Subreddits: 6\n",
      "\n",
      "  📊 EDUCATION SUMMARY:\n",
      "     Total posts: 1665\n",
      "     Posts with text: 1469\n",
      "     Text ratio: 88.2%\n",
      "     Subreddits: 6\n",
      "     Average score: 435.6\n",
      "\n",
      "  ⏳ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 14/17: LABOUR\n",
      "================================================================================\n",
      "\n",
      "🎯 === COLLECTING LABOUR DATA ===\n",
      "\n",
      "  📍 Processing r/jobs...\n",
      "  🔍 Method: SEARCH from r/jobs\n",
      "     Query: 'jobs'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'jobs'\n",
      "  🔍 Method: SEARCH from r/jobs\n",
      "     Query: 'employment'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'employment'\n",
      "  🔍 Method: SEARCH from r/jobs\n",
      "     Query: 'labor'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'labor'\n",
      "  ✅ r/jobs: 300 posts total\n",
      "\n",
      "  📍 Processing r/antiwork...\n",
      "  🔍 Method: SEARCH from r/antiwork\n",
      "     Query: 'jobs'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'jobs'\n",
      "  🔍 Method: SEARCH from r/antiwork\n",
      "     Query: 'employment'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'employment'\n",
      "  🔍 Method: SEARCH from r/antiwork\n",
      "     Query: 'labor'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'labor'\n",
      "  ✅ r/antiwork: 300 posts total\n",
      "\n",
      "  📍 Processing r/WorkReform...\n",
      "  🔍 Method: SEARCH from r/WorkReform\n",
      "     Query: 'jobs'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'jobs'\n",
      "  🔍 Method: SEARCH from r/WorkReform\n",
      "     Query: 'employment'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'employment'\n",
      "  🔍 Method: SEARCH from r/WorkReform\n",
      "     Query: 'labor'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'labor'\n",
      "  ✅ r/WorkReform: 300 posts total\n",
      "\n",
      "  📍 Processing r/labor...\n",
      "  🔍 Method: SEARCH from r/labor\n",
      "     Query: 'jobs'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 70 posts using search\n",
      "     Added 70 posts from search: 'jobs'\n",
      "  🔍 Method: SEARCH from r/labor\n",
      "     Query: 'employment'\n",
      "     ✅ Collected 40 posts using search\n",
      "     Added 40 posts from search: 'employment'\n",
      "  🔍 Method: SEARCH from r/labor\n",
      "     Query: 'labor'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'labor'\n",
      "  🔍 Method: HOT from r/labor\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ✅ r/labor: 290 posts total\n",
      "\n",
      "  📍 Processing r/union...\n",
      "  🔍 Method: SEARCH from r/union\n",
      "     Query: 'jobs'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'jobs'\n",
      "  🔍 Method: SEARCH from r/union\n",
      "     Query: 'employment'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'employment'\n",
      "  🔍 Method: SEARCH from r/union\n",
      "     Query: 'labor'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'labor'\n",
      "  ✅ r/union: 300 posts total\n",
      "\n",
      "  📍 Processing r/employment...\n",
      "  🔍 Method: SEARCH from r/employment\n",
      "     Query: 'jobs'\n",
      "     ✅ Collected 48 posts using search\n",
      "     Added 48 posts from search: 'jobs'\n",
      "  🔍 Method: SEARCH from r/employment\n",
      "     Query: 'employment'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 72 posts using search\n",
      "     Added 72 posts from search: 'employment'\n",
      "  🔍 Method: SEARCH from r/employment\n",
      "     Query: 'labor'\n",
      "     ✅ Collected 0 posts using search\n",
      "  🔍 Method: HOT from r/employment\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/employment\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/employment: 280 posts total\n",
      "\n",
      "  📊 Removed 301 duplicates\n",
      "  ✅ Labour data saved to: reddit_data/Labour_20250823_002638.json\n",
      "     Posts: 1469\n",
      "     Subreddits: 6\n",
      "\n",
      "  📊 LABOUR SUMMARY:\n",
      "     Total posts: 1469\n",
      "     Posts with text: 644\n",
      "     Text ratio: 43.8%\n",
      "     Subreddits: 6\n",
      "     Average score: 2955.2\n",
      "\n",
      "  ⏳ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 15/17: LIFESTYLE LEISURE\n",
      "================================================================================\n",
      "\n",
      "🎯 === COLLECTING LIFESTYLE LEISURE DATA ===\n",
      "\n",
      "  📍 Processing r/lifestyle...\n",
      "  🔍 Method: SEARCH from r/lifestyle\n",
      "     Query: 'lifestyle'\n",
      "     ❌ Error with search method: received 403 HTTP response\n",
      "  🔍 Method: SEARCH from r/lifestyle\n",
      "     Query: 'travel'\n",
      "     ❌ Error with search method: received 403 HTTP response\n",
      "  🔍 Method: SEARCH from r/lifestyle\n",
      "     Query: 'food'\n",
      "     ❌ Error with search method: received 403 HTTP response\n",
      "  🔍 Method: HOT from r/lifestyle\n",
      "     ❌ Error with hot method: received 403 HTTP response\n",
      "  🔍 Method: NEW from r/lifestyle\n",
      "     ❌ Error with new method: received 403 HTTP response\n",
      "  ✅ r/lifestyle: 0 posts total\n",
      "\n",
      "  📍 Processing r/travel...\n",
      "  🔍 Method: SEARCH from r/travel\n",
      "     Query: 'lifestyle'\n",
      "     ✅ Collected 48 posts using search\n",
      "     Added 48 posts from search: 'lifestyle'\n",
      "  🔍 Method: SEARCH from r/travel\n",
      "     Query: 'travel'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'travel'\n",
      "  🔍 Method: SEARCH from r/travel\n",
      "     Query: 'food'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'food'\n",
      "  ✅ r/travel: 248 posts total\n",
      "\n",
      "  📍 Processing r/food...\n",
      "  🔍 Method: SEARCH from r/food\n",
      "     Query: 'lifestyle'\n",
      "     ✅ Collected 2 posts using search\n",
      "     Added 2 posts from search: 'lifestyle'\n",
      "  🔍 Method: SEARCH from r/food\n",
      "     Query: 'travel'\n",
      "     ✅ Collected 41 posts using search\n",
      "     Added 41 posts from search: 'travel'\n",
      "  🔍 Method: SEARCH from r/food\n",
      "     Query: 'food'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'food'\n",
      "  🔍 Method: HOT from r/food\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ✅ r/food: 223 posts total\n",
      "\n",
      "  📍 Processing r/cooking...\n",
      "  🔍 Method: SEARCH from r/cooking\n",
      "     Query: 'lifestyle'\n",
      "     ✅ Collected 32 posts using search\n",
      "     Added 32 posts from search: 'lifestyle'\n",
      "  🔍 Method: SEARCH from r/cooking\n",
      "     Query: 'travel'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'travel'\n",
      "  🔍 Method: SEARCH from r/cooking\n",
      "     Query: 'food'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'food'\n",
      "  ✅ r/cooking: 232 posts total\n",
      "\n",
      "  📍 Processing r/DIY...\n",
      "  🔍 Method: SEARCH from r/DIY\n",
      "     Query: 'lifestyle'\n",
      "     ✅ Collected 7 posts using search\n",
      "     Added 7 posts from search: 'lifestyle'\n",
      "  🔍 Method: SEARCH from r/DIY\n",
      "     Query: 'travel'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'travel'\n",
      "  🔍 Method: SEARCH from r/DIY\n",
      "     Query: 'food'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'food'\n",
      "  ✅ r/DIY: 207 posts total\n",
      "\n",
      "  📍 Processing r/hobbies...\n",
      "  🔍 Method: SEARCH from r/hobbies\n",
      "     Query: 'lifestyle'\n",
      "     ✅ Collected 12 posts using search\n",
      "     Added 12 posts from search: 'lifestyle'\n",
      "  🔍 Method: SEARCH from r/hobbies\n",
      "     Query: 'travel'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 71 posts using search\n",
      "     Added 71 posts from search: 'travel'\n",
      "  🔍 Method: SEARCH from r/hobbies\n",
      "     Query: 'food'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 62 posts using search\n",
      "     Added 62 posts from search: 'food'\n",
      "  🔍 Method: HOT from r/hobbies\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ✅ r/hobbies: 225 posts total\n",
      "\n",
      "  📍 Processing r/fashion...\n",
      "  🔍 Method: SEARCH from r/fashion\n",
      "     Query: 'lifestyle'\n",
      "     ✅ Collected 2 posts using search\n",
      "     Added 2 posts from search: 'lifestyle'\n",
      "  🔍 Method: SEARCH from r/fashion\n",
      "     Query: 'travel'\n",
      "     ✅ Collected 14 posts using search\n",
      "     Added 14 posts from search: 'travel'\n",
      "  🔍 Method: SEARCH from r/fashion\n",
      "     Query: 'food'\n",
      "     ✅ Collected 14 posts using search\n",
      "     Added 14 posts from search: 'food'\n",
      "  🔍 Method: HOT from r/fashion\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/fashion\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/fashion: 190 posts total\n",
      "\n",
      "  📍 Processing r/gardening...\n",
      "  🔍 Method: SEARCH from r/gardening\n",
      "     Query: 'lifestyle'\n",
      "     ✅ Collected 15 posts using search\n",
      "     Added 15 posts from search: 'lifestyle'\n",
      "  🔍 Method: SEARCH from r/gardening\n",
      "     Query: 'travel'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'travel'\n",
      "  🔍 Method: SEARCH from r/gardening\n",
      "     Query: 'food'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'food'\n",
      "  ✅ r/gardening: 215 posts total\n",
      "\n",
      "  📊 Removed 98 duplicates\n",
      "  ✅ Lifestyle_Leisure data saved to: reddit_data/Lifestyle_Leisure_20250823_002858.json\n",
      "     Posts: 1442\n",
      "     Subreddits: 7\n",
      "\n",
      "  📊 LIFESTYLE LEISURE SUMMARY:\n",
      "     Total posts: 1442\n",
      "     Posts with text: 774\n",
      "     Text ratio: 53.7%\n",
      "     Subreddits: 7\n",
      "     Average score: 874.1\n",
      "\n",
      "  ⏳ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 16/17: WEATHER\n",
      "================================================================================\n",
      "\n",
      "🎯 === COLLECTING WEATHER DATA ===\n",
      "\n",
      "  📍 Processing r/weather...\n",
      "  🔍 Method: SEARCH from r/weather\n",
      "     Query: 'weather'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'weather'\n",
      "  🔍 Method: SEARCH from r/weather\n",
      "     Query: 'climate'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'climate'\n",
      "  🔍 Method: SEARCH from r/weather\n",
      "     Query: 'temperature'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'temperature'\n",
      "  🔍 Method: HOT from r/weather\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ✅ r/weather: 380 posts total\n",
      "\n",
      "  📍 Processing r/climate...\n",
      "  🔍 Method: SEARCH from r/climate\n",
      "     Query: 'weather'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'weather'\n",
      "  🔍 Method: SEARCH from r/climate\n",
      "     Query: 'climate'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'climate'\n",
      "  🔍 Method: SEARCH from r/climate\n",
      "     Query: 'temperature'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'temperature'\n",
      "  🔍 Method: HOT from r/climate\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ✅ r/climate: 380 posts total\n",
      "\n",
      "  📍 Processing r/meteorology...\n",
      "  🔍 Method: SEARCH from r/meteorology\n",
      "     Query: 'weather'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'weather'\n",
      "  🔍 Method: SEARCH from r/meteorology\n",
      "     Query: 'climate'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'climate'\n",
      "  🔍 Method: SEARCH from r/meteorology\n",
      "     Query: 'temperature'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'temperature'\n",
      "  🔍 Method: HOT from r/meteorology\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ✅ r/meteorology: 380 posts total\n",
      "\n",
      "  📍 Processing r/TropicalWeather...\n",
      "  🔍 Method: SEARCH from r/TropicalWeather\n",
      "     Query: 'weather'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'weather'\n",
      "  🔍 Method: SEARCH from r/TropicalWeather\n",
      "     Query: 'climate'\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 65 posts using search\n",
      "     Added 65 posts from search: 'climate'\n",
      "  🔍 Method: SEARCH from r/TropicalWeather\n",
      "     Query: 'temperature'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'temperature'\n",
      "  🔍 Method: HOT from r/TropicalWeather\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  🔍 Method: NEW from r/TropicalWeather\n",
      "     Processed 50 posts...\n",
      "     ✅ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ✅ r/TropicalWeather: 425 posts total\n",
      "\n",
      "  📊 Removed 261 duplicates\n",
      "  ✅ Weather data saved to: reddit_data/Weather_20250823_003040.json\n",
      "     Posts: 1304\n",
      "     Subreddits: 4\n",
      "\n",
      "  📊 WEATHER SUMMARY:\n",
      "     Total posts: 1304\n",
      "     Posts with text: 474\n",
      "     Text ratio: 36.3%\n",
      "     Subreddits: 4\n",
      "     Average score: 321.5\n",
      "\n",
      "  ⏳ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 17/17: RELIGION\n",
      "================================================================================\n",
      "\n",
      "🎯 === COLLECTING RELIGION DATA ===\n",
      "\n",
      "  📍 Processing r/religion...\n",
      "  🔍 Method: SEARCH from r/religion\n",
      "     Query: 'religion'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'religion'\n",
      "  🔍 Method: SEARCH from r/religion\n",
      "     Query: 'faith'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'faith'\n",
      "  🔍 Method: SEARCH from r/religion\n",
      "     Query: 'spiritual'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'spiritual'\n",
      "  ✅ r/religion: 300 posts total\n",
      "\n",
      "  📍 Processing r/Christianity...\n",
      "  🔍 Method: SEARCH from r/Christianity\n",
      "     Query: 'religion'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'religion'\n",
      "  🔍 Method: SEARCH from r/Christianity\n",
      "     Query: 'faith'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'faith'\n",
      "  🔍 Method: SEARCH from r/Christianity\n",
      "     Query: 'spiritual'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'spiritual'\n",
      "  ✅ r/Christianity: 300 posts total\n",
      "\n",
      "  📍 Processing r/islam...\n",
      "  🔍 Method: SEARCH from r/islam\n",
      "     Query: 'religion'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'religion'\n",
      "  🔍 Method: SEARCH from r/islam\n",
      "     Query: 'faith'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'faith'\n",
      "  🔍 Method: SEARCH from r/islam\n",
      "     Query: 'spiritual'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'spiritual'\n",
      "  ✅ r/islam: 300 posts total\n",
      "\n",
      "  📍 Processing r/Judaism...\n",
      "  🔍 Method: SEARCH from r/Judaism\n",
      "     Query: 'religion'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'religion'\n",
      "  🔍 Method: SEARCH from r/Judaism\n",
      "     Query: 'faith'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'faith'\n",
      "  🔍 Method: SEARCH from r/Judaism\n",
      "     Query: 'spiritual'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'spiritual'\n",
      "  ✅ r/Judaism: 300 posts total\n",
      "\n",
      "  📍 Processing r/Buddhism...\n",
      "  🔍 Method: SEARCH from r/Buddhism\n",
      "     Query: 'religion'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'religion'\n",
      "  🔍 Method: SEARCH from r/Buddhism\n",
      "     Query: 'faith'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'faith'\n",
      "  🔍 Method: SEARCH from r/Buddhism\n",
      "     Query: 'spiritual'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'spiritual'\n",
      "  ✅ r/Buddhism: 300 posts total\n",
      "\n",
      "  📍 Processing r/Hinduism...\n",
      "  🔍 Method: SEARCH from r/Hinduism\n",
      "     Query: 'religion'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'religion'\n",
      "  🔍 Method: SEARCH from r/Hinduism\n",
      "     Query: 'faith'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'faith'\n",
      "  🔍 Method: SEARCH from r/Hinduism\n",
      "     Query: 'spiritual'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'spiritual'\n",
      "  ✅ r/Hinduism: 300 posts total\n",
      "\n",
      "  📍 Processing r/atheism...\n",
      "  🔍 Method: SEARCH from r/atheism\n",
      "     Query: 'religion'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'religion'\n",
      "  🔍 Method: SEARCH from r/atheism\n",
      "     Query: 'faith'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'faith'\n",
      "  🔍 Method: SEARCH from r/atheism\n",
      "     Query: 'spiritual'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ✅ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'spiritual'\n",
      "  ✅ r/atheism: 300 posts total\n",
      "\n",
      "  📊 Removed 64 duplicates\n",
      "  ✅ Religion data saved to: reddit_data/Religion_20250823_003250.json\n",
      "     Posts: 2036\n",
      "     Subreddits: 7\n",
      "\n",
      "  📊 RELIGION SUMMARY:\n",
      "     Total posts: 2036\n",
      "     Posts with text: 1548\n",
      "     Text ratio: 76.0%\n",
      "     Subreddits: 7\n",
      "     Average score: 360.9\n",
      "\n",
      "================================================================================\n",
      "🎉 COLLECTION COMPLETE!\n",
      "================================================================================\n",
      "Categories processed: 16/17\n",
      "Total posts collected: 20,976\n",
      "Files created: 16\n",
      "\n",
      "📁 All files saved in: reddit_data/\n",
      "📊 Collection summary: reddit_data/collection_summary_20250823_003250.json\n",
      "\n",
      "✅ Successfully collected data for:\n",
      "   • Politics\n",
      "   • Economy Business Finance\n",
      "   • Science Technology\n",
      "   • Health\n",
      "   • Sport\n",
      "   • Arts Culture Entertainment Media\n",
      "   • Society\n",
      "   • Human Interest\n",
      "   • Crime Law Justice\n",
      "   • Disaster Accident Emergency\n",
      "   • Environment\n",
      "   • Education\n",
      "   • Labour\n",
      "   • Lifestyle Leisure\n",
      "   • Weather\n",
      "   • Religion\n",
      "\n",
      "🎯 Data collection completed at: 2025-08-23 00:32:50\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"xeV3sLrkl4xGyM-ScCBkSA\",\n",
    "    client_secret=\"sGrJQoAWabMp3hVcAXTxocV3gkCXoQ\",\n",
    "    user_agent=\"HeadlineHive\"\n",
    ")\n",
    "\n",
    "# Comprehensive category configuration\n",
    "CATEGORIES_CONFIG = {\n",
    "    'Politics': {\n",
    "        'subreddits': ['politics', 'PoliticalDiscussion', 'Ask_Politics', 'neutralpolitics', 'worldpolitics'],\n",
    "        'search_queries': ['politics', 'election', 'government', 'congress', 'senate', 'political'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Economy_Business_Finance': {\n",
    "        'subreddits': ['Economics', 'business', 'stocks', 'investing', 'personalfinance', 'economy', 'finance'],\n",
    "        'search_queries': ['economy', 'business', 'finance', 'stock market', 'investment', 'banking', 'GDP'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Science_Technology': {\n",
    "        'subreddits': ['technology', 'science', 'gadgets', 'tech', 'MachineLearning', 'artificial', 'programming', 'computers'],\n",
    "        'search_queries': ['technology', 'science', 'innovation', 'research', 'AI', 'machine learning', 'tech'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Health': {\n",
    "        'subreddits': ['Health', 'medicine', 'medical', 'healthcare', 'nutrition', 'fitness', 'mentalhealth'],\n",
    "        'search_queries': ['health', 'medicine', 'medical', 'healthcare', 'disease', 'treatment', 'wellness'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Sport': {\n",
    "        'subreddits': ['sports', 'nfl', 'nba', 'soccer', 'baseball', 'hockey', 'olympics', 'football'],\n",
    "        'search_queries': ['sports', 'football', 'basketball', 'soccer', 'baseball', 'olympics', 'athletics'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Arts_Culture_Entertainment_Media': {\n",
    "        'subreddits': ['movies', 'music', 'art', 'books', 'television', 'entertainment', 'culture', 'media'],\n",
    "        'search_queries': ['movies', 'music', 'art', 'books', 'entertainment', 'culture', 'media', 'celebrity'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Society': {\n",
    "        'subreddits': ['sociology', 'society', 'social', 'community', 'TrueReddit', 'news'],\n",
    "        'search_queries': ['society', 'social', 'community', 'culture', 'demographics', 'population'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Human_Interest': {\n",
    "        'subreddits': ['UpliftingNews', 'MadeMeSmile', 'happy', 'wholesome', 'HumansBeingBros', 'todayilearned'],\n",
    "        'search_queries': ['human interest', 'inspiring', 'heartwarming', 'amazing', 'incredible', 'touching'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Crime_Law_Justice': {\n",
    "        'subreddits': ['law', 'legaladvice', 'UnresolvedMysteries', 'TrueCrime', 'justice', 'police'],\n",
    "        'search_queries': ['crime', 'law', 'justice', 'court', 'legal', 'police', 'arrest', 'trial'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Conflict_War_Peace': {\n",
    "        'subreddits': ['worldnews', 'geopolitics', 'Military', 'CredibleDefense', 'conflict'],\n",
    "        'search_queries': ['war', 'conflict', 'military', 'peace', 'international', 'defense', 'security'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Disaster_Accident_Emergency': {\n",
    "        'subreddits': ['worldnews', 'news', 'CatastrophicFailure', 'emergencyservices'],\n",
    "        'search_queries': ['disaster', 'emergency', 'accident', 'earthquake', 'flood', 'fire', 'rescue'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Environment': {\n",
    "        'subreddits': ['environment', 'climatechange', 'sustainability', 'nature', 'ecology', 'green'],\n",
    "        'search_queries': ['environment', 'climate change', 'sustainability', 'ecology', 'pollution', 'conservation'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Education': {\n",
    "        'subreddits': ['education', 'teachers', 'college', 'university', 'students', 'Academia'],\n",
    "        'search_queries': ['education', 'school', 'university', 'college', 'learning', 'academic', 'student'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Labour': {\n",
    "        'subreddits': ['jobs', 'antiwork', 'WorkReform', 'labor', 'union', 'employment'],\n",
    "        'search_queries': ['jobs', 'employment', 'labor', 'union', 'workplace', 'workers', 'career'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Lifestyle_Leisure': {\n",
    "        'subreddits': ['lifestyle', 'travel', 'food', 'cooking', 'DIY', 'hobbies', 'fashion', 'gardening'],\n",
    "        'search_queries': ['lifestyle', 'travel', 'food', 'cooking', 'hobbies', 'leisure', 'fashion'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Weather': {\n",
    "        'subreddits': ['weather', 'climate', 'meteorology', 'TropicalWeather'],\n",
    "        'search_queries': ['weather', 'climate', 'temperature', 'storm', 'hurricane', 'meteorology'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Religion': {\n",
    "        'subreddits': ['religion', 'Christianity', 'islam', 'Judaism', 'Buddhism', 'Hinduism', 'atheism'],\n",
    "        'search_queries': ['religion', 'faith', 'spiritual', 'church', 'religious', 'belief', 'worship'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    }\n",
    "}\n",
    "\n",
    "def collect_reddit_posts(subreddit_name, search_query=None, limit=300, method=\"search\", topic_category=\"general\"):\n",
    "    \"\"\"\n",
    "    Collect Reddit data with proper text extraction\n",
    "    \"\"\"\n",
    "    print(f\"  🔍 Method: {method.upper()} from r/{subreddit_name}\")\n",
    "    if search_query:\n",
    "        print(f\"     Query: '{search_query}'\")\n",
    "    \n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "    \n",
    "    try:\n",
    "        # Choose method\n",
    "        if method == \"search\" and search_query:\n",
    "            submissions = subreddit.search(search_query, limit=limit, time_filter='year')\n",
    "        elif method == \"hot\":\n",
    "            submissions = subreddit.hot(limit=limit)\n",
    "        elif method == \"new\":\n",
    "            submissions = subreddit.new(limit=limit)\n",
    "        elif method == \"top\":\n",
    "            submissions = subreddit.top(limit=limit, time_filter='month')\n",
    "        else:\n",
    "            print(f\"     Unknown method: {method}\")\n",
    "            return []\n",
    "        \n",
    "        for i, submission in enumerate(submissions):\n",
    "            try:\n",
    "                # Extract text content properly\n",
    "                text_content = \"\"\n",
    "                \n",
    "                # For self posts (text posts)\n",
    "                if submission.is_self and submission.selftext:\n",
    "                    text_content = submission.selftext.strip()\n",
    "                \n",
    "                # For link posts, we might want the URL or title as content\n",
    "                elif not submission.is_self:\n",
    "                    text_content = f\"Link post: {submission.url}\"\n",
    "                \n",
    "                # Sometimes selftext exists but is empty/whitespace\n",
    "                if not text_content and hasattr(submission, 'selftext'):\n",
    "                    if submission.selftext and submission.selftext.strip():\n",
    "                        text_content = submission.selftext.strip()\n",
    "                \n",
    "                # If still no content, use title as fallback\n",
    "                if not text_content:\n",
    "                    text_content = f\"[Title only] {submission.title}\"\n",
    "                \n",
    "                post_data = {\n",
    "                    'id': submission.id,\n",
    "                    'title': submission.title if submission.title else \"No title\",\n",
    "                    'text': text_content,\n",
    "                    'author': str(submission.author) if submission.author else '[deleted]',\n",
    "                    'subreddit': submission.subreddit.display_name,\n",
    "                    'score': submission.score if submission.score else 0,\n",
    "                    'upvote_ratio': getattr(submission, 'upvote_ratio', 0),\n",
    "                    'comments': submission.num_comments if submission.num_comments else 0,\n",
    "                    'url': submission.url if submission.url else \"\",\n",
    "                    'is_self': submission.is_self,\n",
    "                    'created': datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'permalink': f\"https://reddit.com{submission.permalink}\",\n",
    "                    'method_collected': method,\n",
    "                    'category': topic_category,\n",
    "                    'flair': submission.link_flair_text if hasattr(submission, 'link_flair_text') else None,\n",
    "                    'nsfw': submission.over_18 if hasattr(submission, 'over_18') else False\n",
    "                }\n",
    "                \n",
    "                posts.append(post_data)\n",
    "                \n",
    "                if (i + 1) % 50 == 0:\n",
    "                    print(f\"     Processed {i + 1} posts...\")\n",
    "                    time.sleep(0.2)  # Rate limiting\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"     Error processing post {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"     ✅ Collected {len(posts)} posts using {method}\")\n",
    "        return posts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"     ❌ Error with {method} method: {e}\")\n",
    "        return []\n",
    "\n",
    "def collect_category_data(category_name, category_config, max_posts_per_category=2000):\n",
    "    \"\"\"\n",
    "    Collect data for a specific category\n",
    "    \"\"\"\n",
    "    print(f\"\\n🎯 === COLLECTING {category_name.upper().replace('_', ' ')} DATA ===\")\n",
    "    category_posts = []\n",
    "    \n",
    "    subreddits = category_config['subreddits']\n",
    "    search_queries = category_config['search_queries']\n",
    "    methods = category_config['methods']\n",
    "    \n",
    "    # Limit search queries to avoid too many API calls\n",
    "    limited_queries = search_queries[:3]  # Use only first 3 queries\n",
    "    posts_per_subreddit = max_posts_per_category // len(subreddits)\n",
    "    \n",
    "    for subreddit in subreddits:\n",
    "        print(f\"\\n  📍 Processing r/{subreddit}...\")\n",
    "        subreddit_posts = []\n",
    "        \n",
    "        try:\n",
    "            # Try search method with limited queries\n",
    "            if 'search' in methods and limited_queries:\n",
    "                for query in limited_queries:\n",
    "                    if len(subreddit_posts) >= posts_per_subreddit:\n",
    "                        break\n",
    "                    \n",
    "                    search_posts = collect_reddit_posts(\n",
    "                        subreddit, query, limit=100, method=\"search\", topic_category=category_name\n",
    "                    )\n",
    "                    if search_posts:\n",
    "                        subreddit_posts.extend(search_posts)\n",
    "                        print(f\"     Added {len(search_posts)} posts from search: '{query}'\")\n",
    "                    time.sleep(1)  # Rate limiting between queries\n",
    "            \n",
    "            # Try hot posts method\n",
    "            if 'hot' in methods and len(subreddit_posts) < posts_per_subreddit:\n",
    "                time.sleep(1)\n",
    "                hot_posts = collect_reddit_posts(\n",
    "                    subreddit, None, limit=80, method=\"hot\", topic_category=category_name\n",
    "                )\n",
    "                if hot_posts:\n",
    "                    subreddit_posts.extend(hot_posts)\n",
    "                    print(f\"     Added {len(hot_posts)} posts from hot\")\n",
    "            \n",
    "            # Try new posts method\n",
    "            if 'new' in methods and len(subreddit_posts) < posts_per_subreddit:\n",
    "                time.sleep(1)\n",
    "                new_posts = collect_reddit_posts(\n",
    "                    subreddit, None, limit=80, method=\"new\", topic_category=category_name\n",
    "                )\n",
    "                if new_posts:\n",
    "                    subreddit_posts.extend(new_posts)\n",
    "                    print(f\"     Added {len(new_posts)} posts from new\")\n",
    "            \n",
    "            category_posts.extend(subreddit_posts)\n",
    "            print(f\"  ✅ r/{subreddit}: {len(subreddit_posts)} posts total\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error with r/{subreddit}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        time.sleep(3)  # Rate limiting between subreddits\n",
    "    \n",
    "    return category_posts\n",
    "\n",
    "def remove_duplicates(all_posts):\n",
    "    \"\"\"Remove duplicate posts based on ID\"\"\"\n",
    "    seen_ids = set()\n",
    "    unique_posts = []\n",
    "    \n",
    "    for post in all_posts:\n",
    "        if post['id'] not in seen_ids:\n",
    "            unique_posts.append(post)\n",
    "            seen_ids.add(post['id'])\n",
    "    \n",
    "    return unique_posts\n",
    "\n",
    "def save_category_json(posts, category_name, output_dir=\"reddit_data\"):\n",
    "    \"\"\"Save category posts to separate JSON file\"\"\"\n",
    "    if not posts:\n",
    "        print(f\"  ❌ No posts to save for {category_name}!\")\n",
    "        return False\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    try:\n",
    "        # Create comprehensive data structure for this category\n",
    "        output_data = {\n",
    "            \"metadata\": {\n",
    "                \"category\": category_name.replace('_', ' '),\n",
    "                \"total_posts\": len(posts),\n",
    "                \"collection_date\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"methods_used\": list(set([post.get('method_collected', 'unknown') for post in posts])),\n",
    "                \"subreddits\": list(set([post['subreddit'] for post in posts])),\n",
    "                \"date_range\": {\n",
    "                    \"earliest\": min([post['created'] for post in posts]),\n",
    "                    \"latest\": max([post['created'] for post in posts])\n",
    "                },\n",
    "                \"text_statistics\": {\n",
    "                    \"posts_with_text\": len([p for p in posts if p['text'] and not p['text'].startswith('[Title only]') and not p['text'].startswith('Link post:')]),\n",
    "                    \"average_score\": sum([p['score'] for p in posts]) / len(posts),\n",
    "                    \"total_comments\": sum([p['comments'] for p in posts])\n",
    "                }\n",
    "            },\n",
    "            \"posts\": posts\n",
    "        }\n",
    "        \n",
    "        # Save to category-specific file\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"{output_dir}/{category_name}_{timestamp}.json\"\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"  ✅ {category_name} data saved to: {filename}\")\n",
    "        print(f\"     Posts: {len(posts)}\")\n",
    "        print(f\"     Subreddits: {len(output_data['metadata']['subreddits'])}\")\n",
    "        return filename\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error saving {category_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def display_category_summary(posts, category_name):\n",
    "    \"\"\"Display summary for a category\"\"\"\n",
    "    if not posts:\n",
    "        return\n",
    "    \n",
    "    posts_with_text = [p for p in posts if p['text'] and not p['text'].startswith('[Title only]') and not p['text'].startswith('Link post:')]\n",
    "    \n",
    "    print(f\"\\n  📊 {category_name.replace('_', ' ').upper()} SUMMARY:\")\n",
    "    print(f\"     Total posts: {len(posts)}\")\n",
    "    print(f\"     Posts with text: {len(posts_with_text)}\")\n",
    "    print(f\"     Text ratio: {len(posts_with_text)/len(posts)*100:.1f}%\")\n",
    "    print(f\"     Subreddits: {len(set([p['subreddit'] for p in posts]))}\")\n",
    "    print(f\"     Average score: {sum([p['score'] for p in posts]) / len(posts):.1f}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 REDDIT COMPREHENSIVE DATA COLLECTION\")\n",
    "    print(\"17 Categories - Separate JSON Files\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Test Reddit connection\n",
    "    try:\n",
    "        print(\"🔌 Testing Reddit connection...\")\n",
    "        test_sub = reddit.subreddit('python')\n",
    "        print(f\"✅ Connected! Test subreddit: {test_sub.display_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Reddit connection failed: {e}\")\n",
    "        print(\"Check your Reddit API credentials!\")\n",
    "        exit()\n",
    "    \n",
    "    # Create summary for all categories\n",
    "    collection_summary = {\n",
    "        \"collection_date\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"categories_collected\": [],\n",
    "        \"total_posts\": 0,\n",
    "        \"files_created\": []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n📋 CATEGORIES TO COLLECT ({len(CATEGORIES_CONFIG)}):\")\n",
    "    for i, category in enumerate(CATEGORIES_CONFIG.keys(), 1):\n",
    "        print(f\"  {i:2d}. {category.replace('_', ' ')}\")\n",
    "    \n",
    "    # Collect data for each category\n",
    "    for i, (category_name, category_config) in enumerate(CATEGORIES_CONFIG.items(), 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CATEGORY {i}/{len(CATEGORIES_CONFIG)}: {category_name.replace('_', ' ').upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            # Collect category data\n",
    "            category_posts = collect_category_data(category_name, category_config, max_posts_per_category=1500)\n",
    "            \n",
    "            if category_posts:\n",
    "                # Remove duplicates\n",
    "                unique_posts = remove_duplicates(category_posts)\n",
    "                print(f\"\\n  📊 Removed {len(category_posts) - len(unique_posts)} duplicates\")\n",
    "                \n",
    "                # Save to separate JSON file\n",
    "                filename = save_category_json(unique_posts, category_name)\n",
    "                \n",
    "                if filename:\n",
    "                    # Update summary\n",
    "                    collection_summary[\"categories_collected\"].append(category_name)\n",
    "                    collection_summary[\"total_posts\"] += len(unique_posts)\n",
    "                    collection_summary[\"files_created\"].append(filename)\n",
    "                    \n",
    "                    # Display summary\n",
    "                    display_category_summary(unique_posts, category_name)\n",
    "                \n",
    "            else:\n",
    "                print(f\"  ❌ No data collected for {category_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error processing {category_name}: {e}\")\n",
    "        \n",
    "        # Wait between categories to be respectful\n",
    "        if i < len(CATEGORIES_CONFIG):\n",
    "            wait_time = 30  # 30 seconds between categories\n",
    "            print(f\"\\n  ⏳ Waiting {wait_time} seconds before next category...\")\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    # Save collection summary\n",
    "    try:\n",
    "        summary_file = f\"reddit_data/collection_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(collection_summary, f, indent=2, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving summary: {e}\")\n",
    "    \n",
    "    # Final results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"🎉 COLLECTION COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Categories processed: {len(collection_summary['categories_collected'])}/{len(CATEGORIES_CONFIG)}\")\n",
    "    print(f\"Total posts collected: {collection_summary['total_posts']:,}\")\n",
    "    print(f\"Files created: {len(collection_summary['files_created'])}\")\n",
    "    print(f\"\\n📁 All files saved in: reddit_data/\")\n",
    "    print(f\"📊 Collection summary: {summary_file}\")\n",
    "    \n",
    "    if collection_summary['categories_collected']:\n",
    "        print(f\"\\n✅ Successfully collected data for:\")\n",
    "        for category in collection_summary['categories_collected']:\n",
    "            print(f\"   • {category.replace('_', ' ')}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Data collection completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa4351e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Using cached praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting prawcore<3,>=2.4 (from praw)\n",
      "  Using cached prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting update_checker>=0.18 (from praw)\n",
      "  Using cached update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\yagne\\miniconda3\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\yagne\\miniconda3\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yagne\\miniconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yagne\\miniconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yagne\\miniconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yagne\\miniconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.12.14)\n",
      "Using cached praw-7.8.1-py3-none-any.whl (189 kB)\n",
      "Using cached prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
      "Using cached update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Installing collected packages: update_checker, prawcore, praw\n",
      "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"
     ]
    }
   ],
   "source": [
    "! pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68c378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
