{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed2954c",
   "metadata": {},
   "source": [
    "# Twitter scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eb5bc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22AI%20OR%20Artificial%20Intelligence%20since%3A2025-08-15%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D: blocked (404)\n",
      "4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22AI%20OR%20Artificial%20Intelligence%20since%3A2025-08-15%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up.\n",
      "Errors: blocked (404), blocked (404), blocked (404), blocked (404)\n"
     ]
    },
    {
     "ename": "ScraperException",
     "evalue": "4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22AI%20OR%20Artificial%20Intelligence%20since%3A2025-08-15%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mScraperException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(tweets)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m df_twitter \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_twitter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAI OR Artificial Intelligence since:2025-08-15\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_twitter\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m, in \u001b[0;36mscrape_twitter\u001b[1;34m(query, limit)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mscrape_twitter\u001b[39m(query, limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m      5\u001b[0m     tweets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, tweet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sntwitter\u001b[38;5;241m.\u001b[39mTwitterSearchScraper(query)\u001b[38;5;241m.\u001b[39mget_items()):\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m limit:\n\u001b[0;32m      8\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\snscrape\\modules\\twitter.py:1763\u001b[0m, in \u001b[0;36mTwitterSearchScraper.get_items\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1760\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m'\u001b[39m: variables, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m: features}\n\u001b[0;32m   1761\u001b[0m paginationParams \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m'\u001b[39m: paginationVariables, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m: features}\n\u001b[1;32m-> 1763\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_api_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline\u001b[39m\u001b[38;5;124m'\u001b[39m, _TwitterAPIType\u001b[38;5;241m.\u001b[39mGRAPHQL, params, paginationParams, cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor, instructionsPath \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_by_raw_query\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_timeline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m   1764\u001b[0m \t\u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graphql_timeline_instructions_to_tweets(obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_by_raw_query\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_timeline\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeline\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\snscrape\\modules\\twitter.py:915\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._iter_api_data\u001b[1;34m(self, endpoint, apiType, params, paginationParams, cursor, direction, instructionsPath)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetrieving scroll page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcursor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 915\u001b[0m \tobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_api_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreqParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    916\u001b[0m \t\u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    918\u001b[0m \t\u001b[38;5;66;03m# No data format test, just a hard and loud crash if anything's wrong :-)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\snscrape\\modules\\twitter.py:886\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._get_api_data\u001b[1;34m(self, endpoint, apiType, params, instructionsPath)\u001b[0m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m apiType \u001b[38;5;129;01mis\u001b[39;00m _TwitterAPIType\u001b[38;5;241m.\u001b[39mGRAPHQL:\n\u001b[0;32m    885\u001b[0m \tparams \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39murlencode({k: json\u001b[38;5;241m.\u001b[39mdumps(v, separators \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems()}, quote_via \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39mquote)\n\u001b[1;32m--> 886\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apiHeaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponseOkCallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfunctools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_api_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39m_snscrapeObj\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\snscrape\\base.py:275\u001b[0m, in \u001b[0;36mScraper._get\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 275\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\snscrape\\base.py:271\u001b[0m, in \u001b[0;36mScraper._request\u001b[1;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[0m\n\u001b[0;32m    269\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39mfatal(msg)\n\u001b[0;32m    270\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39mfatal(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mErrors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(errors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 271\u001b[0m \t\u001b[38;5;28;01mraise\u001b[39;00m ScraperException(msg)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReached unreachable code\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mScraperException\u001b[0m: 4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22AI%20OR%20Artificial%20Intelligence%20since%3A2025-08-15%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up."
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_twitter(query, limit=50):\n",
    "    tweets = []\n",
    "    for i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
    "        if i >= limit:\n",
    "            break\n",
    "        tweets.append({\n",
    "            \"date\": tweet.date,\n",
    "            \"id\": tweet.id,\n",
    "            \"content\": tweet.content,\n",
    "            \"username\": tweet.user.username,\n",
    "            \"retweets\": tweet.retweetCount,\n",
    "            \"likes\": tweet.likeCount,\n",
    "        })\n",
    "    return pd.DataFrame(tweets)\n",
    "\n",
    "# Example usage\n",
    "df_twitter = scrape_twitter(\"AI OR Artificial Intelligence since:2025-08-15\")\n",
    "print(df_twitter.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82987048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snscrape"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached snscrape-0.7.0.20230622-py3-none-any.whl (74 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Collecting lxml\n",
      "  Using cached lxml-6.0.0-cp310-cp310-win_amd64.whl (4.0 MB)\n",
      "Collecting requests[socks]\n",
      "  Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Collecting typing-extensions>=4.0.0\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Using cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.4.3-cp310-cp310-win_amd64.whl (107 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, soupsieve, PySocks, lxml, idna, filelock, charset_normalizer, certifi, requests, beautifulsoup4, snscrape\n",
      "Successfully installed PySocks-1.7.1 beautifulsoup4-4.13.4 certifi-2025.8.3 charset_normalizer-3.4.3 filelock-3.19.1 idna-3.10 lxml-6.0.0 requests-2.32.5 snscrape-0.7.0.20230622 soupsieve-2.7 typing-extensions-4.14.1 urllib3-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install snscrape\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe0843b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FileFinder' object has no attribute 'find_module'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msnscrape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtwitter\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msntwitter\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([tweet\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m sntwitter\u001b[38;5;241m.\u001b[39mTwitterSearchScraper(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mget_items()])\n",
      "File \u001b[1;32mc:\\Users\\yagne\\miniconda3\\Lib\\site-packages\\snscrape\\modules\\__init__.py:17\u001b[0m\n\u001b[0;32m     13\u001b[0m \t\tmodule \u001b[38;5;241m=\u001b[39m importer\u001b[38;5;241m.\u001b[39mfind_module(moduleName)\u001b[38;5;241m.\u001b[39mload_module(moduleName)\n\u001b[0;32m     14\u001b[0m \t\t\u001b[38;5;28mglobals\u001b[39m()[moduleNameWithoutPrefix] \u001b[38;5;241m=\u001b[39m module\n\u001b[1;32m---> 17\u001b[0m \u001b[43m_import_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yagne\\miniconda3\\Lib\\site-packages\\snscrape\\modules\\__init__.py:13\u001b[0m, in \u001b[0;36m_import_modules\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m moduleNameWithoutPrefix \u001b[38;5;241m=\u001b[39m moduleName[prefixLen:]\n\u001b[0;32m     12\u001b[0m __all__\u001b[38;5;241m.\u001b[39mappend(moduleNameWithoutPrefix)\n\u001b[1;32m---> 13\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimporter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_module\u001b[49m(moduleName)\u001b[38;5;241m.\u001b[39mload_module(moduleName)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mglobals\u001b[39m()[moduleNameWithoutPrefix] \u001b[38;5;241m=\u001b[39m module\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FileFinder' object has no attribute 'find_module'"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([tweet.content for tweet in sntwitter.TwitterSearchScraper(\"AI\").get_items()])\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef307b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89be22de",
   "metadata": {},
   "source": [
    "# Reddit Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d0e87c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Politics from r/politics...\n",
      "Scraping Economy from r/economics...\n",
      "Scraping Health from r/health...\n",
      "Scraping Sport from r/sports...\n",
      "Scraping Technology from r/technology...\n",
      "Scraping Entertainment from r/movies...\n",
      "Scraping Society from r/worldnews...\n",
      "Saved scraping results to reddit_data.csv\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"xeV3sLrkl4xGyM-ScCBkSA\",\n",
    "    client_secret=\"sGrJQoAWabMp3hVcAXTxocV3gkCXoQ\",\n",
    "    user_agent=\"HeadlineHive\"\n",
    ")\n",
    "\n",
    "def scrape_reddit(subreddit, limit=100):\n",
    "    posts = []\n",
    "    for post in reddit.subreddit(subreddit).hot(limit=limit):\n",
    "        posts.append({\n",
    "            \"title\": post.title,\n",
    "            \"text\": post.selftext,\n",
    "            \"score\": post.score,\n",
    "            \"subreddit\": subreddit\n",
    "        })\n",
    "    return pd.DataFrame(posts)\n",
    "\n",
    "subs = {\n",
    "    \"Politics\": \"politics\",\n",
    "    \"Economy\": \"economics\",\n",
    "    \"Health\": \"health\",\n",
    "    \"Sport\": \"sports\",\n",
    "    \"Technology\": \"technology\",\n",
    "    \"Entertainment\": \"movies\",\n",
    "    \"Society\": \"worldnews\"\n",
    "}\n",
    "\n",
    "all_reddit = pd.DataFrame()\n",
    "for cat, sub in subs.items():\n",
    "    print(f\"Scraping {cat} from r/{sub}...\")\n",
    "    df = scrape_reddit(sub, limit=200)\n",
    "    df[\"category\"] = cat\n",
    "    all_reddit = pd.concat([all_reddit, df], ignore_index=True)\n",
    "\n",
    "all_reddit.to_csv(\"reddit_data.csv\", index=False)\n",
    "print(\"Saved scraping results to reddit_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc3073b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Reddit data collection with multiple methods...\n",
      "\n",
      "==================================================\n",
      "üîç Method 1: Searching r/politics for 'politics'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 100 posts...\n",
      "Collected 200 posts...\n",
      "‚úÖ Method 1 Success: 234 posts\n",
      "\n",
      "==================================================\n",
      "üî• Method 2: Getting ['hot', 'new', 'top'] posts from r/politics...\n",
      "Fetching hot posts...\n",
      "Got 200 hot posts\n",
      "Fetching new posts...\n",
      "Got 200 new posts\n",
      "Fetching top posts...\n",
      "Got 200 top posts\n",
      "‚úÖ Method 2 Success: 600 posts\n",
      "After keyword filtering: 44 posts\n",
      "\n",
      "==================================================\n",
      "üì° Method 3: Using Reddit JSON API for r/politics...\n",
      "Collected 25 posts via JSON API...\n",
      "Collected 50 posts via JSON API...\n",
      "Collected 75 posts via JSON API...\n",
      "Collected 100 posts via JSON API...\n",
      "Collected 125 posts via JSON API...\n",
      "Collected 150 posts via JSON API...\n",
      "Collected 175 posts via JSON API...\n",
      "Collected 200 posts via JSON API...\n",
      "‚úÖ Method 3 Success: 200 posts\n",
      "\n",
      "==================================================\n",
      "üéØ Method 4: Collecting from multiple subreddits...\n",
      "Processing r/politics...\n",
      "üîç Method 1: Searching r/politics for 'politics'...\n",
      "Collected 100 posts...\n",
      "Collected 200 posts...\n",
      "Got 200 posts from r/politics\n",
      "Processing r/PoliticalDiscussion...\n",
      "üîç Method 1: Searching r/PoliticalDiscussion for 'politics'...\n",
      "Collected 100 posts...\n",
      "Collected 200 posts...\n",
      "Got 200 posts from r/PoliticalDiscussion\n",
      "Processing r/neutralpolitics...\n",
      "üîç Method 1: Searching r/neutralpolitics for 'politics'...\n",
      "Collected 100 posts...\n",
      "Collected 200 posts...\n",
      "Got 200 posts from r/neutralpolitics\n",
      "Processing r/Ask_Politics...\n",
      "üîç Method 1: Searching r/Ask_Politics for 'politics'...\n",
      "Collected 100 posts...\n",
      "Collected 200 posts...\n",
      "Got 200 posts from r/Ask_Politics\n",
      "‚úÖ Method 4 Success: 800 posts from multiple subreddits\n",
      "\n",
      "üéâ FINAL RESULT: 1051 unique posts collected!\n",
      "Date range: 2012-06-01 05:28:02 to 2025-08-21 12:06:17\n",
      "üíæ Data saved to: reddit_politics_data_20250821_174542.csv\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas.io.formats.string'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 275\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;66;03m# Show sample\u001b[39;00m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSample data:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 275\u001b[0m     \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauthor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcreated_utc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚ùå All methods failed. This might be due to:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\pandas\\core\\frame.py:1214\u001b[0m, in \u001b[0;36m__repr__\u001b[1;34m(self)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\pandas\\core\\frame.py:1394\u001b[0m, in \u001b[0;36mto_string\u001b[1;34m(self, buf, columns, col_space, header, index, na_rep, formatters, float_format, sparsify, index_names, justify, max_rows, max_cols, show_dimensions, decimal, line_width, min_rows, max_colwidth, encoding)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\pandas\\io\\formats\\format.py:959\u001b[0m, in \u001b[0;36mto_string\u001b[1;34m(self, buf, encoding, line_width)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas.io.formats.string'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import praw\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"xeV3sLrkl4xGyM-ScCBkSA\",\n",
    "    client_secret=\"sGrJQoAWabMp3hVcAXTxocV3gkCXoQ\",\n",
    "    user_agent=\"HeadlineHive\"\n",
    ")\n",
    "\n",
    "def method_1_praw_search(subreddit_name, query, limit=1000, time_filter='all'):\n",
    "    \"\"\"\n",
    "    Method 1: PRAW Search (Most Reliable)\n",
    "    Uses Reddit's official API through PRAW\n",
    "    \"\"\"\n",
    "    print(f\"üîç Method 1: Searching r/{subreddit_name} for '{query}'...\")\n",
    "    \n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "    \n",
    "    try:\n",
    "        # Search posts\n",
    "        for submission in subreddit.search(query, limit=limit, time_filter=time_filter):\n",
    "            posts.append({\n",
    "                \"id\": submission.id,\n",
    "                \"title\": submission.title,\n",
    "                \"selftext\": submission.selftext,\n",
    "                \"author\": str(submission.author) if submission.author else \"[deleted]\",\n",
    "                \"subreddit\": submission.subreddit.display_name,\n",
    "                \"created_utc\": pd.to_datetime(submission.created_utc, unit='s'),\n",
    "                \"url\": submission.url,\n",
    "                \"score\": submission.score,\n",
    "                \"num_comments\": submission.num_comments,\n",
    "                \"upvote_ratio\": submission.upvote_ratio,\n",
    "                \"is_self\": submission.is_self,\n",
    "                \"permalink\": f\"https://reddit.com{submission.permalink}\"\n",
    "            })\n",
    "            \n",
    "            if len(posts) % 100 == 0:\n",
    "                print(f\"Collected {len(posts)} posts...\")\n",
    "                time.sleep(0.1)  # Be gentle with API\n",
    "                \n",
    "        return pd.DataFrame(posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"PRAW search error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def method_2_praw_hot_new_top(subreddit_name, categories=['hot', 'new', 'top'], limit_per_category=500):\n",
    "    \"\"\"\n",
    "    Method 2: PRAW Hot/New/Top Posts\n",
    "    Gets recent popular posts from specific categories\n",
    "    \"\"\"\n",
    "    print(f\"üî• Method 2: Getting {categories} posts from r/{subreddit_name}...\")\n",
    "    \n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    all_posts = []\n",
    "    \n",
    "    for category in categories:\n",
    "        print(f\"Fetching {category} posts...\")\n",
    "        posts = []\n",
    "        \n",
    "        try:\n",
    "            if category == 'hot':\n",
    "                submissions = subreddit.hot(limit=limit_per_category)\n",
    "            elif category == 'new':\n",
    "                submissions = subreddit.new(limit=limit_per_category)\n",
    "            elif category == 'top':\n",
    "                submissions = subreddit.top(time_filter='month', limit=limit_per_category)\n",
    "            elif category == 'rising':\n",
    "                submissions = subreddit.rising(limit=limit_per_category)\n",
    "            \n",
    "            for submission in submissions:\n",
    "                posts.append({\n",
    "                    \"id\": submission.id,\n",
    "                    \"title\": submission.title,\n",
    "                    \"selftext\": submission.selftext,\n",
    "                    \"author\": str(submission.author) if submission.author else \"[deleted]\",\n",
    "                    \"subreddit\": submission.subreddit.display_name,\n",
    "                    \"created_utc\": pd.to_datetime(submission.created_utc, unit='s'),\n",
    "                    \"url\": submission.url,\n",
    "                    \"score\": submission.score,\n",
    "                    \"num_comments\": submission.num_comments,\n",
    "                    \"upvote_ratio\": submission.upvote_ratio,\n",
    "                    \"category\": category,\n",
    "                    \"permalink\": f\"https://reddit.com{submission.permalink}\"\n",
    "                })\n",
    "                \n",
    "            print(f\"Got {len(posts)} {category} posts\")\n",
    "            all_posts.extend(posts)\n",
    "            time.sleep(1)  # Rate limiting\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {category} posts: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(all_posts)\n",
    "\n",
    "def method_3_reddit_json_api(subreddit_name, category='hot', limit=100):\n",
    "    \"\"\"\n",
    "    Method 3: Direct Reddit JSON API\n",
    "    Uses Reddit's JSON endpoints (no authentication required)\n",
    "    \"\"\"\n",
    "    print(f\"üì° Method 3: Using Reddit JSON API for r/{subreddit_name}...\")\n",
    "    \n",
    "    base_url = f\"https://www.reddit.com/r/{subreddit_name}/{category}.json\"\n",
    "    posts = []\n",
    "    after = None\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'HeadlineHive/1.0'\n",
    "    }\n",
    "    \n",
    "    while len(posts) < limit:\n",
    "        params = {'limit': 25}\n",
    "        if after:\n",
    "            params['after'] = after\n",
    "            \n",
    "        try:\n",
    "            response = requests.get(base_url, params=params, headers=headers)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                children = data['data']['children']\n",
    "                \n",
    "                if not children:\n",
    "                    break\n",
    "                \n",
    "                for child in children:\n",
    "                    post = child['data']\n",
    "                    posts.append({\n",
    "                        \"id\": post.get('id'),\n",
    "                        \"title\": post.get('title'),\n",
    "                        \"selftext\": post.get('selftext'),\n",
    "                        \"author\": post.get('author'),\n",
    "                        \"subreddit\": post.get('subreddit'),\n",
    "                        \"created_utc\": pd.to_datetime(post.get('created_utc'), unit='s'),\n",
    "                        \"url\": post.get('url'),\n",
    "                        \"score\": post.get('score'),\n",
    "                        \"num_comments\": post.get('num_comments'),\n",
    "                        \"upvote_ratio\": post.get('upvote_ratio'),\n",
    "                        \"permalink\": f\"https://reddit.com{post.get('permalink')}\"\n",
    "                    })\n",
    "                \n",
    "                after = data['data']['after']\n",
    "                print(f\"Collected {len(posts)} posts via JSON API...\")\n",
    "                \n",
    "                if not after:  # No more pages\n",
    "                    break\n",
    "                    \n",
    "                time.sleep(1)  # Rate limiting\n",
    "                \n",
    "            else:\n",
    "                print(f\"HTTP Error: {response.status_code}\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"JSON API error: {e}\")\n",
    "            break\n",
    "    \n",
    "    return pd.DataFrame(posts[:limit])\n",
    "\n",
    "def method_4_multiple_subreddits(subreddit_list, query, method='search'):\n",
    "    \"\"\"\n",
    "    Method 4: Collect from multiple subreddits\n",
    "    \"\"\"\n",
    "    print(f\"üéØ Method 4: Collecting from multiple subreddits...\")\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for sub in subreddit_list:\n",
    "        print(f\"Processing r/{sub}...\")\n",
    "        try:\n",
    "            if method == 'search':\n",
    "                df = method_1_praw_search(sub, query, limit=200)\n",
    "            elif method == 'hot':\n",
    "                df = method_2_praw_hot_new_top(sub, ['hot'], limit_per_category=200)\n",
    "                \n",
    "            if not df.empty:\n",
    "                all_data.append(df)\n",
    "                print(f\"Got {len(df)} posts from r/{sub}\")\n",
    "            \n",
    "            time.sleep(2)  # Be respectful between subreddits\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with r/{sub}: {e}\")\n",
    "    \n",
    "    if all_data:\n",
    "        return pd.concat(all_data, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def filter_by_keywords(df, keywords, column='title'):\n",
    "    \"\"\"\n",
    "    Filter dataframe by keywords in title or text\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    pattern = '|'.join(keywords)\n",
    "    mask = df[column].str.contains(pattern, case=False, na=False)\n",
    "    \n",
    "    if 'selftext' in df.columns:\n",
    "        selftext_mask = df['selftext'].str.contains(pattern, case=False, na=False)\n",
    "        mask = mask | selftext_mask\n",
    "    \n",
    "    return df[mask]\n",
    "\n",
    "# Example usage with multiple fallback methods\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting Reddit data collection with multiple methods...\\n\")\n",
    "    \n",
    "    # Try Method 1: PRAW Search (most reliable for keyword-based collection)\n",
    "    print(\"=\" * 50)\n",
    "    df1 = method_1_praw_search('politics', 'politics', limit=500)\n",
    "    \n",
    "    if not df1.empty:\n",
    "        print(f\"‚úÖ Method 1 Success: {len(df1)} posts\")\n",
    "    else:\n",
    "        print(\"‚ùå Method 1 failed\")\n",
    "    \n",
    "    # Try Method 2: Get hot/new/top posts from politics subreddit\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    df2 = method_2_praw_hot_new_top('politics', ['hot', 'new', 'top'], limit_per_category=200)\n",
    "    \n",
    "    if not df2.empty:\n",
    "        print(f\"‚úÖ Method 2 Success: {len(df2)} posts\")\n",
    "        # Filter for politics-related content\n",
    "        df2_filtered = filter_by_keywords(df2, ['politics', 'political', 'election', 'government', 'congress'])\n",
    "        print(f\"After keyword filtering: {len(df2_filtered)} posts\")\n",
    "    else:\n",
    "        print(\"‚ùå Method 2 failed\")\n",
    "    \n",
    "    # Try Method 3: Reddit JSON API\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    df3 = method_3_reddit_json_api('politics', 'hot', limit=200)\n",
    "    \n",
    "    if not df3.empty:\n",
    "        print(f\"‚úÖ Method 3 Success: {len(df3)} posts\")\n",
    "    else:\n",
    "        print(\"‚ùå Method 3 failed\")\n",
    "    \n",
    "    # Try Method 4: Multiple subreddits\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    political_subs = ['politics', 'PoliticalDiscussion', 'neutralpolitics', 'Ask_Politics']\n",
    "    df4 = method_4_multiple_subreddits(political_subs, 'politics', method='search')\n",
    "    \n",
    "    if not df4.empty:\n",
    "        print(f\"‚úÖ Method 4 Success: {len(df4)} posts from multiple subreddits\")\n",
    "    else:\n",
    "        print(\"‚ùå Method 4 failed\")\n",
    "    \n",
    "    # Combine all successful methods\n",
    "    successful_dfs = [df for df in [df1, df2_filtered if 'df2_filtered' in locals() else df2, df3, df4] if not df.empty]\n",
    "    \n",
    "    if successful_dfs:\n",
    "        final_df = pd.concat(successful_dfs, ignore_index=True)\n",
    "        # Remove duplicates based on post ID\n",
    "        final_df = final_df.drop_duplicates(subset=['id'], keep='first')\n",
    "        \n",
    "        print(f\"\\nüéâ FINAL RESULT: {len(final_df)} unique posts collected!\")\n",
    "        print(f\"Date range: {final_df['created_utc'].min()} to {final_df['created_utc'].max()}\")\n",
    "        \n",
    "        # Save results\n",
    "        output_file = f\"reddit_politics_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        final_df.to_csv(output_file, index=False)\n",
    "        print(f\"üíæ Data saved to: {output_file}\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(\"\\nSample data:\")\n",
    "        print(final_df[['title', 'author', 'score', 'created_utc']].head())\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ùå All methods failed. This might be due to:\")\n",
    "        print(\"- Reddit API rate limiting\")\n",
    "        print(\"- Network connectivity issues\") \n",
    "        print(\"- Reddit credentials issues\")\n",
    "        print(\"- Subreddit restrictions\")\n",
    "        \n",
    "        # Show troubleshooting steps\n",
    "        print(\"\\nüîß Troubleshooting steps:\")\n",
    "        print(\"1. Check your Reddit API credentials\")\n",
    "        print(\"2. Try again in a few minutes (rate limiting)\")\n",
    "        print(\"3. Test with a smaller limit\")\n",
    "        print(\"4. Check if the subreddit exists and is public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8182e1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Simple Reddit Data Collection\n",
      "==================================================\n",
      "Testing Reddit connection...\n",
      "‚úÖ Connected! Test subreddit: python\n",
      "\n",
      "üìç Method 1: Searching for politics posts...\n",
      "üîç Collecting posts from r/politics...\n",
      "Searching for: 'politics'\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "Collected 150 posts...\n",
      "Collected 200 posts...\n",
      "‚úÖ Collected 234 politics posts\n",
      "‚úÖ Data saved to reddit_politics_20250821_174915.csv\n",
      "‚úÖ JSON data saved to reddit_politics_20250821_174915.json\n",
      "\n",
      "üìã Sample of 3 posts:\n",
      "============================================================\n",
      "Post 1:\n",
      "  Title: DOJ Opens Door To Stripping Citizenship Over Politics...\n",
      "  Author: marji80\n",
      "  Score: 33598\n",
      "  Comments: 3300\n",
      "  Created: 2025-07-02 19:42:01\n",
      "  URL: https://talkingpointsmemo.com/news/doj-opens-door-to-strippi...\n",
      "----------------------------------------\n",
      "Post 2:\n",
      "  Title: Everybody Hates Trump Now | Six months after Trump bragged about a ‚Äúhistoric‚Äù re...\n",
      "  Author: Aggravating_Money992\n",
      "  Score: 17373\n",
      "  Comments: 1238\n",
      "  Created: 2025-07-31 16:41:20\n",
      "  URL: https://newrepublic.com/article/198624/everybody-hates-trump...\n",
      "----------------------------------------\n",
      "Post 3:\n",
      "  Title: Tim Walz: Trump Will Start Arresting Political Opponents...\n",
      "  Author: thedailybeast\n",
      "  Score: 58945\n",
      "  Comments: 4112\n",
      "  Created: 2025-03-20 20:30:01\n",
      "  URL: https://www.thedailybeast.com/tim-walz-trump-will-start-arre...\n",
      "----------------------------------------\n",
      "\n",
      "üìä Data Summary:\n",
      "Total posts: 234\n",
      "Average score: 22926.1\n",
      "Max score: 129734\n",
      "Unique authors: 172\n",
      "\n",
      "üìç Method 2: Trying other political subreddits...\n",
      "Trying r/PoliticalDiscussion...\n",
      "üîç Collecting posts from r/PoliticalDiscussion...\n",
      "Getting hot posts...\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "‚úÖ Added 100 posts from r/PoliticalDiscussion\n",
      "Trying r/Ask_Politics...\n",
      "üîç Collecting posts from r/Ask_Politics...\n",
      "Getting hot posts...\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "‚úÖ Added 100 posts from r/Ask_Politics\n",
      "Trying r/neutralpolitics...\n",
      "üîç Collecting posts from r/neutralpolitics...\n",
      "Getting hot posts...\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "‚úÖ Added 100 posts from r/neutralpolitics\n",
      "\n",
      "üéâ FINAL TOTAL: 534 posts collected!\n",
      "After removing duplicates: 534 unique posts\n",
      "‚úÖ Data saved to reddit_political_data_final_20250821_174930.csv\n",
      "‚úÖ JSON data saved to reddit_political_data_final_20250821_174930.json\n",
      "\n",
      "üéØ Data collection complete!\n",
      "CSV file: reddit_political_data_final_20250821_174930.csv\n",
      "JSON file: reddit_political_data_final_20250821_174930.json\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"xeV3sLrkl4xGyM-ScCBkSA\",\n",
    "    client_secret=\"sGrJQoAWabMp3hVcAXTxocV3gkCXoQ\",\n",
    "    user_agent=\"HeadlineHive\"\n",
    ")\n",
    "\n",
    "def collect_reddit_posts_simple(subreddit_name, search_query=None, limit=500):\n",
    "    \"\"\"\n",
    "    Simple Reddit data collection without pandas dependencies\n",
    "    \"\"\"\n",
    "    print(f\"üîç Collecting posts from r/{subreddit_name}...\")\n",
    "    \n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "    \n",
    "    try:\n",
    "        if search_query:\n",
    "            print(f\"Searching for: '{search_query}'\")\n",
    "            submissions = subreddit.search(search_query, limit=limit)\n",
    "        else:\n",
    "            print(\"Getting hot posts...\")\n",
    "            submissions = subreddit.hot(limit=limit)\n",
    "        \n",
    "        for i, submission in enumerate(submissions):\n",
    "            try:\n",
    "                post_data = {\n",
    "                    'id': submission.id,\n",
    "                    'title': submission.title,\n",
    "                    'text': submission.selftext,\n",
    "                    'author': str(submission.author) if submission.author else '[deleted]',\n",
    "                    'subreddit': submission.subreddit.display_name,\n",
    "                    'score': submission.score,\n",
    "                    'comments': submission.num_comments,\n",
    "                    'url': submission.url,\n",
    "                    'created': datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'permalink': f\"https://reddit.com{submission.permalink}\"\n",
    "                }\n",
    "                posts.append(post_data)\n",
    "                \n",
    "                if (i + 1) % 50 == 0:\n",
    "                    print(f\"Collected {i + 1} posts...\")\n",
    "                    time.sleep(0.5)  # Be nice to the API\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing post {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return posts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting posts: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_to_csv(posts, filename):\n",
    "    \"\"\"Save posts to CSV file\"\"\"\n",
    "    if not posts:\n",
    "        print(\"No posts to save!\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        fieldnames = posts[0].keys()\n",
    "        \n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for post in posts:\n",
    "                # Clean text fields to avoid CSV issues\n",
    "                clean_post = {}\n",
    "                for key, value in post.items():\n",
    "                    if isinstance(value, str):\n",
    "                        # Remove problematic characters\n",
    "                        clean_post[key] = value.replace('\\n', ' ').replace('\\r', ' ')[:1000]\n",
    "                    else:\n",
    "                        clean_post[key] = value\n",
    "                writer.writerow(clean_post)\n",
    "        \n",
    "        print(f\"‚úÖ Data saved to {filename}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving CSV: {e}\")\n",
    "        return False\n",
    "\n",
    "def save_to_json(posts, filename):\n",
    "    \"\"\"Save posts to JSON file\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(posts, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"‚úÖ JSON data saved to {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving JSON: {e}\")\n",
    "        return False\n",
    "\n",
    "def display_sample(posts, num_samples=3):\n",
    "    \"\"\"Display sample posts without pandas\"\"\"\n",
    "    if not posts:\n",
    "        print(\"No posts to display\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüìã Sample of {min(num_samples, len(posts))} posts:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, post in enumerate(posts[:num_samples]):\n",
    "        print(f\"Post {i+1}:\")\n",
    "        print(f\"  Title: {post['title'][:80]}...\")\n",
    "        print(f\"  Author: {post['author']}\")\n",
    "        print(f\"  Score: {post['score']}\")\n",
    "        print(f\"  Comments: {post['comments']}\")\n",
    "        print(f\"  Created: {post['created']}\")\n",
    "        print(f\"  URL: {post['url'][:60]}...\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Simple Reddit Data Collection\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test Reddit connection\n",
    "    try:\n",
    "        print(\"Testing Reddit connection...\")\n",
    "        test_sub = reddit.subreddit('python')\n",
    "        print(f\"‚úÖ Connected! Test subreddit: {test_sub.display_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Reddit connection failed: {e}\")\n",
    "        print(\"Check your Reddit API credentials!\")\n",
    "        exit()\n",
    "    \n",
    "    # Method 1: Search for politics posts\n",
    "    print(\"\\nüìç Method 1: Searching for politics posts...\")\n",
    "    politics_posts = collect_reddit_posts_simple('politics', 'politics', limit=300)\n",
    "    \n",
    "    if politics_posts:\n",
    "        print(f\"‚úÖ Collected {len(politics_posts)} politics posts\")\n",
    "        \n",
    "        # Save data\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        csv_file = f\"reddit_politics_{timestamp}.csv\"\n",
    "        json_file = f\"reddit_politics_{timestamp}.json\"\n",
    "        \n",
    "        save_to_csv(politics_posts, csv_file)\n",
    "        save_to_json(politics_posts, json_file)\n",
    "        \n",
    "        # Display sample\n",
    "        display_sample(politics_posts)\n",
    "        \n",
    "        # Basic stats\n",
    "        print(f\"\\nüìä Data Summary:\")\n",
    "        print(f\"Total posts: {len(politics_posts)}\")\n",
    "        \n",
    "        scores = [post['score'] for post in politics_posts]\n",
    "        if scores:\n",
    "            print(f\"Average score: {sum(scores) / len(scores):.1f}\")\n",
    "            print(f\"Max score: {max(scores)}\")\n",
    "        \n",
    "        authors = [post['author'] for post in politics_posts if post['author'] != '[deleted]']\n",
    "        unique_authors = len(set(authors))\n",
    "        print(f\"Unique authors: {unique_authors}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Failed to collect politics posts\")\n",
    "    \n",
    "    # Method 2: Try other political subreddits\n",
    "    print(\"\\nüìç Method 2: Trying other political subreddits...\")\n",
    "    other_subs = ['PoliticalDiscussion', 'Ask_Politics', 'neutralpolitics']\n",
    "    all_posts = politics_posts.copy() if politics_posts else []\n",
    "    \n",
    "    for sub in other_subs:\n",
    "        print(f\"Trying r/{sub}...\")\n",
    "        try:\n",
    "            sub_posts = collect_reddit_posts_simple(sub, None, limit=100)\n",
    "            if sub_posts:\n",
    "                all_posts.extend(sub_posts)\n",
    "                print(f\"‚úÖ Added {len(sub_posts)} posts from r/{sub}\")\n",
    "            time.sleep(2)  # Rate limiting\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed r/{sub}: {e}\")\n",
    "    \n",
    "    # Final results\n",
    "    if all_posts:\n",
    "        print(f\"\\nüéâ FINAL TOTAL: {len(all_posts)} posts collected!\")\n",
    "        \n",
    "        # Remove duplicates manually\n",
    "        seen_ids = set()\n",
    "        unique_posts = []\n",
    "        for post in all_posts:\n",
    "            if post['id'] not in seen_ids:\n",
    "                unique_posts.append(post)\n",
    "                seen_ids.add(post['id'])\n",
    "        \n",
    "        print(f\"After removing duplicates: {len(unique_posts)} unique posts\")\n",
    "        \n",
    "        # Save final dataset\n",
    "        final_csv = f\"reddit_political_data_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        final_json = f\"reddit_political_data_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        \n",
    "        save_to_csv(unique_posts, final_csv)\n",
    "        save_to_json(unique_posts, final_json)\n",
    "        \n",
    "        print(\"\\nüéØ Data collection complete!\")\n",
    "        print(f\"CSV file: {final_csv}\")\n",
    "        print(f\"JSON file: {final_json}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ùå No data collected. Possible issues:\")\n",
    "        print(\"- Reddit API credentials invalid\")\n",
    "        print(\"- Network connectivity problems\")\n",
    "        print(\"- Rate limiting (try again later)\")\n",
    "        print(\"- Subreddit access restrictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "853c1956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Simple Reddit Data Collection\n",
      "==================================================\n",
      "Testing Reddit connection...\n",
      "‚úÖ Connected! Test subreddit: python\n",
      "\n",
      "üìç Method 1: Searching for politics posts...\n",
      "üîç Collecting posts from r/politics...\n",
      "Searching for: 'politics'\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "Collected 150 posts...\n",
      "Collected 200 posts...\n",
      "‚úÖ Collected 234 politics posts\n",
      "‚úÖ JSON data saved to reddit_politics_20250821_180555.json\n",
      "\n",
      "üìã Sample of 3 posts:\n",
      "============================================================\n",
      "Post 1:\n",
      "  Title: DOJ Opens Door To Stripping Citizenship Over Politics...\n",
      "  Author: marji80\n",
      "  Score: 33606\n",
      "  Comments: 3300\n",
      "  Created: 2025-07-02 19:42:01\n",
      "  URL: https://talkingpointsmemo.com/news/doj-opens-door-to-strippi...\n",
      "----------------------------------------\n",
      "Post 2:\n",
      "  Title: Everybody Hates Trump Now | Six months after Trump bragged about a ‚Äúhistoric‚Äù re...\n",
      "  Author: Aggravating_Money992\n",
      "  Score: 17373\n",
      "  Comments: 1238\n",
      "  Created: 2025-07-31 16:41:20\n",
      "  URL: https://newrepublic.com/article/198624/everybody-hates-trump...\n",
      "----------------------------------------\n",
      "Post 3:\n",
      "  Title: Tim Walz: Trump Will Start Arresting Political Opponents...\n",
      "  Author: thedailybeast\n",
      "  Score: 58943\n",
      "  Comments: 4112\n",
      "  Created: 2025-03-20 20:30:01\n",
      "  URL: https://www.thedailybeast.com/tim-walz-trump-will-start-arre...\n",
      "----------------------------------------\n",
      "\n",
      "üìä Data Summary:\n",
      "Total posts: 234\n",
      "Average score: 22925.9\n",
      "Max score: 129732\n",
      "Unique authors: 172\n",
      "\n",
      "üìç Method 2: Trying other political subreddits...\n",
      "Trying r/PoliticalDiscussion...\n",
      "üîç Collecting posts from r/PoliticalDiscussion...\n",
      "Getting hot posts...\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "‚úÖ Added 100 posts from r/PoliticalDiscussion\n",
      "Trying r/Ask_Politics...\n",
      "üîç Collecting posts from r/Ask_Politics...\n",
      "Getting hot posts...\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "‚úÖ Added 100 posts from r/Ask_Politics\n",
      "Trying r/neutralpolitics...\n",
      "üîç Collecting posts from r/neutralpolitics...\n",
      "Getting hot posts...\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "‚úÖ Added 100 posts from r/neutralpolitics\n",
      "\n",
      "üéâ FINAL TOTAL: 534 posts collected!\n",
      "After removing duplicates: 534 unique posts\n",
      "‚úÖ JSON data saved to reddit_political_data_final_20250821_180610.json\n",
      "\n",
      "üéØ Data collection complete!\n",
      "JSON file: reddit_political_data_final_20250821_180610.json\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"xeV3sLrkl4xGyM-ScCBkSA\",\n",
    "    client_secret=\"sGrJQoAWabMp3hVcAXTxocV3gkCXoQ\",\n",
    "    user_agent=\"HeadlineHive\"\n",
    ")\n",
    "\n",
    "def collect_reddit_posts_simple(subreddit_name, search_query=None, limit=500):\n",
    "    \"\"\"\n",
    "    Simple Reddit data collection without pandas dependencies\n",
    "    \"\"\"\n",
    "    print(f\"üîç Collecting posts from r/{subreddit_name}...\")\n",
    "    \n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "    \n",
    "    try:\n",
    "        if search_query:\n",
    "            print(f\"Searching for: '{search_query}'\")\n",
    "            submissions = subreddit.search(search_query, limit=limit)\n",
    "        else:\n",
    "            print(\"Getting hot posts...\")\n",
    "            submissions = subreddit.hot(limit=limit)\n",
    "        \n",
    "        for i, submission in enumerate(submissions):\n",
    "            try:\n",
    "                post_data = {\n",
    "                    'id': submission.id,\n",
    "                    'title': submission.title,\n",
    "                    'text': submission.selftext,\n",
    "                    'author': str(submission.author) if submission.author else '[deleted]',\n",
    "                    'subreddit': submission.subreddit.display_name,\n",
    "                    'score': submission.score,\n",
    "                    'comments': submission.num_comments,\n",
    "                    'url': submission.url,\n",
    "                    'created': datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'permalink': f\"https://reddit.com{submission.permalink}\"\n",
    "                }\n",
    "                posts.append(post_data)\n",
    "                \n",
    "                if (i + 1) % 50 == 0:\n",
    "                    print(f\"Collected {i + 1} posts...\")\n",
    "                    time.sleep(0.5)  # Be nice to the API\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing post {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return posts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting posts: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_to_csv(posts, filename):\n",
    "    \"\"\"Save posts to CSV file\"\"\"\n",
    "    if not posts:\n",
    "        print(\"No posts to save!\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        fieldnames = posts[0].keys()\n",
    "        \n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for post in posts:\n",
    "                # Clean text fields to avoid CSV issues\n",
    "                clean_post = {}\n",
    "                for key, value in post.items():\n",
    "                    if isinstance(value, str):\n",
    "                        # Remove problematic characters\n",
    "                        clean_post[key] = value.replace('\\n', ' ').replace('\\r', ' ')[:1000]\n",
    "                    else:\n",
    "                        clean_post[key] = value\n",
    "                writer.writerow(clean_post)\n",
    "        \n",
    "        print(f\"‚úÖ Data saved to {filename}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving CSV: {e}\")\n",
    "        return False\n",
    "\n",
    "def save_to_json(posts, filename):\n",
    "    \"\"\"Save posts to JSON file\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(posts, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"‚úÖ JSON data saved to {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving JSON: {e}\")\n",
    "        return False\n",
    "\n",
    "def display_sample(posts, num_samples=3):\n",
    "    \"\"\"Display sample posts without pandas\"\"\"\n",
    "    if not posts:\n",
    "        print(\"No posts to display\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüìã Sample of {min(num_samples, len(posts))} posts:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, post in enumerate(posts[:num_samples]):\n",
    "        print(f\"Post {i+1}:\")\n",
    "        print(f\"  Title: {post['title'][:80]}...\")\n",
    "        print(f\"  Author: {post['author']}\")\n",
    "        print(f\"  Score: {post['score']}\")\n",
    "        print(f\"  Comments: {post['comments']}\")\n",
    "        print(f\"  Created: {post['created']}\")\n",
    "        print(f\"  URL: {post['url'][:60]}...\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Simple Reddit Data Collection\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test Reddit connection\n",
    "    try:\n",
    "        print(\"Testing Reddit connection...\")\n",
    "        test_sub = reddit.subreddit('python')\n",
    "        print(f\"‚úÖ Connected! Test subreddit: {test_sub.display_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Reddit connection failed: {e}\")\n",
    "        print(\"Check your Reddit API credentials!\")\n",
    "        exit()\n",
    "    \n",
    "    # Method 1: Search for politics posts\n",
    "    print(\"\\nüìç Method 1: Searching for politics posts...\")\n",
    "    politics_posts = collect_reddit_posts_simple('politics', 'politics', limit=300)\n",
    "    \n",
    "    if politics_posts:\n",
    "        print(f\"‚úÖ Collected {len(politics_posts)} politics posts\")\n",
    "        \n",
    "        # Save data (JSON only)\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        json_file = f\"reddit_politics_{timestamp}.json\"\n",
    "        \n",
    "        save_to_json(politics_posts, json_file)\n",
    "        \n",
    "        # Display sample\n",
    "        display_sample(politics_posts)\n",
    "        \n",
    "        # Basic stats\n",
    "        print(f\"\\nüìä Data Summary:\")\n",
    "        print(f\"Total posts: {len(politics_posts)}\")\n",
    "        \n",
    "        scores = [post['score'] for post in politics_posts]\n",
    "        if scores:\n",
    "            print(f\"Average score: {sum(scores) / len(scores):.1f}\")\n",
    "            print(f\"Max score: {max(scores)}\")\n",
    "        \n",
    "        authors = [post['author'] for post in politics_posts if post['author'] != '[deleted]']\n",
    "        unique_authors = len(set(authors))\n",
    "        print(f\"Unique authors: {unique_authors}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Failed to collect politics posts\")\n",
    "    \n",
    "    # Method 2: Try other political subreddits\n",
    "    print(\"\\nüìç Method 2: Trying other political subreddits...\")\n",
    "    other_subs = ['PoliticalDiscussion', 'Ask_Politics', 'neutralpolitics']\n",
    "    all_posts = politics_posts.copy() if politics_posts else []\n",
    "    \n",
    "    for sub in other_subs:\n",
    "        print(f\"Trying r/{sub}...\")\n",
    "        try:\n",
    "            sub_posts = collect_reddit_posts_simple(sub, None, limit=100)\n",
    "            if sub_posts:\n",
    "                all_posts.extend(sub_posts)\n",
    "                print(f\"‚úÖ Added {len(sub_posts)} posts from r/{sub}\")\n",
    "            time.sleep(2)  # Rate limiting\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed r/{sub}: {e}\")\n",
    "    \n",
    "    # Final results\n",
    "    if all_posts:\n",
    "        print(f\"\\nüéâ FINAL TOTAL: {len(all_posts)} posts collected!\")\n",
    "        \n",
    "        # Remove duplicates manually\n",
    "        seen_ids = set()\n",
    "        unique_posts = []\n",
    "        for post in all_posts:\n",
    "            if post['id'] not in seen_ids:\n",
    "                unique_posts.append(post)\n",
    "                seen_ids.add(post['id'])\n",
    "        \n",
    "        print(f\"After removing duplicates: {len(unique_posts)} unique posts\")\n",
    "        \n",
    "        # Save final dataset (JSON only)\n",
    "        final_json = f\"reddit_political_data_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        \n",
    "        save_to_json(unique_posts, final_json)\n",
    "        \n",
    "        print(\"\\nüéØ Data collection complete!\")\n",
    "        print(f\"JSON file: {final_json}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ùå No data collected. Possible issues:\")\n",
    "        print(\"- Reddit API credentials invalid\")\n",
    "        print(\"- Network connectivity problems\")\n",
    "        print(\"- Rate limiting (try again later)\")\n",
    "        print(\"- Subreddit access restrictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b78ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Reddit Data Collection - Single JSON Output\n",
      "============================================================\n",
      "üîå Testing Reddit connection...\n",
      "‚úÖ Connected! Test subreddit: python\n",
      "\n",
      "üéØ Starting comprehensive data collection...\n",
      "\n",
      "üìç Processing r/politics...\n",
      "üîç Method: SEARCH from r/politics\n",
      "   Query: 'politics'\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "   Processed 125 posts...\n",
      "   Processed 150 posts...\n",
      "   Processed 175 posts...\n",
      "   Processed 200 posts...\n",
      "‚úÖ Collected 200 posts using search\n",
      "   Added 200 posts from search\n",
      "üîç Method: HOT from r/politics\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "‚úÖ Collected 100 posts using hot\n",
      "   Added 100 posts from hot\n",
      "üîç Method: NEW from r/politics\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "‚úÖ Collected 100 posts using new\n",
      "   Added 100 posts from new\n",
      "\n",
      "üìç Processing r/PoliticalDiscussion...\n",
      "üîç Method: SEARCH from r/PoliticalDiscussion\n",
      "   Query: 'politics'\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "   Processed 125 posts...\n",
      "   Processed 150 posts...\n",
      "   Processed 175 posts...\n",
      "   Processed 200 posts...\n",
      "‚úÖ Collected 200 posts using search\n",
      "   Added 200 posts from search\n",
      "üîç Method: HOT from r/PoliticalDiscussion\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "‚úÖ Collected 100 posts using hot\n",
      "   Added 100 posts from hot\n",
      "üîç Method: NEW from r/PoliticalDiscussion\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "‚úÖ Collected 100 posts using new\n",
      "   Added 100 posts from new\n",
      "\n",
      "üìç Processing r/Ask_Politics...\n",
      "üîç Method: SEARCH from r/Ask_Politics\n",
      "   Query: 'politics'\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "   Processed 125 posts...\n",
      "   Processed 150 posts...\n",
      "   Processed 175 posts...\n",
      "   Processed 200 posts...\n",
      "‚úÖ Collected 200 posts using search\n",
      "   Added 200 posts from search\n",
      "üîç Method: HOT from r/Ask_Politics\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "‚úÖ Collected 100 posts using hot\n",
      "   Added 100 posts from hot\n",
      "üîç Method: NEW from r/Ask_Politics\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "‚úÖ Collected 100 posts using new\n",
      "   Added 100 posts from new\n",
      "\n",
      "üìç Processing r/neutralpolitics...\n",
      "üîç Method: SEARCH from r/neutralpolitics\n",
      "   Query: 'politics'\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "   Processed 125 posts...\n",
      "   Processed 150 posts...\n",
      "   Processed 175 posts...\n",
      "   Processed 200 posts...\n",
      "‚úÖ Collected 200 posts using search\n",
      "   Added 200 posts from search\n",
      "üîç Method: HOT from r/neutralpolitics\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "‚úÖ Collected 100 posts using hot\n",
      "   Added 100 posts from hot\n",
      "üîç Method: NEW from r/neutralpolitics\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "‚úÖ Collected 100 posts using new\n",
      "   Added 100 posts from new\n",
      "\n",
      "üìä Processing 1600 total posts...\n",
      "After removing duplicates: 1219 unique posts\n",
      "Posts with text content: 868\n",
      "Posts without text content: 351\n",
      "‚úÖ All data saved to: reddit_political_data_complete_20250821_181052.json\n",
      "   Total posts: 1219\n",
      "\n",
      "üìã Sample of 5 posts:\n",
      "================================================================================\n",
      "\n",
      "Post 1:\n",
      "  ID: 1lpwwbr\n",
      "  Title: DOJ Opens Door To Stripping Citizenship Over Politics...\n",
      "  Author: marji80\n",
      "  Subreddit: r/politics\n",
      "  Score: 33601 | Comments: 3300\n",
      "  Method: search\n",
      "  Is Self Post: False\n",
      "  Text: Link post: https://talkingpointsmemo.com/news/doj-opens-door-to-stripping-citizenship-over-politics\n",
      "  Created: 2025-07-02 19:42:01\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Post 2:\n",
      "  ID: 1mdyoyp\n",
      "  Title: Everybody Hates Trump Now | Six months after Trump bragged about a ‚Äúhistoric‚Äù realignment, voters fr...\n",
      "  Author: Aggravating_Money992\n",
      "  Subreddit: r/politics\n",
      "  Score: 17367 | Comments: 1238\n",
      "  Method: search\n",
      "  Is Self Post: False\n",
      "  Text: Link post: https://newrepublic.com/article/198624/everybody-hates-trump-approval-rating-polls\n",
      "  Created: 2025-07-31 16:41:20\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Post 3:\n",
      "  ID: 1jfqvem\n",
      "  Title: Tim Walz: Trump Will Start Arresting Political Opponents...\n",
      "  Author: thedailybeast\n",
      "  Subreddit: r/politics\n",
      "  Score: 58941 | Comments: 4112\n",
      "  Method: search\n",
      "  Is Self Post: False\n",
      "  Text: Link post: https://www.thedailybeast.com/tim-walz-trump-will-start-arresting-political-opponents/\n",
      "  Created: 2025-03-20 20:30:01\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Post 4:\n",
      "  ID: 1m323ju\n",
      "  Title: Politics, not Performance Killed ‚ÄòThe Late Show‚Äô with Stephen Colbert...\n",
      "  Author: AlexandrTheTolerable\n",
      "  Subreddit: r/politics\n",
      "  Score: 12306 | Comments: 845\n",
      "  Method: search\n",
      "  Is Self Post: False\n",
      "  Text: Link post: https://www.rollingstone.com/tv-movies/tv-movie-features/late-show-with-stephen-colbert-ending-analysis-1235388587/\n",
      "  Created: 2025-07-18 18:47:27\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Post 5:\n",
      "  ID: 1k4pnor\n",
      "  Title: Musk wants to leave politics because he‚Äôs tired of ‚Äòattacks‚Äô from the left: report...\n",
      "  Author: BreakfastTop6899\n",
      "  Subreddit: r/politics\n",
      "  Score: 13091 | Comments: 1823\n",
      "  Method: search\n",
      "  Is Self Post: False\n",
      "  Text: Link post: https://www.independent.co.uk/news/world/americas/us-politics/elon-musk-donald-trump-doge-b2736753.html\n",
      "  Created: 2025-04-22 03:17:46\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìà Final Statistics:\n",
      "Total unique posts: 1219\n",
      "Posts with actual text: 868\n",
      "Subreddits covered: 4\n",
      "Methods used: {'new', 'hot', 'search'}\n",
      "Average text length: 1136 characters\n",
      "Longest text: 16093 characters\n",
      "\n",
      "üéâ Complete dataset saved to: reddit_political_data_complete_20250821_181052.json\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"xeV3sLrkl4xGyM-ScCBkSA\",\n",
    "    client_secret=\"sGrJQoAWabMp3hVcAXTxocV3gkCXoQ\",\n",
    "    user_agent=\"HeadlineHive\"\n",
    ")\n",
    "\n",
    "def collect_reddit_posts(subreddit_name, search_query=None, limit=500, method=\"search\"):\n",
    "    \"\"\"\n",
    "    Collect Reddit data with proper text extraction\n",
    "    \"\"\"\n",
    "    print(f\"üîç Method: {method.upper()} from r/{subreddit_name}\")\n",
    "    if search_query:\n",
    "        print(f\"   Query: '{search_query}'\")\n",
    "    \n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "    \n",
    "    try:\n",
    "        # Choose method\n",
    "        if method == \"search\" and search_query:\n",
    "            submissions = subreddit.search(search_query, limit=limit, time_filter='all')\n",
    "        elif method == \"hot\":\n",
    "            submissions = subreddit.hot(limit=limit)\n",
    "        elif method == \"new\":\n",
    "            submissions = subreddit.new(limit=limit)\n",
    "        elif method == \"top\":\n",
    "            submissions = subreddit.top(limit=limit, time_filter='month')\n",
    "        else:\n",
    "            print(f\"Unknown method: {method}\")\n",
    "            return []\n",
    "        \n",
    "        for i, submission in enumerate(submissions):\n",
    "            try:\n",
    "                # Extract text content properly\n",
    "                text_content = \"\"\n",
    "                \n",
    "                # For self posts (text posts)\n",
    "                if submission.is_self and submission.selftext:\n",
    "                    text_content = submission.selftext.strip()\n",
    "                \n",
    "                # For link posts, we might want the URL or title as content\n",
    "                elif not submission.is_self:\n",
    "                    text_content = f\"Link post: {submission.url}\"\n",
    "                \n",
    "                # Sometimes selftext exists but is empty/whitespace\n",
    "                if not text_content and hasattr(submission, 'selftext'):\n",
    "                    if submission.selftext and submission.selftext.strip():\n",
    "                        text_content = submission.selftext.strip()\n",
    "                \n",
    "                # If still no content, use title as fallback\n",
    "                if not text_content:\n",
    "                    text_content = f\"[Title only] {submission.title}\"\n",
    "                \n",
    "                post_data = {\n",
    "                    'id': submission.id,\n",
    "                    'title': submission.title if submission.title else \"No title\",\n",
    "                    'text': text_content,\n",
    "                    'author': str(submission.author) if submission.author else '[deleted]',\n",
    "                    'subreddit': submission.subreddit.display_name,\n",
    "                    'score': submission.score if submission.score else 0,\n",
    "                    'upvote_ratio': getattr(submission, 'upvote_ratio', 0),\n",
    "                    'comments': submission.num_comments if submission.num_comments else 0,\n",
    "                    'url': submission.url if submission.url else \"\",\n",
    "                    'is_self': submission.is_self,\n",
    "                    'created': datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'permalink': f\"https://reddit.com{submission.permalink}\",\n",
    "                    'method_collected': method,\n",
    "                    'flair': submission.link_flair_text if hasattr(submission, 'link_flair_text') else None,\n",
    "                    'nsfw': submission.over_18 if hasattr(submission, 'over_18') else False\n",
    "                }\n",
    "                \n",
    "                posts.append(post_data)\n",
    "                \n",
    "                if (i + 1) % 25 == 0:\n",
    "                    print(f\"   Processed {i + 1} posts...\")\n",
    "                    time.sleep(0.3)  # Rate limiting\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   Error processing post {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"‚úÖ Collected {len(posts)} posts using {method}\")\n",
    "        return posts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with {method} method: {e}\")\n",
    "        return []\n",
    "\n",
    "def remove_duplicates(all_posts):\n",
    "    \"\"\"Remove duplicate posts based on ID\"\"\"\n",
    "    seen_ids = set()\n",
    "    unique_posts = []\n",
    "    \n",
    "    for post in all_posts:\n",
    "        if post['id'] not in seen_ids:\n",
    "            unique_posts.append(post)\n",
    "            seen_ids.add(post['id'])\n",
    "    \n",
    "    return unique_posts\n",
    "\n",
    "def save_single_json(posts, filename):\n",
    "    \"\"\"Save all posts to a single JSON file\"\"\"\n",
    "    if not posts:\n",
    "        print(\"‚ùå No posts to save!\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Create comprehensive data structure\n",
    "        output_data = {\n",
    "            \"metadata\": {\n",
    "                \"total_posts\": len(posts),\n",
    "                \"collection_date\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"methods_used\": list(set([post.get('method_collected', 'unknown') for post in posts])),\n",
    "                \"subreddits\": list(set([post['subreddit'] for post in posts])),\n",
    "                \"date_range\": {\n",
    "                    \"earliest\": min([post['created'] for post in posts]),\n",
    "                    \"latest\": max([post['created'] for post in posts])\n",
    "                }\n",
    "            },\n",
    "            \"posts\": posts\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"‚úÖ All data saved to: {filename}\")\n",
    "        print(f\"   Total posts: {len(posts)}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving JSON: {e}\")\n",
    "        return False\n",
    "\n",
    "def display_sample_with_text(posts, num_samples=3):\n",
    "    \"\"\"Display sample posts showing text content\"\"\"\n",
    "    if not posts:\n",
    "        print(\"No posts to display\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüìã Sample of {min(num_samples, len(posts))} posts:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, post in enumerate(posts[:num_samples]):\n",
    "        print(f\"\\nPost {i+1}:\")\n",
    "        print(f\"  ID: {post['id']}\")\n",
    "        print(f\"  Title: {post['title'][:100]}...\")\n",
    "        print(f\"  Author: {post['author']}\")\n",
    "        print(f\"  Subreddit: r/{post['subreddit']}\")\n",
    "        print(f\"  Score: {post['score']} | Comments: {post['comments']}\")\n",
    "        print(f\"  Method: {post['method_collected']}\")\n",
    "        print(f\"  Is Self Post: {post['is_self']}\")\n",
    "        print(f\"  Text Preview: {post['text'][:200]}...\" if len(post['text']) > 200 else f\"  Text: {post['text']}\")\n",
    "        print(f\"  Created: {post['created']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Main execution - Single JSON output with both methods\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Reddit Data Collection - Single JSON Output\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test Reddit connection\n",
    "    try:\n",
    "        print(\"üîå Testing Reddit connection...\")\n",
    "        test_sub = reddit.subreddit('python')\n",
    "        print(f\"‚úÖ Connected! Test subreddit: {test_sub.display_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Reddit connection failed: {e}\")\n",
    "        print(\"Check your Reddit API credentials!\")\n",
    "        exit()\n",
    "    \n",
    "    # Collect from multiple sources using both methods\n",
    "    all_posts = []\n",
    "    \n",
    "    # Configuration\n",
    "    subreddits_to_check = ['politics', 'PoliticalDiscussion', 'Ask_Politics', 'neutralpolitics']\n",
    "    methods_to_try = ['search', 'hot', 'new']\n",
    "    \n",
    "    print(\"\\nüéØ Starting comprehensive data collection...\")\n",
    "    \n",
    "    # Method 1: Search for politics-related content\n",
    "    for subreddit in subreddits_to_check:\n",
    "        print(f\"\\nüìç Processing r/{subreddit}...\")\n",
    "        \n",
    "        # Try search method first\n",
    "        if 'search' in methods_to_try:\n",
    "            search_posts = collect_reddit_posts(subreddit, 'politics', limit=200, method=\"search\")\n",
    "            if search_posts:\n",
    "                all_posts.extend(search_posts)\n",
    "                print(f\"   Added {len(search_posts)} posts from search\")\n",
    "        \n",
    "        # Try hot posts method\n",
    "        if 'hot' in methods_to_try:\n",
    "            time.sleep(1)  # Rate limiting\n",
    "            hot_posts = collect_reddit_posts(subreddit, None, limit=100, method=\"hot\")\n",
    "            if hot_posts:\n",
    "                all_posts.extend(hot_posts)\n",
    "                print(f\"   Added {len(hot_posts)} posts from hot\")\n",
    "        \n",
    "        # Try new posts method\n",
    "        if 'new' in methods_to_try:\n",
    "            time.sleep(1)  # Rate limiting\n",
    "            new_posts = collect_reddit_posts(subreddit, None, limit=100, method=\"new\")\n",
    "            if new_posts:\n",
    "                all_posts.extend(new_posts)\n",
    "                print(f\"   Added {len(new_posts)} posts from new\")\n",
    "        \n",
    "        time.sleep(2)  # Rate limiting between subreddits\n",
    "    \n",
    "    # Process and save results\n",
    "    if all_posts:\n",
    "        print(f\"\\nüìä Processing {len(all_posts)} total posts...\")\n",
    "        \n",
    "        # Remove duplicates\n",
    "        unique_posts = remove_duplicates(all_posts)\n",
    "        print(f\"After removing duplicates: {len(unique_posts)} unique posts\")\n",
    "        \n",
    "        # Filter out posts with no meaningful text content if desired\n",
    "        posts_with_text = []\n",
    "        posts_without_text = []\n",
    "        \n",
    "        for post in unique_posts:\n",
    "            if post['text'] and not post['text'].startswith('[Title only]') and not post['text'].startswith('Link post:'):\n",
    "                posts_with_text.append(post)\n",
    "            else:\n",
    "                posts_without_text.append(post)\n",
    "        \n",
    "        print(f\"Posts with text content: {len(posts_with_text)}\")\n",
    "        print(f\"Posts without text content: {len(posts_without_text)}\")\n",
    "        \n",
    "        # Save single JSON file with all data\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        final_json = f\"reddit_political_data_complete_{timestamp}.json\"\n",
    "        \n",
    "        save_single_json(unique_posts, final_json)\n",
    "        \n",
    "        # Display sample with text content\n",
    "        display_sample_with_text(unique_posts, 5)\n",
    "        \n",
    "        # Show statistics\n",
    "        print(f\"\\nüìà Final Statistics:\")\n",
    "        print(f\"Total unique posts: {len(unique_posts)}\")\n",
    "        print(f\"Posts with actual text: {len(posts_with_text)}\")\n",
    "        print(f\"Subreddits covered: {len(set([post['subreddit'] for post in unique_posts]))}\")\n",
    "        print(f\"Methods used: {set([post['method_collected'] for post in unique_posts])}\")\n",
    "        \n",
    "        # Show text content statistics\n",
    "        text_lengths = [len(post['text']) for post in posts_with_text]\n",
    "        if text_lengths:\n",
    "            print(f\"Average text length: {sum(text_lengths) / len(text_lengths):.0f} characters\")\n",
    "            print(f\"Longest text: {max(text_lengths)} characters\")\n",
    "        \n",
    "        print(f\"\\nüéâ Complete dataset saved to: {final_json}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ùå No data collected. Possible issues:\")\n",
    "        print(\"- Reddit API credentials invalid\")\n",
    "        print(\"- Network connectivity problems\") \n",
    "        print(\"- Rate limiting (try again later)\")\n",
    "        print(\"- Subreddit access restrictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a2e3a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ REDDIT COMPREHENSIVE DATA COLLECTION\n",
      "17 Categories - Separate JSON Files\n",
      "================================================================================\n",
      "üîå Testing Reddit connection...\n",
      "‚úÖ Connected! Test subreddit: python\n",
      "\n",
      "üìã CATEGORIES TO COLLECT (17):\n",
      "   1. Politics\n",
      "   2. Economy Business Finance\n",
      "   3. Science Technology\n",
      "   4. Health\n",
      "   5. Sport\n",
      "   6. Arts Culture Entertainment Media\n",
      "   7. Society\n",
      "   8. Human Interest\n",
      "   9. Crime Law Justice\n",
      "  10. Conflict War Peace\n",
      "  11. Disaster Accident Emergency\n",
      "  12. Environment\n",
      "  13. Education\n",
      "  14. Labour\n",
      "  15. Lifestyle Leisure\n",
      "  16. Weather\n",
      "  17. Religion\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 1/17: POLITICS\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING POLITICS DATA ===\n",
      "\n",
      "  üìç Processing r/politics...\n",
      "  üîç Method: SEARCH from r/politics\n",
      "     Query: 'politics'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'politics'\n",
      "  üîç Method: SEARCH from r/politics\n",
      "     Query: 'election'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'election'\n",
      "  üîç Method: SEARCH from r/politics\n",
      "     Query: 'government'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'government'\n",
      "  ‚úÖ r/politics: 300 posts total\n",
      "\n",
      "  üìç Processing r/PoliticalDiscussion...\n",
      "  üîç Method: SEARCH from r/PoliticalDiscussion\n",
      "     Query: 'politics'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'politics'\n",
      "  üîç Method: SEARCH from r/PoliticalDiscussion\n",
      "     Query: 'election'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'election'\n",
      "  üîç Method: SEARCH from r/PoliticalDiscussion\n",
      "     Query: 'government'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'government'\n",
      "  ‚úÖ r/PoliticalDiscussion: 300 posts total\n",
      "\n",
      "  üìç Processing r/Ask_Politics...\n",
      "  üîç Method: SEARCH from r/Ask_Politics\n",
      "     Query: 'politics'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 89 posts using search\n",
      "     Added 89 posts from search: 'politics'\n",
      "  üîç Method: SEARCH from r/Ask_Politics\n",
      "     Query: 'election'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 74 posts using search\n",
      "     Added 74 posts from search: 'election'\n",
      "  üîç Method: SEARCH from r/Ask_Politics\n",
      "     Query: 'government'\n",
      "     ‚úÖ Collected 30 posts using search\n",
      "     Added 30 posts from search: 'government'\n",
      "  üîç Method: HOT from r/Ask_Politics\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/Ask_Politics\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/Ask_Politics: 353 posts total\n",
      "\n",
      "  üìç Processing r/neutralpolitics...\n",
      "  üîç Method: SEARCH from r/neutralpolitics\n",
      "     Query: 'politics'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 60 posts using search\n",
      "     Added 60 posts from search: 'politics'\n",
      "  üîç Method: SEARCH from r/neutralpolitics\n",
      "     Query: 'election'\n",
      "     ‚úÖ Collected 36 posts using search\n",
      "     Added 36 posts from search: 'election'\n",
      "  üîç Method: SEARCH from r/neutralpolitics\n",
      "     Query: 'government'\n",
      "     ‚úÖ Collected 33 posts using search\n",
      "     Added 33 posts from search: 'government'\n",
      "  üîç Method: HOT from r/neutralpolitics\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/neutralpolitics\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/neutralpolitics: 289 posts total\n",
      "\n",
      "  üìç Processing r/worldpolitics...\n",
      "  üîç Method: SEARCH from r/worldpolitics\n",
      "     Query: 'politics'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/worldpolitics\n",
      "     Query: 'election'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/worldpolitics\n",
      "     Query: 'government'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: HOT from r/worldpolitics\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/worldpolitics\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/worldpolitics: 160 posts total\n",
      "\n",
      "  üìä Removed 446 duplicates\n",
      "  ‚úÖ Politics data saved to: reddit_data/Politics_20250822_235911.json\n",
      "     Posts: 956\n",
      "     Subreddits: 5\n",
      "\n",
      "  üìä POLITICS SUMMARY:\n",
      "     Total posts: 956\n",
      "     Posts with text: 581\n",
      "     Text ratio: 60.8%\n",
      "     Subreddits: 5\n",
      "     Average score: 4875.5\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 2/17: ECONOMY BUSINESS FINANCE\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING ECONOMY BUSINESS FINANCE DATA ===\n",
      "\n",
      "  üìç Processing r/Economics...\n",
      "  üîç Method: SEARCH from r/Economics\n",
      "     Query: 'economy'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'economy'\n",
      "  üîç Method: SEARCH from r/Economics\n",
      "     Query: 'business'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'business'\n",
      "  üîç Method: SEARCH from r/Economics\n",
      "     Query: 'finance'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 66 posts using search\n",
      "     Added 66 posts from search: 'finance'\n",
      "  ‚úÖ r/Economics: 266 posts total\n",
      "\n",
      "  üìç Processing r/business...\n",
      "  üîç Method: SEARCH from r/business\n",
      "     Query: 'economy'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 62 posts using search\n",
      "     Added 62 posts from search: 'economy'\n",
      "  üîç Method: SEARCH from r/business\n",
      "     Query: 'business'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'business'\n",
      "  üîç Method: SEARCH from r/business\n",
      "     Query: 'finance'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'finance'\n",
      "  ‚úÖ r/business: 262 posts total\n",
      "\n",
      "  üìç Processing r/stocks...\n",
      "  üîç Method: SEARCH from r/stocks\n",
      "     Query: 'economy'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'economy'\n",
      "  üîç Method: SEARCH from r/stocks\n",
      "     Query: 'business'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'business'\n",
      "  üîç Method: SEARCH from r/stocks\n",
      "     Query: 'finance'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'finance'\n",
      "  ‚úÖ r/stocks: 300 posts total\n",
      "\n",
      "  üìç Processing r/investing...\n",
      "  üîç Method: SEARCH from r/investing\n",
      "     Query: 'economy'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'economy'\n",
      "  üîç Method: SEARCH from r/investing\n",
      "     Query: 'business'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'business'\n",
      "  üîç Method: SEARCH from r/investing\n",
      "     Query: 'finance'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'finance'\n",
      "  ‚úÖ r/investing: 300 posts total\n",
      "\n",
      "  üìç Processing r/personalfinance...\n",
      "  üîç Method: SEARCH from r/personalfinance\n",
      "     Query: 'economy'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'economy'\n",
      "  üîç Method: SEARCH from r/personalfinance\n",
      "     Query: 'business'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'business'\n",
      "  üîç Method: SEARCH from r/personalfinance\n",
      "     Query: 'finance'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'finance'\n",
      "  ‚úÖ r/personalfinance: 300 posts total\n",
      "\n",
      "  üìç Processing r/economy...\n",
      "  üîç Method: SEARCH from r/economy\n",
      "     Query: 'economy'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/economy\n",
      "     Query: 'business'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'business'\n",
      "  üîç Method: SEARCH from r/economy\n",
      "     Query: 'finance'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'finance'\n",
      "  üîç Method: HOT from r/economy\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/economy: 280 posts total\n",
      "\n",
      "  üìç Processing r/finance...\n",
      "  üîç Method: SEARCH from r/finance\n",
      "     Query: 'economy'\n",
      "     ‚úÖ Collected 10 posts using search\n",
      "     Added 10 posts from search: 'economy'\n",
      "  üîç Method: SEARCH from r/finance\n",
      "     Query: 'business'\n",
      "     ‚úÖ Collected 6 posts using search\n",
      "     Added 6 posts from search: 'business'\n",
      "  üîç Method: SEARCH from r/finance\n",
      "     Query: 'finance'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'finance'\n",
      "  üîç Method: HOT from r/finance\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/finance\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/finance: 276 posts total\n",
      "\n",
      "  üìä Removed 194 duplicates\n",
      "  ‚úÖ Economy_Business_Finance data saved to: reddit_data/Economy_Business_Finance_20250823_000119.json\n",
      "     Posts: 1790\n",
      "     Subreddits: 7\n",
      "\n",
      "  üìä ECONOMY BUSINESS FINANCE SUMMARY:\n",
      "     Total posts: 1790\n",
      "     Posts with text: 1093\n",
      "     Text ratio: 61.1%\n",
      "     Subreddits: 7\n",
      "     Average score: 1062.8\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 3/17: SCIENCE TECHNOLOGY\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING SCIENCE TECHNOLOGY DATA ===\n",
      "\n",
      "  üìç Processing r/technology...\n",
      "  üîç Method: SEARCH from r/technology\n",
      "     Query: 'technology'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'technology'\n",
      "  üîç Method: SEARCH from r/technology\n",
      "     Query: 'science'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'science'\n",
      "  ‚úÖ r/technology: 200 posts total\n",
      "\n",
      "  üìç Processing r/science...\n",
      "  üîç Method: SEARCH from r/science\n",
      "     Query: 'technology'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 76 posts using search\n",
      "     Added 76 posts from search: 'technology'\n",
      "  üîç Method: SEARCH from r/science\n",
      "     Query: 'science'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/science\n",
      "     Query: 'innovation'\n",
      "     ‚úÖ Collected 40 posts using search\n",
      "     Added 40 posts from search: 'innovation'\n",
      "  üîç Method: HOT from r/science\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/science: 196 posts total\n",
      "\n",
      "  üìç Processing r/gadgets...\n",
      "  üîç Method: SEARCH from r/gadgets\n",
      "     Query: 'technology'\n",
      "     ‚úÖ Collected 8 posts using search\n",
      "     Added 8 posts from search: 'technology'\n",
      "  üîç Method: SEARCH from r/gadgets\n",
      "     Query: 'science'\n",
      "     ‚úÖ Collected 7 posts using search\n",
      "     Added 7 posts from search: 'science'\n",
      "  üîç Method: SEARCH from r/gadgets\n",
      "     Query: 'innovation'\n",
      "     ‚úÖ Collected 11 posts using search\n",
      "     Added 11 posts from search: 'innovation'\n",
      "  üîç Method: HOT from r/gadgets\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/gadgets\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/gadgets: 186 posts total\n",
      "\n",
      "  üìç Processing r/tech...\n",
      "  üîç Method: SEARCH from r/tech\n",
      "     Query: 'technology'\n",
      "     ‚úÖ Collected 26 posts using search\n",
      "     Added 26 posts from search: 'technology'\n",
      "  üîç Method: SEARCH from r/tech\n",
      "     Query: 'science'\n",
      "     ‚úÖ Collected 40 posts using search\n",
      "     Added 40 posts from search: 'science'\n",
      "  üîç Method: SEARCH from r/tech\n",
      "     Query: 'innovation'\n",
      "     ‚úÖ Collected 15 posts using search\n",
      "     Added 15 posts from search: 'innovation'\n",
      "  üîç Method: HOT from r/tech\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/tech\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/tech: 241 posts total\n",
      "\n",
      "  üìç Processing r/MachineLearning...\n",
      "  üîç Method: SEARCH from r/MachineLearning\n",
      "     Query: 'technology'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 57 posts using search\n",
      "     Added 57 posts from search: 'technology'\n",
      "  üîç Method: SEARCH from r/MachineLearning\n",
      "     Query: 'science'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'science'\n",
      "  üîç Method: SEARCH from r/MachineLearning\n",
      "     Query: 'innovation'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: HOT from r/MachineLearning\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/MachineLearning: 237 posts total\n",
      "\n",
      "  üìç Processing r/artificial...\n",
      "  üîç Method: SEARCH from r/artificial\n",
      "     Query: 'technology'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'technology'\n",
      "  üîç Method: SEARCH from r/artificial\n",
      "     Query: 'science'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'science'\n",
      "  ‚úÖ r/artificial: 200 posts total\n",
      "\n",
      "  üìç Processing r/programming...\n",
      "  üîç Method: SEARCH from r/programming\n",
      "     Query: 'technology'\n",
      "     ‚úÖ Collected 39 posts using search\n",
      "     Added 39 posts from search: 'technology'\n",
      "  üîç Method: SEARCH from r/programming\n",
      "     Query: 'science'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 62 posts using search\n",
      "     Added 62 posts from search: 'science'\n",
      "  üîç Method: SEARCH from r/programming\n",
      "     Query: 'innovation'\n",
      "     ‚úÖ Collected 32 posts using search\n",
      "     Added 32 posts from search: 'innovation'\n",
      "  üîç Method: HOT from r/programming\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/programming: 213 posts total\n",
      "\n",
      "  üìç Processing r/computers...\n",
      "  üîç Method: SEARCH from r/computers\n",
      "     Query: 'technology'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'technology'\n",
      "  üîç Method: SEARCH from r/computers\n",
      "     Query: 'science'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'science'\n",
      "  ‚úÖ r/computers: 200 posts total\n",
      "\n",
      "  üìä Removed 195 duplicates\n",
      "  ‚úÖ Science_Technology data saved to: reddit_data/Science_Technology_20250823_000331.json\n",
      "     Posts: 1478\n",
      "     Subreddits: 8\n",
      "\n",
      "  üìä SCIENCE TECHNOLOGY SUMMARY:\n",
      "     Total posts: 1478\n",
      "     Posts with text: 407\n",
      "     Text ratio: 27.5%\n",
      "     Subreddits: 8\n",
      "     Average score: 2318.2\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 4/17: HEALTH\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING HEALTH DATA ===\n",
      "\n",
      "  üìç Processing r/Health...\n",
      "  üîç Method: SEARCH from r/Health\n",
      "     Query: 'health'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'health'\n",
      "  üîç Method: SEARCH from r/Health\n",
      "     Query: 'medicine'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 61 posts using search\n",
      "     Added 61 posts from search: 'medicine'\n",
      "  üîç Method: SEARCH from r/Health\n",
      "     Query: 'medical'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'medical'\n",
      "  ‚úÖ r/Health: 261 posts total\n",
      "\n",
      "  üìç Processing r/medicine...\n",
      "  üîç Method: SEARCH from r/medicine\n",
      "     Query: 'health'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'health'\n",
      "  üîç Method: SEARCH from r/medicine\n",
      "     Query: 'medicine'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'medicine'\n",
      "  üîç Method: SEARCH from r/medicine\n",
      "     Query: 'medical'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'medical'\n",
      "  ‚úÖ r/medicine: 300 posts total\n",
      "\n",
      "  üìç Processing r/medical...\n",
      "  üîç Method: SEARCH from r/medical\n",
      "     Query: 'health'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/medical\n",
      "     Query: 'medicine'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/medical\n",
      "     Query: 'medical'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: HOT from r/medical\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/medical\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/medical: 160 posts total\n",
      "\n",
      "  üìç Processing r/healthcare...\n",
      "  üîç Method: SEARCH from r/healthcare\n",
      "     Query: 'health'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'health'\n",
      "  üîç Method: SEARCH from r/healthcare\n",
      "     Query: 'medicine'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 90 posts using search\n",
      "     Added 90 posts from search: 'medicine'\n",
      "  üîç Method: SEARCH from r/healthcare\n",
      "     Query: 'medical'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'medical'\n",
      "  ‚úÖ r/healthcare: 290 posts total\n",
      "\n",
      "  üìç Processing r/nutrition...\n",
      "  üîç Method: SEARCH from r/nutrition\n",
      "     Query: 'health'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'health'\n",
      "  üîç Method: SEARCH from r/nutrition\n",
      "     Query: 'medicine'\n",
      "     ‚úÖ Collected 29 posts using search\n",
      "     Added 29 posts from search: 'medicine'\n",
      "  üîç Method: SEARCH from r/nutrition\n",
      "     Query: 'medical'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 66 posts using search\n",
      "     Added 66 posts from search: 'medical'\n",
      "  üîç Method: HOT from r/nutrition\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/nutrition: 275 posts total\n",
      "\n",
      "  üìç Processing r/fitness...\n",
      "  üîç Method: SEARCH from r/fitness\n",
      "     Query: 'health'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/fitness\n",
      "     Query: 'medicine'\n",
      "     ‚úÖ Collected 2 posts using search\n",
      "     Added 2 posts from search: 'medicine'\n",
      "  üîç Method: SEARCH from r/fitness\n",
      "     Query: 'medical'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'medical'\n",
      "  üîç Method: HOT from r/fitness\n",
      "     ‚úÖ Collected 42 posts using hot\n",
      "     Added 42 posts from hot\n",
      "  üîç Method: NEW from r/fitness\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/fitness: 224 posts total\n",
      "\n",
      "  üìç Processing r/mentalhealth...\n",
      "  üîç Method: SEARCH from r/mentalhealth\n",
      "     Query: 'health'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'health'\n",
      "  üîç Method: SEARCH from r/mentalhealth\n",
      "     Query: 'medicine'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'medicine'\n",
      "  üîç Method: SEARCH from r/mentalhealth\n",
      "     Query: 'medical'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'medical'\n",
      "  ‚úÖ r/mentalhealth: 300 posts total\n",
      "\n",
      "  üìä Removed 205 duplicates\n",
      "  ‚úÖ Health data saved to: reddit_data/Health_20250823_000541.json\n",
      "     Posts: 1605\n",
      "     Subreddits: 7\n",
      "\n",
      "  üìä HEALTH SUMMARY:\n",
      "     Total posts: 1605\n",
      "     Posts with text: 1165\n",
      "     Text ratio: 72.6%\n",
      "     Subreddits: 7\n",
      "     Average score: 167.8\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 5/17: SPORT\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING SPORT DATA ===\n",
      "\n",
      "  üìç Processing r/sports...\n",
      "  üîç Method: SEARCH from r/sports\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  üîç Method: SEARCH from r/sports\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'football'\n",
      "  ‚úÖ r/sports: 200 posts total\n",
      "\n",
      "  üìç Processing r/nfl...\n",
      "  üîç Method: SEARCH from r/nfl\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  üîç Method: SEARCH from r/nfl\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'football'\n",
      "  ‚úÖ r/nfl: 200 posts total\n",
      "\n",
      "  üìç Processing r/nba...\n",
      "  üîç Method: SEARCH from r/nba\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  üîç Method: SEARCH from r/nba\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'football'\n",
      "  ‚úÖ r/nba: 200 posts total\n",
      "\n",
      "  üìç Processing r/soccer...\n",
      "  üîç Method: SEARCH from r/soccer\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  üîç Method: SEARCH from r/soccer\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'football'\n",
      "  ‚úÖ r/soccer: 200 posts total\n",
      "\n",
      "  üìç Processing r/baseball...\n",
      "  üîç Method: SEARCH from r/baseball\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  üîç Method: SEARCH from r/baseball\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'football'\n",
      "  ‚úÖ r/baseball: 200 posts total\n",
      "\n",
      "  üìç Processing r/hockey...\n",
      "  üîç Method: SEARCH from r/hockey\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  üîç Method: SEARCH from r/hockey\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'football'\n",
      "  ‚úÖ r/hockey: 200 posts total\n",
      "\n",
      "  üìç Processing r/olympics...\n",
      "  üîç Method: SEARCH from r/olympics\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  üîç Method: SEARCH from r/olympics\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 58 posts using search\n",
      "     Added 58 posts from search: 'football'\n",
      "  üîç Method: SEARCH from r/olympics\n",
      "     Query: 'basketball'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 53 posts using search\n",
      "     Added 53 posts from search: 'basketball'\n",
      "  ‚úÖ r/olympics: 211 posts total\n",
      "\n",
      "  üìç Processing r/football...\n",
      "  üîç Method: SEARCH from r/football\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  üîç Method: SEARCH from r/football\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'football'\n",
      "  ‚úÖ r/football: 200 posts total\n",
      "\n",
      "  üìä Removed 51 duplicates\n",
      "  ‚úÖ Sport data saved to: reddit_data/Sport_20250823_000732.json\n",
      "     Posts: 1560\n",
      "     Subreddits: 8\n",
      "\n",
      "  üìä SPORT SUMMARY:\n",
      "     Total posts: 1560\n",
      "     Posts with text: 419\n",
      "     Text ratio: 26.9%\n",
      "     Subreddits: 8\n",
      "     Average score: 3217.7\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 6/17: ARTS CULTURE ENTERTAINMENT MEDIA\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING ARTS CULTURE ENTERTAINMENT MEDIA DATA ===\n",
      "\n",
      "  üìç Processing r/movies...\n",
      "  üîç Method: SEARCH from r/movies\n",
      "     Query: 'movies'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'movies'\n",
      "  üîç Method: SEARCH from r/movies\n",
      "     Query: 'music'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'music'\n",
      "  ‚úÖ r/movies: 200 posts total\n",
      "\n",
      "  üìç Processing r/music...\n",
      "  üîç Method: SEARCH from r/music\n",
      "     Query: 'movies'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'movies'\n",
      "  üîç Method: SEARCH from r/music\n",
      "     Query: 'music'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'music'\n",
      "  ‚úÖ r/music: 200 posts total\n",
      "\n",
      "  üìç Processing r/art...\n",
      "  üîç Method: SEARCH from r/art\n",
      "     Query: 'movies'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 74 posts using search\n",
      "     Added 74 posts from search: 'movies'\n",
      "  üîç Method: SEARCH from r/art\n",
      "     Query: 'music'\n",
      "     ‚úÖ Collected 43 posts using search\n",
      "     Added 43 posts from search: 'music'\n",
      "  üîç Method: SEARCH from r/art\n",
      "     Query: 'art'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'art'\n",
      "  ‚úÖ r/art: 217 posts total\n",
      "\n",
      "  üìç Processing r/books...\n",
      "  üîç Method: SEARCH from r/books\n",
      "     Query: 'movies'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'movies'\n",
      "  üîç Method: SEARCH from r/books\n",
      "     Query: 'music'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 70 posts using search\n",
      "     Added 70 posts from search: 'music'\n",
      "  üîç Method: SEARCH from r/books\n",
      "     Query: 'art'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'art'\n",
      "  ‚úÖ r/books: 270 posts total\n",
      "\n",
      "  üìç Processing r/television...\n",
      "  üîç Method: SEARCH from r/television\n",
      "     Query: 'movies'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'movies'\n",
      "  üîç Method: SEARCH from r/television\n",
      "     Query: 'music'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'music'\n",
      "  ‚úÖ r/television: 200 posts total\n",
      "\n",
      "  üìç Processing r/entertainment...\n",
      "  üîç Method: SEARCH from r/entertainment\n",
      "     Query: 'movies'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/entertainment\n",
      "     Query: 'music'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'music'\n",
      "  üîç Method: SEARCH from r/entertainment\n",
      "     Query: 'art'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'art'\n",
      "  ‚úÖ r/entertainment: 200 posts total\n",
      "\n",
      "  üìç Processing r/culture...\n",
      "  üîç Method: SEARCH from r/culture\n",
      "     Query: 'movies'\n",
      "     ‚úÖ Collected 13 posts using search\n",
      "     Added 13 posts from search: 'movies'\n",
      "  üîç Method: SEARCH from r/culture\n",
      "     Query: 'music'\n",
      "     ‚úÖ Collected 30 posts using search\n",
      "     Added 30 posts from search: 'music'\n",
      "  üîç Method: SEARCH from r/culture\n",
      "     Query: 'art'\n",
      "     ‚úÖ Collected 32 posts using search\n",
      "     Added 32 posts from search: 'art'\n",
      "  üîç Method: HOT from r/culture\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/culture\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/culture: 235 posts total\n",
      "\n",
      "  üìç Processing r/media...\n",
      "  üîç Method: SEARCH from r/media\n",
      "     Query: 'movies'\n",
      "     ‚ùå Error with search method: received 404 HTTP response\n",
      "  üîç Method: SEARCH from r/media\n",
      "     Query: 'music'\n",
      "     ‚ùå Error with search method: received 404 HTTP response\n",
      "  üîç Method: SEARCH from r/media\n",
      "     Query: 'art'\n",
      "     ‚ùå Error with search method: received 404 HTTP response\n",
      "  üîç Method: HOT from r/media\n",
      "     ‚ùå Error with hot method: received 404 HTTP response\n",
      "  üîç Method: NEW from r/media\n",
      "     ‚ùå Error with new method: received 404 HTTP response\n",
      "  ‚úÖ r/media: 0 posts total\n",
      "\n",
      "  üìä Removed 121 duplicates\n",
      "  ‚úÖ Arts_Culture_Entertainment_Media data saved to: reddit_data/Arts_Culture_Entertainment_Media_20250823_000935.json\n",
      "     Posts: 1401\n",
      "     Subreddits: 7\n",
      "\n",
      "  üìä ARTS CULTURE ENTERTAINMENT MEDIA SUMMARY:\n",
      "     Total posts: 1401\n",
      "     Posts with text: 550\n",
      "     Text ratio: 39.3%\n",
      "     Subreddits: 7\n",
      "     Average score: 2243.2\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 7/17: SOCIETY\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING SOCIETY DATA ===\n",
      "\n",
      "  üìç Processing r/sociology...\n",
      "  üîç Method: SEARCH from r/sociology\n",
      "     Query: 'society'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'society'\n",
      "  üîç Method: SEARCH from r/sociology\n",
      "     Query: 'social'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'social'\n",
      "  üîç Method: SEARCH from r/sociology\n",
      "     Query: 'community'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 65 posts using search\n",
      "     Added 65 posts from search: 'community'\n",
      "  ‚úÖ r/sociology: 265 posts total\n",
      "\n",
      "  üìç Processing r/society...\n",
      "  üîç Method: SEARCH from r/society\n",
      "     Query: 'society'\n",
      "     ‚ùå Error with search method: received 403 HTTP response\n",
      "  üîç Method: SEARCH from r/society\n",
      "     Query: 'social'\n",
      "     ‚ùå Error with search method: received 403 HTTP response\n",
      "  üîç Method: SEARCH from r/society\n",
      "     Query: 'community'\n",
      "     ‚ùå Error with search method: received 403 HTTP response\n",
      "  üîç Method: HOT from r/society\n",
      "     ‚ùå Error with hot method: received 403 HTTP response\n",
      "  üîç Method: NEW from r/society\n",
      "     ‚ùå Error with new method: received 403 HTTP response\n",
      "  ‚úÖ r/society: 0 posts total\n",
      "\n",
      "  üìç Processing r/social...\n",
      "  üîç Method: SEARCH from r/social\n",
      "     Query: 'society'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/social\n",
      "     Query: 'social'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/social\n",
      "     Query: 'community'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: HOT from r/social\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/social\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/social: 160 posts total\n",
      "\n",
      "  üìç Processing r/community...\n",
      "  üîç Method: SEARCH from r/community\n",
      "     Query: 'society'\n",
      "     ‚úÖ Collected 3 posts using search\n",
      "     Added 3 posts from search: 'society'\n",
      "  üîç Method: SEARCH from r/community\n",
      "     Query: 'social'\n",
      "     ‚úÖ Collected 19 posts using search\n",
      "     Added 19 posts from search: 'social'\n",
      "  üîç Method: SEARCH from r/community\n",
      "     Query: 'community'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'community'\n",
      "  üîç Method: HOT from r/community\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/community\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/community: 282 posts total\n",
      "\n",
      "  üìç Processing r/TrueReddit...\n",
      "  üîç Method: SEARCH from r/TrueReddit\n",
      "     Query: 'society'\n",
      "     ‚úÖ Collected 10 posts using search\n",
      "     Added 10 posts from search: 'society'\n",
      "  üîç Method: SEARCH from r/TrueReddit\n",
      "     Query: 'social'\n",
      "     ‚úÖ Collected 41 posts using search\n",
      "     Added 41 posts from search: 'social'\n",
      "  üîç Method: SEARCH from r/TrueReddit\n",
      "     Query: 'community'\n",
      "     ‚úÖ Collected 4 posts using search\n",
      "     Added 4 posts from search: 'community'\n",
      "  üîç Method: HOT from r/TrueReddit\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/TrueReddit\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/TrueReddit: 215 posts total\n",
      "\n",
      "  üìç Processing r/news...\n",
      "  üîç Method: SEARCH from r/news\n",
      "     Query: 'society'\n",
      "     ‚úÖ Collected 26 posts using search\n",
      "     Added 26 posts from search: 'society'\n",
      "  üîç Method: SEARCH from r/news\n",
      "     Query: 'social'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 81 posts using search\n",
      "     Added 81 posts from search: 'social'\n",
      "  üîç Method: SEARCH from r/news\n",
      "     Query: 'community'\n",
      "     ‚úÖ Collected 18 posts using search\n",
      "     Added 18 posts from search: 'community'\n",
      "  üîç Method: HOT from r/news\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/news\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/news: 285 posts total\n",
      "\n",
      "  üìä Removed 357 duplicates\n",
      "  ‚úÖ Society data saved to: reddit_data/Society_20250823_001122.json\n",
      "     Posts: 850\n",
      "     Subreddits: 5\n",
      "\n",
      "  üìä SOCIETY SUMMARY:\n",
      "     Total posts: 850\n",
      "     Posts with text: 339\n",
      "     Text ratio: 39.9%\n",
      "     Subreddits: 5\n",
      "     Average score: 2397.7\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 8/17: HUMAN INTEREST\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING HUMAN INTEREST DATA ===\n",
      "\n",
      "  üìç Processing r/UpliftingNews...\n",
      "  üîç Method: SEARCH from r/UpliftingNews\n",
      "     Query: 'human interest'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/UpliftingNews\n",
      "     Query: 'inspiring'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 50 posts using search\n",
      "     Added 50 posts from search: 'inspiring'\n",
      "  üîç Method: SEARCH from r/UpliftingNews\n",
      "     Query: 'heartwarming'\n",
      "     ‚úÖ Collected 26 posts using search\n",
      "     Added 26 posts from search: 'heartwarming'\n",
      "  üîç Method: HOT from r/UpliftingNews\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/UpliftingNews\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/UpliftingNews: 236 posts total\n",
      "\n",
      "  üìç Processing r/MadeMeSmile...\n",
      "  üîç Method: SEARCH from r/MadeMeSmile\n",
      "     Query: 'human interest'\n",
      "     ‚úÖ Collected 6 posts using search\n",
      "     Added 6 posts from search: 'human interest'\n",
      "  üîç Method: SEARCH from r/MadeMeSmile\n",
      "     Query: 'inspiring'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'inspiring'\n",
      "  üîç Method: SEARCH from r/MadeMeSmile\n",
      "     Query: 'heartwarming'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'heartwarming'\n",
      "  üîç Method: HOT from r/MadeMeSmile\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/MadeMeSmile: 286 posts total\n",
      "\n",
      "  üìç Processing r/happy...\n",
      "  üîç Method: SEARCH from r/happy\n",
      "     Query: 'human interest'\n",
      "     ‚úÖ Collected 1 posts using search\n",
      "     Added 1 posts from search: 'human interest'\n",
      "  üîç Method: SEARCH from r/happy\n",
      "     Query: 'inspiring'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 64 posts using search\n",
      "     Added 64 posts from search: 'inspiring'\n",
      "  üîç Method: SEARCH from r/happy\n",
      "     Query: 'heartwarming'\n",
      "     ‚úÖ Collected 9 posts using search\n",
      "     Added 9 posts from search: 'heartwarming'\n",
      "  üîç Method: HOT from r/happy\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/happy\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/happy: 234 posts total\n",
      "\n",
      "  üìç Processing r/wholesome...\n",
      "  üîç Method: SEARCH from r/wholesome\n",
      "     Query: 'human interest'\n",
      "     ‚úÖ Collected 1 posts using search\n",
      "     Added 1 posts from search: 'human interest'\n",
      "  üîç Method: SEARCH from r/wholesome\n",
      "     Query: 'inspiring'\n",
      "     ‚úÖ Collected 21 posts using search\n",
      "     Added 21 posts from search: 'inspiring'\n",
      "  üîç Method: SEARCH from r/wholesome\n",
      "     Query: 'heartwarming'\n",
      "     ‚úÖ Collected 29 posts using search\n",
      "     Added 29 posts from search: 'heartwarming'\n",
      "  üîç Method: HOT from r/wholesome\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/wholesome\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/wholesome: 211 posts total\n",
      "\n",
      "  üìç Processing r/HumansBeingBros...\n",
      "  üîç Method: SEARCH from r/HumansBeingBros\n",
      "     Query: 'human interest'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/HumansBeingBros\n",
      "     Query: 'inspiring'\n",
      "     ‚úÖ Collected 5 posts using search\n",
      "     Added 5 posts from search: 'inspiring'\n",
      "  üîç Method: SEARCH from r/HumansBeingBros\n",
      "     Query: 'heartwarming'\n",
      "     ‚úÖ Collected 11 posts using search\n",
      "     Added 11 posts from search: 'heartwarming'\n",
      "  üîç Method: HOT from r/HumansBeingBros\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/HumansBeingBros\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/HumansBeingBros: 176 posts total\n",
      "\n",
      "  üìç Processing r/todayilearned...\n",
      "  üîç Method: SEARCH from r/todayilearned\n",
      "     Query: 'human interest'\n",
      "     ‚úÖ Collected 10 posts using search\n",
      "     Added 10 posts from search: 'human interest'\n",
      "  üîç Method: SEARCH from r/todayilearned\n",
      "     Query: 'inspiring'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'inspiring'\n",
      "  üîç Method: SEARCH from r/todayilearned\n",
      "     Query: 'heartwarming'\n",
      "     ‚úÖ Collected 5 posts using search\n",
      "     Added 5 posts from search: 'heartwarming'\n",
      "  üîç Method: HOT from r/todayilearned\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/todayilearned\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/todayilearned: 275 posts total\n",
      "\n",
      "  üìä Removed 409 duplicates\n",
      "  ‚úÖ Human_Interest data saved to: reddit_data/Human_Interest_20250823_001325.json\n",
      "     Posts: 1009\n",
      "     Subreddits: 6\n",
      "\n",
      "  üìä HUMAN INTEREST SUMMARY:\n",
      "     Total posts: 1009\n",
      "     Posts with text: 115\n",
      "     Text ratio: 11.4%\n",
      "     Subreddits: 6\n",
      "     Average score: 8879.6\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 9/17: CRIME LAW JUSTICE\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING CRIME LAW JUSTICE DATA ===\n",
      "\n",
      "  üìç Processing r/law...\n",
      "  üîç Method: SEARCH from r/law\n",
      "     Query: 'crime'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'crime'\n",
      "  üîç Method: SEARCH from r/law\n",
      "     Query: 'law'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'law'\n",
      "  üîç Method: SEARCH from r/law\n",
      "     Query: 'justice'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'justice'\n",
      "  ‚úÖ r/law: 300 posts total\n",
      "\n",
      "  üìç Processing r/legaladvice...\n",
      "  üîç Method: SEARCH from r/legaladvice\n",
      "     Query: 'crime'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'crime'\n",
      "  üîç Method: SEARCH from r/legaladvice\n",
      "     Query: 'law'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'law'\n",
      "  üîç Method: SEARCH from r/legaladvice\n",
      "     Query: 'justice'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'justice'\n",
      "  ‚úÖ r/legaladvice: 300 posts total\n",
      "\n",
      "  üìç Processing r/UnresolvedMysteries...\n",
      "  üîç Method: SEARCH from r/UnresolvedMysteries\n",
      "     Query: 'crime'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'crime'\n",
      "  üîç Method: SEARCH from r/UnresolvedMysteries\n",
      "     Query: 'law'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'law'\n",
      "  üîç Method: SEARCH from r/UnresolvedMysteries\n",
      "     Query: 'justice'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'justice'\n",
      "  ‚úÖ r/UnresolvedMysteries: 300 posts total\n",
      "\n",
      "  üìç Processing r/TrueCrime...\n",
      "  üîç Method: SEARCH from r/TrueCrime\n",
      "     Query: 'crime'\n",
      "     ‚úÖ Collected 32 posts using search\n",
      "     Added 32 posts from search: 'crime'\n",
      "  üîç Method: SEARCH from r/TrueCrime\n",
      "     Query: 'law'\n",
      "     ‚úÖ Collected 8 posts using search\n",
      "     Added 8 posts from search: 'law'\n",
      "  üîç Method: SEARCH from r/TrueCrime\n",
      "     Query: 'justice'\n",
      "     ‚úÖ Collected 8 posts using search\n",
      "     Added 8 posts from search: 'justice'\n",
      "  üîç Method: HOT from r/TrueCrime\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/TrueCrime\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/TrueCrime: 208 posts total\n",
      "\n",
      "  üìç Processing r/justice...\n",
      "  üîç Method: SEARCH from r/justice\n",
      "     Query: 'crime'\n",
      "     ‚ùå Error with search method: received 403 HTTP response\n",
      "  üîç Method: SEARCH from r/justice\n",
      "     Query: 'law'\n",
      "     ‚ùå Error with search method: received 403 HTTP response\n",
      "  üîç Method: SEARCH from r/justice\n",
      "     Query: 'justice'\n",
      "     ‚ùå Error with search method: received 403 HTTP response\n",
      "  üîç Method: HOT from r/justice\n",
      "     ‚ùå Error with hot method: received 403 HTTP response\n",
      "  üîç Method: NEW from r/justice\n",
      "     ‚ùå Error with new method: received 403 HTTP response\n",
      "  ‚úÖ r/justice: 0 posts total\n",
      "\n",
      "  üìç Processing r/police...\n",
      "  üîç Method: SEARCH from r/police\n",
      "     Query: 'crime'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'crime'\n",
      "  üîç Method: SEARCH from r/police\n",
      "     Query: 'law'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/police/search/?q=law&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205552B70>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: SEARCH from r/police\n",
      "     Query: 'justice'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/police/search/?q=justice&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205490260>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: HOT from r/police\n",
      "     ‚ùå Error with hot method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/police/hot?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015204672300>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: NEW from r/police\n",
      "     ‚ùå Error with new method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/police/new?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152054932F0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  ‚úÖ r/police: 100 posts total\n",
      "\n",
      "  üìä Removed 210 duplicates\n",
      "  ‚úÖ Crime_Law_Justice data saved to: reddit_data/Crime_Law_Justice_20250823_001548.json\n",
      "     Posts: 998\n",
      "     Subreddits: 5\n",
      "\n",
      "  üìä CRIME LAW JUSTICE SUMMARY:\n",
      "     Total posts: 998\n",
      "     Posts with text: 675\n",
      "     Text ratio: 67.6%\n",
      "     Subreddits: 5\n",
      "     Average score: 4161.3\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 10/17: CONFLICT WAR PEACE\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING CONFLICT WAR PEACE DATA ===\n",
      "\n",
      "  üìç Processing r/worldnews...\n",
      "  üîç Method: SEARCH from r/worldnews\n",
      "     Query: 'war'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/worldnews/search/?q=war&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205550500>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: SEARCH from r/worldnews\n",
      "     Query: 'conflict'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/worldnews/search/?q=conflict&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205552BA0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: SEARCH from r/worldnews\n",
      "     Query: 'military'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/worldnews/search/?q=military&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205553CE0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: HOT from r/worldnews\n",
      "     ‚ùå Error with hot method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/worldnews/hot?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205551AC0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: NEW from r/worldnews\n",
      "     ‚ùå Error with new method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/worldnews/new?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015204641A90>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  ‚úÖ r/worldnews: 0 posts total\n",
      "\n",
      "  üìç Processing r/geopolitics...\n",
      "  üîç Method: SEARCH from r/geopolitics\n",
      "     Query: 'war'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/geopolitics/search/?q=war&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205553830>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: SEARCH from r/geopolitics\n",
      "     Query: 'conflict'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/geopolitics/search/?q=conflict&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152053CDE80>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: SEARCH from r/geopolitics\n",
      "     Query: 'military'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/geopolitics/search/?q=military&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152055530E0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: HOT from r/geopolitics\n",
      "     ‚ùå Error with hot method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/geopolitics/hot?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205550F50>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: NEW from r/geopolitics\n",
      "     ‚ùå Error with new method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/geopolitics/new?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152053CD0A0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  ‚úÖ r/geopolitics: 0 posts total\n",
      "\n",
      "  üìç Processing r/Military...\n",
      "  üîç Method: SEARCH from r/Military\n",
      "     Query: 'war'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/Military/search/?q=war&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205551280>: Failed to resolve 'oauth.reddit.com' ([Errno 11002] getaddrinfo failed)\"))\n",
      "  üîç Method: SEARCH from r/Military\n",
      "     Query: 'conflict'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/Military/search/?q=conflict&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152053CD520>: Failed to resolve 'oauth.reddit.com' ([Errno 11002] getaddrinfo failed)\"))\n",
      "  üîç Method: SEARCH from r/Military\n",
      "     Query: 'military'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/Military/search/?q=military&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205550890>: Failed to resolve 'oauth.reddit.com' ([Errno 11002] getaddrinfo failed)\"))\n",
      "  üîç Method: HOT from r/Military\n",
      "     ‚ùå Error with hot method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/Military/hot?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152053CFB00>: Failed to resolve 'oauth.reddit.com' ([Errno 11002] getaddrinfo failed)\"))\n",
      "  üîç Method: NEW from r/Military\n",
      "     ‚ùå Error with new method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/Military/new?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152046719A0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  ‚úÖ r/Military: 0 posts total\n",
      "\n",
      "  üìç Processing r/CredibleDefense...\n",
      "  üîç Method: SEARCH from r/CredibleDefense\n",
      "     Query: 'war'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/CredibleDefense/search/?q=war&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205551E80>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: SEARCH from r/CredibleDefense\n",
      "     Query: 'conflict'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/CredibleDefense/search/?q=conflict&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152055B5B20>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: SEARCH from r/CredibleDefense\n",
      "     Query: 'military'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/CredibleDefense/search/?q=military&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205553140>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: HOT from r/CredibleDefense\n",
      "     ‚ùå Error with hot method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/CredibleDefense/hot?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152055B71A0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: NEW from r/CredibleDefense\n",
      "     ‚ùå Error with new method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/CredibleDefense/new?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152055B6F00>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  ‚úÖ r/CredibleDefense: 0 posts total\n",
      "\n",
      "  üìç Processing r/conflict...\n",
      "  üîç Method: SEARCH from r/conflict\n",
      "     Query: 'war'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/conflict/search/?q=war&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152055507A0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: SEARCH from r/conflict\n",
      "     Query: 'conflict'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/conflict/search/?q=conflict&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152055B41A0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: SEARCH from r/conflict\n",
      "     Query: 'military'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/conflict/search/?q=military&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152055B7320>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: HOT from r/conflict\n",
      "     ‚ùå Error with hot method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/conflict/hot?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152055517C0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: NEW from r/conflict\n",
      "     ‚ùå Error with new method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/conflict/new?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205493020>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  ‚úÖ r/conflict: 0 posts total\n",
      "  ‚ùå No data collected for Conflict_War_Peace\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 11/17: DISASTER ACCIDENT EMERGENCY\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING DISASTER ACCIDENT EMERGENCY DATA ===\n",
      "\n",
      "  üìç Processing r/worldnews...\n",
      "  üîç Method: SEARCH from r/worldnews\n",
      "     Query: 'disaster'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/worldnews/search/?q=disaster&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152053CF200>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: SEARCH from r/worldnews\n",
      "     Query: 'emergency'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/worldnews/search/?q=emergency&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152053CD2E0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: SEARCH from r/worldnews\n",
      "     Query: 'accident'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/worldnews/search/?q=accident&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015204672180>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: HOT from r/worldnews\n",
      "     ‚ùå Error with hot method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/worldnews/hot?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152053CE450>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: NEW from r/worldnews\n",
      "     ‚ùå Error with new method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/worldnews/new?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152055512B0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  ‚úÖ r/worldnews: 0 posts total\n",
      "\n",
      "  üìç Processing r/news...\n",
      "  üîç Method: SEARCH from r/news\n",
      "     Query: 'disaster'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/news/search/?q=disaster&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152053CDFD0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: SEARCH from r/news\n",
      "     Query: 'emergency'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/news/search/?q=emergency&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015204673620>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: SEARCH from r/news\n",
      "     Query: 'accident'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/news/search/?q=accident&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015204673620>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: HOT from r/news\n",
      "     ‚ùå Error with hot method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/news/hot?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152053CE510>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: NEW from r/news\n",
      "     ‚ùå Error with new method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/news/new?limit=80&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152055501A0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  ‚úÖ r/news: 0 posts total\n",
      "\n",
      "  üìç Processing r/CatastrophicFailure...\n",
      "  üîç Method: SEARCH from r/CatastrophicFailure\n",
      "     Query: 'disaster'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/CatastrophicFailure/search/?q=disaster&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000152053CEE40>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: SEARCH from r/CatastrophicFailure\n",
      "     Query: 'emergency'\n",
      "     ‚ùå Error with search method: error with request HTTPSConnectionPool(host='oauth.reddit.com', port=443): Max retries exceeded with url: /r/CatastrophicFailure/search/?q=emergency&restrict_sr=True&sort=relevance&syntax=lucene&t=year&limit=100&raw_json=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000015205550BF0>: Failed to resolve 'oauth.reddit.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "  üîç Method: SEARCH from r/CatastrophicFailure\n",
      "     Query: 'accident'\n",
      "     ‚úÖ Collected 34 posts using search\n",
      "     Added 34 posts from search: 'accident'\n",
      "  üîç Method: HOT from r/CatastrophicFailure\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/CatastrophicFailure\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/CatastrophicFailure: 194 posts total\n",
      "\n",
      "  üìç Processing r/emergencyservices...\n",
      "  üîç Method: SEARCH from r/emergencyservices\n",
      "     Query: 'disaster'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/emergencyservices\n",
      "     Query: 'emergency'\n",
      "     ‚úÖ Collected 2 posts using search\n",
      "     Added 2 posts from search: 'emergency'\n",
      "  üîç Method: SEARCH from r/emergencyservices\n",
      "     Query: 'accident'\n",
      "     ‚úÖ Collected 1 posts using search\n",
      "     Added 1 posts from search: 'accident'\n",
      "  üîç Method: HOT from r/emergencyservices\n",
      "     ‚úÖ Collected 22 posts using hot\n",
      "     Added 22 posts from hot\n",
      "  üîç Method: NEW from r/emergencyservices\n",
      "     ‚úÖ Collected 22 posts using new\n",
      "     Added 22 posts from new\n",
      "  ‚úÖ r/emergencyservices: 47 posts total\n",
      "\n",
      "  üìä Removed 109 duplicates\n",
      "  ‚úÖ Disaster_Accident_Emergency data saved to: reddit_data/Disaster_Accident_Emergency_20250823_002100.json\n",
      "     Posts: 132\n",
      "     Subreddits: 2\n",
      "\n",
      "  üìä DISASTER ACCIDENT EMERGENCY SUMMARY:\n",
      "     Total posts: 132\n",
      "     Posts with text: 15\n",
      "     Text ratio: 11.4%\n",
      "     Subreddits: 2\n",
      "     Average score: 949.8\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 12/17: ENVIRONMENT\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING ENVIRONMENT DATA ===\n",
      "\n",
      "  üìç Processing r/environment...\n",
      "  üîç Method: SEARCH from r/environment\n",
      "     Query: 'environment'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'environment'\n",
      "  üîç Method: SEARCH from r/environment\n",
      "     Query: 'climate change'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'climate change'\n",
      "  üîç Method: SEARCH from r/environment\n",
      "     Query: 'sustainability'\n",
      "     ‚úÖ Collected 47 posts using search\n",
      "     Added 47 posts from search: 'sustainability'\n",
      "  üîç Method: HOT from r/environment\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/environment: 327 posts total\n",
      "\n",
      "  üìç Processing r/climatechange...\n",
      "  üîç Method: SEARCH from r/climatechange\n",
      "     Query: 'environment'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 77 posts using search\n",
      "     Added 77 posts from search: 'environment'\n",
      "  üîç Method: SEARCH from r/climatechange\n",
      "     Query: 'climate change'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'climate change'\n",
      "  üîç Method: SEARCH from r/climatechange\n",
      "     Query: 'sustainability'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 94 posts using search\n",
      "     Added 94 posts from search: 'sustainability'\n",
      "  ‚úÖ r/climatechange: 271 posts total\n",
      "\n",
      "  üìç Processing r/sustainability...\n",
      "  üîç Method: SEARCH from r/sustainability\n",
      "     Query: 'environment'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 61 posts using search\n",
      "     Added 61 posts from search: 'environment'\n",
      "  üîç Method: SEARCH from r/sustainability\n",
      "     Query: 'climate change'\n",
      "     ‚úÖ Collected 40 posts using search\n",
      "     Added 40 posts from search: 'climate change'\n",
      "  üîç Method: SEARCH from r/sustainability\n",
      "     Query: 'sustainability'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sustainability'\n",
      "  üîç Method: HOT from r/sustainability\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/sustainability: 281 posts total\n",
      "\n",
      "  üìç Processing r/nature...\n",
      "  üîç Method: SEARCH from r/nature\n",
      "     Query: 'environment'\n",
      "     ‚úÖ Collected 10 posts using search\n",
      "     Added 10 posts from search: 'environment'\n",
      "  üîç Method: SEARCH from r/nature\n",
      "     Query: 'climate change'\n",
      "     ‚úÖ Collected 18 posts using search\n",
      "     Added 18 posts from search: 'climate change'\n",
      "  üîç Method: SEARCH from r/nature\n",
      "     Query: 'sustainability'\n",
      "     ‚úÖ Collected 5 posts using search\n",
      "     Added 5 posts from search: 'sustainability'\n",
      "  üîç Method: HOT from r/nature\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/nature\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/nature: 193 posts total\n",
      "\n",
      "  üìç Processing r/ecology...\n",
      "  üîç Method: SEARCH from r/ecology\n",
      "     Query: 'environment'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'environment'\n",
      "  üîç Method: SEARCH from r/ecology\n",
      "     Query: 'climate change'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 50 posts using search\n",
      "     Added 50 posts from search: 'climate change'\n",
      "  üîç Method: SEARCH from r/ecology\n",
      "     Query: 'sustainability'\n",
      "     ‚úÖ Collected 46 posts using search\n",
      "     Added 46 posts from search: 'sustainability'\n",
      "  üîç Method: HOT from r/ecology\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/ecology: 276 posts total\n",
      "\n",
      "  üìç Processing r/green...\n",
      "  üîç Method: SEARCH from r/green\n",
      "     Query: 'environment'\n",
      "     ‚úÖ Collected 13 posts using search\n",
      "     Added 13 posts from search: 'environment'\n",
      "  üîç Method: SEARCH from r/green\n",
      "     Query: 'climate change'\n",
      "     ‚úÖ Collected 16 posts using search\n",
      "     Added 16 posts from search: 'climate change'\n",
      "  üîç Method: SEARCH from r/green\n",
      "     Query: 'sustainability'\n",
      "     ‚úÖ Collected 29 posts using search\n",
      "     Added 29 posts from search: 'sustainability'\n",
      "  üîç Method: HOT from r/green\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/green\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/green: 218 posts total\n",
      "\n",
      "  üìä Removed 285 duplicates\n",
      "  ‚úÖ Environment data saved to: reddit_data/Environment_20250823_002255.json\n",
      "     Posts: 1281\n",
      "     Subreddits: 6\n",
      "\n",
      "  üìä ENVIRONMENT SUMMARY:\n",
      "     Total posts: 1281\n",
      "     Posts with text: 519\n",
      "     Text ratio: 40.5%\n",
      "     Subreddits: 6\n",
      "     Average score: 351.0\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 13/17: EDUCATION\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING EDUCATION DATA ===\n",
      "\n",
      "  üìç Processing r/education...\n",
      "  üîç Method: SEARCH from r/education\n",
      "     Query: 'education'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'education'\n",
      "  üîç Method: SEARCH from r/education\n",
      "     Query: 'school'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'school'\n",
      "  üîç Method: SEARCH from r/education\n",
      "     Query: 'university'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'university'\n",
      "  ‚úÖ r/education: 300 posts total\n",
      "\n",
      "  üìç Processing r/teachers...\n",
      "  üîç Method: SEARCH from r/teachers\n",
      "     Query: 'education'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'education'\n",
      "  üîç Method: SEARCH from r/teachers\n",
      "     Query: 'school'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'school'\n",
      "  üîç Method: SEARCH from r/teachers\n",
      "     Query: 'university'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'university'\n",
      "  ‚úÖ r/teachers: 300 posts total\n",
      "\n",
      "  üìç Processing r/college...\n",
      "  üîç Method: SEARCH from r/college\n",
      "     Query: 'education'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'education'\n",
      "  üîç Method: SEARCH from r/college\n",
      "     Query: 'school'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'school'\n",
      "  üîç Method: SEARCH from r/college\n",
      "     Query: 'university'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'university'\n",
      "  ‚úÖ r/college: 300 posts total\n",
      "\n",
      "  üìç Processing r/university...\n",
      "  üîç Method: SEARCH from r/university\n",
      "     Query: 'education'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'education'\n",
      "  üîç Method: SEARCH from r/university\n",
      "     Query: 'school'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'school'\n",
      "  üîç Method: SEARCH from r/university\n",
      "     Query: 'university'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'university'\n",
      "  ‚úÖ r/university: 300 posts total\n",
      "\n",
      "  üìç Processing r/students...\n",
      "  üîç Method: SEARCH from r/students\n",
      "     Query: 'education'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'education'\n",
      "  üîç Method: SEARCH from r/students\n",
      "     Query: 'school'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'school'\n",
      "  üîç Method: SEARCH from r/students\n",
      "     Query: 'university'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'university'\n",
      "  ‚úÖ r/students: 300 posts total\n",
      "\n",
      "  üìç Processing r/Academia...\n",
      "  üîç Method: SEARCH from r/Academia\n",
      "     Query: 'education'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'education'\n",
      "  üîç Method: SEARCH from r/Academia\n",
      "     Query: 'school'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'school'\n",
      "  üîç Method: SEARCH from r/Academia\n",
      "     Query: 'university'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'university'\n",
      "  ‚úÖ r/Academia: 300 posts total\n",
      "\n",
      "  üìä Removed 135 duplicates\n",
      "  ‚úÖ Education data saved to: reddit_data/Education_20250823_002444.json\n",
      "     Posts: 1665\n",
      "     Subreddits: 6\n",
      "\n",
      "  üìä EDUCATION SUMMARY:\n",
      "     Total posts: 1665\n",
      "     Posts with text: 1469\n",
      "     Text ratio: 88.2%\n",
      "     Subreddits: 6\n",
      "     Average score: 435.6\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 14/17: LABOUR\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING LABOUR DATA ===\n",
      "\n",
      "  üìç Processing r/jobs...\n",
      "  üîç Method: SEARCH from r/jobs\n",
      "     Query: 'jobs'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'jobs'\n",
      "  üîç Method: SEARCH from r/jobs\n",
      "     Query: 'employment'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'employment'\n",
      "  üîç Method: SEARCH from r/jobs\n",
      "     Query: 'labor'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'labor'\n",
      "  ‚úÖ r/jobs: 300 posts total\n",
      "\n",
      "  üìç Processing r/antiwork...\n",
      "  üîç Method: SEARCH from r/antiwork\n",
      "     Query: 'jobs'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'jobs'\n",
      "  üîç Method: SEARCH from r/antiwork\n",
      "     Query: 'employment'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'employment'\n",
      "  üîç Method: SEARCH from r/antiwork\n",
      "     Query: 'labor'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'labor'\n",
      "  ‚úÖ r/antiwork: 300 posts total\n",
      "\n",
      "  üìç Processing r/WorkReform...\n",
      "  üîç Method: SEARCH from r/WorkReform\n",
      "     Query: 'jobs'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'jobs'\n",
      "  üîç Method: SEARCH from r/WorkReform\n",
      "     Query: 'employment'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'employment'\n",
      "  üîç Method: SEARCH from r/WorkReform\n",
      "     Query: 'labor'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'labor'\n",
      "  ‚úÖ r/WorkReform: 300 posts total\n",
      "\n",
      "  üìç Processing r/labor...\n",
      "  üîç Method: SEARCH from r/labor\n",
      "     Query: 'jobs'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 70 posts using search\n",
      "     Added 70 posts from search: 'jobs'\n",
      "  üîç Method: SEARCH from r/labor\n",
      "     Query: 'employment'\n",
      "     ‚úÖ Collected 40 posts using search\n",
      "     Added 40 posts from search: 'employment'\n",
      "  üîç Method: SEARCH from r/labor\n",
      "     Query: 'labor'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'labor'\n",
      "  üîç Method: HOT from r/labor\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/labor: 290 posts total\n",
      "\n",
      "  üìç Processing r/union...\n",
      "  üîç Method: SEARCH from r/union\n",
      "     Query: 'jobs'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'jobs'\n",
      "  üîç Method: SEARCH from r/union\n",
      "     Query: 'employment'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'employment'\n",
      "  üîç Method: SEARCH from r/union\n",
      "     Query: 'labor'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'labor'\n",
      "  ‚úÖ r/union: 300 posts total\n",
      "\n",
      "  üìç Processing r/employment...\n",
      "  üîç Method: SEARCH from r/employment\n",
      "     Query: 'jobs'\n",
      "     ‚úÖ Collected 48 posts using search\n",
      "     Added 48 posts from search: 'jobs'\n",
      "  üîç Method: SEARCH from r/employment\n",
      "     Query: 'employment'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 72 posts using search\n",
      "     Added 72 posts from search: 'employment'\n",
      "  üîç Method: SEARCH from r/employment\n",
      "     Query: 'labor'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: HOT from r/employment\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/employment\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/employment: 280 posts total\n",
      "\n",
      "  üìä Removed 301 duplicates\n",
      "  ‚úÖ Labour data saved to: reddit_data/Labour_20250823_002638.json\n",
      "     Posts: 1469\n",
      "     Subreddits: 6\n",
      "\n",
      "  üìä LABOUR SUMMARY:\n",
      "     Total posts: 1469\n",
      "     Posts with text: 644\n",
      "     Text ratio: 43.8%\n",
      "     Subreddits: 6\n",
      "     Average score: 2955.2\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 15/17: LIFESTYLE LEISURE\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING LIFESTYLE LEISURE DATA ===\n",
      "\n",
      "  üìç Processing r/lifestyle...\n",
      "  üîç Method: SEARCH from r/lifestyle\n",
      "     Query: 'lifestyle'\n",
      "     ‚ùå Error with search method: received 403 HTTP response\n",
      "  üîç Method: SEARCH from r/lifestyle\n",
      "     Query: 'travel'\n",
      "     ‚ùå Error with search method: received 403 HTTP response\n",
      "  üîç Method: SEARCH from r/lifestyle\n",
      "     Query: 'food'\n",
      "     ‚ùå Error with search method: received 403 HTTP response\n",
      "  üîç Method: HOT from r/lifestyle\n",
      "     ‚ùå Error with hot method: received 403 HTTP response\n",
      "  üîç Method: NEW from r/lifestyle\n",
      "     ‚ùå Error with new method: received 403 HTTP response\n",
      "  ‚úÖ r/lifestyle: 0 posts total\n",
      "\n",
      "  üìç Processing r/travel...\n",
      "  üîç Method: SEARCH from r/travel\n",
      "     Query: 'lifestyle'\n",
      "     ‚úÖ Collected 48 posts using search\n",
      "     Added 48 posts from search: 'lifestyle'\n",
      "  üîç Method: SEARCH from r/travel\n",
      "     Query: 'travel'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'travel'\n",
      "  üîç Method: SEARCH from r/travel\n",
      "     Query: 'food'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'food'\n",
      "  ‚úÖ r/travel: 248 posts total\n",
      "\n",
      "  üìç Processing r/food...\n",
      "  üîç Method: SEARCH from r/food\n",
      "     Query: 'lifestyle'\n",
      "     ‚úÖ Collected 2 posts using search\n",
      "     Added 2 posts from search: 'lifestyle'\n",
      "  üîç Method: SEARCH from r/food\n",
      "     Query: 'travel'\n",
      "     ‚úÖ Collected 41 posts using search\n",
      "     Added 41 posts from search: 'travel'\n",
      "  üîç Method: SEARCH from r/food\n",
      "     Query: 'food'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'food'\n",
      "  üîç Method: HOT from r/food\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/food: 223 posts total\n",
      "\n",
      "  üìç Processing r/cooking...\n",
      "  üîç Method: SEARCH from r/cooking\n",
      "     Query: 'lifestyle'\n",
      "     ‚úÖ Collected 32 posts using search\n",
      "     Added 32 posts from search: 'lifestyle'\n",
      "  üîç Method: SEARCH from r/cooking\n",
      "     Query: 'travel'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'travel'\n",
      "  üîç Method: SEARCH from r/cooking\n",
      "     Query: 'food'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'food'\n",
      "  ‚úÖ r/cooking: 232 posts total\n",
      "\n",
      "  üìç Processing r/DIY...\n",
      "  üîç Method: SEARCH from r/DIY\n",
      "     Query: 'lifestyle'\n",
      "     ‚úÖ Collected 7 posts using search\n",
      "     Added 7 posts from search: 'lifestyle'\n",
      "  üîç Method: SEARCH from r/DIY\n",
      "     Query: 'travel'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'travel'\n",
      "  üîç Method: SEARCH from r/DIY\n",
      "     Query: 'food'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'food'\n",
      "  ‚úÖ r/DIY: 207 posts total\n",
      "\n",
      "  üìç Processing r/hobbies...\n",
      "  üîç Method: SEARCH from r/hobbies\n",
      "     Query: 'lifestyle'\n",
      "     ‚úÖ Collected 12 posts using search\n",
      "     Added 12 posts from search: 'lifestyle'\n",
      "  üîç Method: SEARCH from r/hobbies\n",
      "     Query: 'travel'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 71 posts using search\n",
      "     Added 71 posts from search: 'travel'\n",
      "  üîç Method: SEARCH from r/hobbies\n",
      "     Query: 'food'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 62 posts using search\n",
      "     Added 62 posts from search: 'food'\n",
      "  üîç Method: HOT from r/hobbies\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/hobbies: 225 posts total\n",
      "\n",
      "  üìç Processing r/fashion...\n",
      "  üîç Method: SEARCH from r/fashion\n",
      "     Query: 'lifestyle'\n",
      "     ‚úÖ Collected 2 posts using search\n",
      "     Added 2 posts from search: 'lifestyle'\n",
      "  üîç Method: SEARCH from r/fashion\n",
      "     Query: 'travel'\n",
      "     ‚úÖ Collected 14 posts using search\n",
      "     Added 14 posts from search: 'travel'\n",
      "  üîç Method: SEARCH from r/fashion\n",
      "     Query: 'food'\n",
      "     ‚úÖ Collected 14 posts using search\n",
      "     Added 14 posts from search: 'food'\n",
      "  üîç Method: HOT from r/fashion\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/fashion\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/fashion: 190 posts total\n",
      "\n",
      "  üìç Processing r/gardening...\n",
      "  üîç Method: SEARCH from r/gardening\n",
      "     Query: 'lifestyle'\n",
      "     ‚úÖ Collected 15 posts using search\n",
      "     Added 15 posts from search: 'lifestyle'\n",
      "  üîç Method: SEARCH from r/gardening\n",
      "     Query: 'travel'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'travel'\n",
      "  üîç Method: SEARCH from r/gardening\n",
      "     Query: 'food'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'food'\n",
      "  ‚úÖ r/gardening: 215 posts total\n",
      "\n",
      "  üìä Removed 98 duplicates\n",
      "  ‚úÖ Lifestyle_Leisure data saved to: reddit_data/Lifestyle_Leisure_20250823_002858.json\n",
      "     Posts: 1442\n",
      "     Subreddits: 7\n",
      "\n",
      "  üìä LIFESTYLE LEISURE SUMMARY:\n",
      "     Total posts: 1442\n",
      "     Posts with text: 774\n",
      "     Text ratio: 53.7%\n",
      "     Subreddits: 7\n",
      "     Average score: 874.1\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 16/17: WEATHER\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING WEATHER DATA ===\n",
      "\n",
      "  üìç Processing r/weather...\n",
      "  üîç Method: SEARCH from r/weather\n",
      "     Query: 'weather'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'weather'\n",
      "  üîç Method: SEARCH from r/weather\n",
      "     Query: 'climate'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'climate'\n",
      "  üîç Method: SEARCH from r/weather\n",
      "     Query: 'temperature'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'temperature'\n",
      "  üîç Method: HOT from r/weather\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/weather: 380 posts total\n",
      "\n",
      "  üìç Processing r/climate...\n",
      "  üîç Method: SEARCH from r/climate\n",
      "     Query: 'weather'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'weather'\n",
      "  üîç Method: SEARCH from r/climate\n",
      "     Query: 'climate'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'climate'\n",
      "  üîç Method: SEARCH from r/climate\n",
      "     Query: 'temperature'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'temperature'\n",
      "  üîç Method: HOT from r/climate\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/climate: 380 posts total\n",
      "\n",
      "  üìç Processing r/meteorology...\n",
      "  üîç Method: SEARCH from r/meteorology\n",
      "     Query: 'weather'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'weather'\n",
      "  üîç Method: SEARCH from r/meteorology\n",
      "     Query: 'climate'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'climate'\n",
      "  üîç Method: SEARCH from r/meteorology\n",
      "     Query: 'temperature'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'temperature'\n",
      "  üîç Method: HOT from r/meteorology\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/meteorology: 380 posts total\n",
      "\n",
      "  üìç Processing r/TropicalWeather...\n",
      "  üîç Method: SEARCH from r/TropicalWeather\n",
      "     Query: 'weather'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'weather'\n",
      "  üîç Method: SEARCH from r/TropicalWeather\n",
      "     Query: 'climate'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 65 posts using search\n",
      "     Added 65 posts from search: 'climate'\n",
      "  üîç Method: SEARCH from r/TropicalWeather\n",
      "     Query: 'temperature'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'temperature'\n",
      "  üîç Method: HOT from r/TropicalWeather\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/TropicalWeather\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/TropicalWeather: 425 posts total\n",
      "\n",
      "  üìä Removed 261 duplicates\n",
      "  ‚úÖ Weather data saved to: reddit_data/Weather_20250823_003040.json\n",
      "     Posts: 1304\n",
      "     Subreddits: 4\n",
      "\n",
      "  üìä WEATHER SUMMARY:\n",
      "     Total posts: 1304\n",
      "     Posts with text: 474\n",
      "     Text ratio: 36.3%\n",
      "     Subreddits: 4\n",
      "     Average score: 321.5\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 17/17: RELIGION\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING RELIGION DATA ===\n",
      "\n",
      "  üìç Processing r/religion...\n",
      "  üîç Method: SEARCH from r/religion\n",
      "     Query: 'religion'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'religion'\n",
      "  üîç Method: SEARCH from r/religion\n",
      "     Query: 'faith'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'faith'\n",
      "  üîç Method: SEARCH from r/religion\n",
      "     Query: 'spiritual'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'spiritual'\n",
      "  ‚úÖ r/religion: 300 posts total\n",
      "\n",
      "  üìç Processing r/Christianity...\n",
      "  üîç Method: SEARCH from r/Christianity\n",
      "     Query: 'religion'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'religion'\n",
      "  üîç Method: SEARCH from r/Christianity\n",
      "     Query: 'faith'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'faith'\n",
      "  üîç Method: SEARCH from r/Christianity\n",
      "     Query: 'spiritual'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'spiritual'\n",
      "  ‚úÖ r/Christianity: 300 posts total\n",
      "\n",
      "  üìç Processing r/islam...\n",
      "  üîç Method: SEARCH from r/islam\n",
      "     Query: 'religion'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'religion'\n",
      "  üîç Method: SEARCH from r/islam\n",
      "     Query: 'faith'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'faith'\n",
      "  üîç Method: SEARCH from r/islam\n",
      "     Query: 'spiritual'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'spiritual'\n",
      "  ‚úÖ r/islam: 300 posts total\n",
      "\n",
      "  üìç Processing r/Judaism...\n",
      "  üîç Method: SEARCH from r/Judaism\n",
      "     Query: 'religion'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'religion'\n",
      "  üîç Method: SEARCH from r/Judaism\n",
      "     Query: 'faith'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'faith'\n",
      "  üîç Method: SEARCH from r/Judaism\n",
      "     Query: 'spiritual'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'spiritual'\n",
      "  ‚úÖ r/Judaism: 300 posts total\n",
      "\n",
      "  üìç Processing r/Buddhism...\n",
      "  üîç Method: SEARCH from r/Buddhism\n",
      "     Query: 'religion'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'religion'\n",
      "  üîç Method: SEARCH from r/Buddhism\n",
      "     Query: 'faith'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'faith'\n",
      "  üîç Method: SEARCH from r/Buddhism\n",
      "     Query: 'spiritual'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'spiritual'\n",
      "  ‚úÖ r/Buddhism: 300 posts total\n",
      "\n",
      "  üìç Processing r/Hinduism...\n",
      "  üîç Method: SEARCH from r/Hinduism\n",
      "     Query: 'religion'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'religion'\n",
      "  üîç Method: SEARCH from r/Hinduism\n",
      "     Query: 'faith'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'faith'\n",
      "  üîç Method: SEARCH from r/Hinduism\n",
      "     Query: 'spiritual'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'spiritual'\n",
      "  ‚úÖ r/Hinduism: 300 posts total\n",
      "\n",
      "  üìç Processing r/atheism...\n",
      "  üîç Method: SEARCH from r/atheism\n",
      "     Query: 'religion'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'religion'\n",
      "  üîç Method: SEARCH from r/atheism\n",
      "     Query: 'faith'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'faith'\n",
      "  üîç Method: SEARCH from r/atheism\n",
      "     Query: 'spiritual'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'spiritual'\n",
      "  ‚úÖ r/atheism: 300 posts total\n",
      "\n",
      "  üìä Removed 64 duplicates\n",
      "  ‚úÖ Religion data saved to: reddit_data/Religion_20250823_003250.json\n",
      "     Posts: 2036\n",
      "     Subreddits: 7\n",
      "\n",
      "  üìä RELIGION SUMMARY:\n",
      "     Total posts: 2036\n",
      "     Posts with text: 1548\n",
      "     Text ratio: 76.0%\n",
      "     Subreddits: 7\n",
      "     Average score: 360.9\n",
      "\n",
      "================================================================================\n",
      "üéâ COLLECTION COMPLETE!\n",
      "================================================================================\n",
      "Categories processed: 16/17\n",
      "Total posts collected: 20,976\n",
      "Files created: 16\n",
      "\n",
      "üìÅ All files saved in: reddit_data/\n",
      "üìä Collection summary: reddit_data/collection_summary_20250823_003250.json\n",
      "\n",
      "‚úÖ Successfully collected data for:\n",
      "   ‚Ä¢ Politics\n",
      "   ‚Ä¢ Economy Business Finance\n",
      "   ‚Ä¢ Science Technology\n",
      "   ‚Ä¢ Health\n",
      "   ‚Ä¢ Sport\n",
      "   ‚Ä¢ Arts Culture Entertainment Media\n",
      "   ‚Ä¢ Society\n",
      "   ‚Ä¢ Human Interest\n",
      "   ‚Ä¢ Crime Law Justice\n",
      "   ‚Ä¢ Disaster Accident Emergency\n",
      "   ‚Ä¢ Environment\n",
      "   ‚Ä¢ Education\n",
      "   ‚Ä¢ Labour\n",
      "   ‚Ä¢ Lifestyle Leisure\n",
      "   ‚Ä¢ Weather\n",
      "   ‚Ä¢ Religion\n",
      "\n",
      "üéØ Data collection completed at: 2025-08-23 00:32:50\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"xeV3sLrkl4xGyM-ScCBkSA\",\n",
    "    client_secret=\"sGrJQoAWabMp3hVcAXTxocV3gkCXoQ\",\n",
    "    user_agent=\"HeadlineHive\"\n",
    ")\n",
    "\n",
    "# Comprehensive category configuration\n",
    "CATEGORIES_CONFIG = {\n",
    "    'Politics': {\n",
    "        'subreddits': ['politics', 'PoliticalDiscussion', 'Ask_Politics', 'neutralpolitics', 'worldpolitics'],\n",
    "        'search_queries': ['politics', 'election', 'government', 'congress', 'senate', 'political'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Economy_Business_Finance': {\n",
    "        'subreddits': ['Economics', 'business', 'stocks', 'investing', 'personalfinance', 'economy', 'finance'],\n",
    "        'search_queries': ['economy', 'business', 'finance', 'stock market', 'investment', 'banking', 'GDP'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Science_Technology': {\n",
    "        'subreddits': ['technology', 'science', 'gadgets', 'tech', 'MachineLearning', 'artificial', 'programming', 'computers'],\n",
    "        'search_queries': ['technology', 'science', 'innovation', 'research', 'AI', 'machine learning', 'tech'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Health': {\n",
    "        'subreddits': ['Health', 'medicine', 'medical', 'healthcare', 'nutrition', 'fitness', 'mentalhealth'],\n",
    "        'search_queries': ['health', 'medicine', 'medical', 'healthcare', 'disease', 'treatment', 'wellness'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Sport': {\n",
    "        'subreddits': ['sports', 'nfl', 'nba', 'soccer', 'baseball', 'hockey', 'olympics', 'football'],\n",
    "        'search_queries': ['sports', 'football', 'basketball', 'soccer', 'baseball', 'olympics', 'athletics'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Arts_Culture_Entertainment_Media': {\n",
    "        'subreddits': ['movies', 'music', 'art', 'books', 'television', 'entertainment', 'culture', 'media'],\n",
    "        'search_queries': ['movies', 'music', 'art', 'books', 'entertainment', 'culture', 'media', 'celebrity'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Society': {\n",
    "        'subreddits': ['sociology', 'society', 'social', 'community', 'TrueReddit', 'news'],\n",
    "        'search_queries': ['society', 'social', 'community', 'culture', 'demographics', 'population'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Human_Interest': {\n",
    "        'subreddits': ['UpliftingNews', 'MadeMeSmile', 'happy', 'wholesome', 'HumansBeingBros', 'todayilearned'],\n",
    "        'search_queries': ['human interest', 'inspiring', 'heartwarming', 'amazing', 'incredible', 'touching'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Crime_Law_Justice': {\n",
    "        'subreddits': ['law', 'legaladvice', 'UnresolvedMysteries', 'TrueCrime', 'justice', 'police'],\n",
    "        'search_queries': ['crime', 'law', 'justice', 'court', 'legal', 'police', 'arrest', 'trial'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Conflict_War_Peace': {\n",
    "        'subreddits': ['worldnews', 'geopolitics', 'Military', 'CredibleDefense', 'conflict'],\n",
    "        'search_queries': ['war', 'conflict', 'military', 'peace', 'international', 'defense', 'security'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Disaster_Accident_Emergency': {\n",
    "        'subreddits': ['worldnews', 'news', 'CatastrophicFailure', 'emergencyservices'],\n",
    "        'search_queries': ['disaster', 'emergency', 'accident', 'earthquake', 'flood', 'fire', 'rescue'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Environment': {\n",
    "        'subreddits': ['environment', 'climatechange', 'sustainability', 'nature', 'ecology', 'green'],\n",
    "        'search_queries': ['environment', 'climate change', 'sustainability', 'ecology', 'pollution', 'conservation'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Education': {\n",
    "        'subreddits': ['education', 'teachers', 'college', 'university', 'students', 'Academia'],\n",
    "        'search_queries': ['education', 'school', 'university', 'college', 'learning', 'academic', 'student'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Labour': {\n",
    "        'subreddits': ['jobs', 'antiwork', 'WorkReform', 'labor', 'union', 'employment'],\n",
    "        'search_queries': ['jobs', 'employment', 'labor', 'union', 'workplace', 'workers', 'career'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Lifestyle_Leisure': {\n",
    "        'subreddits': ['lifestyle', 'travel', 'food', 'cooking', 'DIY', 'hobbies', 'fashion', 'gardening'],\n",
    "        'search_queries': ['lifestyle', 'travel', 'food', 'cooking', 'hobbies', 'leisure', 'fashion'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Weather': {\n",
    "        'subreddits': ['weather', 'climate', 'meteorology', 'TropicalWeather'],\n",
    "        'search_queries': ['weather', 'climate', 'temperature', 'storm', 'hurricane', 'meteorology'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Religion': {\n",
    "        'subreddits': ['religion', 'Christianity', 'islam', 'Judaism', 'Buddhism', 'Hinduism', 'atheism'],\n",
    "        'search_queries': ['religion', 'faith', 'spiritual', 'church', 'religious', 'belief', 'worship'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    }\n",
    "}\n",
    "\n",
    "def collect_reddit_posts(subreddit_name, search_query=None, limit=300, method=\"search\", topic_category=\"general\"):\n",
    "    \"\"\"\n",
    "    Collect Reddit data with proper text extraction\n",
    "    \"\"\"\n",
    "    print(f\"  üîç Method: {method.upper()} from r/{subreddit_name}\")\n",
    "    if search_query:\n",
    "        print(f\"     Query: '{search_query}'\")\n",
    "    \n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "    \n",
    "    try:\n",
    "        # Choose method\n",
    "        if method == \"search\" and search_query:\n",
    "            submissions = subreddit.search(search_query, limit=limit, time_filter='year')\n",
    "        elif method == \"hot\":\n",
    "            submissions = subreddit.hot(limit=limit)\n",
    "        elif method == \"new\":\n",
    "            submissions = subreddit.new(limit=limit)\n",
    "        elif method == \"top\":\n",
    "            submissions = subreddit.top(limit=limit, time_filter='month')\n",
    "        else:\n",
    "            print(f\"     Unknown method: {method}\")\n",
    "            return []\n",
    "        \n",
    "        for i, submission in enumerate(submissions):\n",
    "            try:\n",
    "                # Extract text content properly\n",
    "                text_content = \"\"\n",
    "                \n",
    "                # For self posts (text posts)\n",
    "                if submission.is_self and submission.selftext:\n",
    "                    text_content = submission.selftext.strip()\n",
    "                \n",
    "                # For link posts, we might want the URL or title as content\n",
    "                elif not submission.is_self:\n",
    "                    text_content = f\"Link post: {submission.url}\"\n",
    "                \n",
    "                # Sometimes selftext exists but is empty/whitespace\n",
    "                if not text_content and hasattr(submission, 'selftext'):\n",
    "                    if submission.selftext and submission.selftext.strip():\n",
    "                        text_content = submission.selftext.strip()\n",
    "                \n",
    "                # If still no content, use title as fallback\n",
    "                if not text_content:\n",
    "                    text_content = f\"[Title only] {submission.title}\"\n",
    "                \n",
    "                post_data = {\n",
    "                    'id': submission.id,\n",
    "                    'title': submission.title if submission.title else \"No title\",\n",
    "                    'text': text_content,\n",
    "                    'author': str(submission.author) if submission.author else '[deleted]',\n",
    "                    'subreddit': submission.subreddit.display_name,\n",
    "                    'score': submission.score if submission.score else 0,\n",
    "                    'upvote_ratio': getattr(submission, 'upvote_ratio', 0),\n",
    "                    'comments': submission.num_comments if submission.num_comments else 0,\n",
    "                    'url': submission.url if submission.url else \"\",\n",
    "                    'is_self': submission.is_self,\n",
    "                    'created': datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'permalink': f\"https://reddit.com{submission.permalink}\",\n",
    "                    'method_collected': method,\n",
    "                    'category': topic_category,\n",
    "                    'flair': submission.link_flair_text if hasattr(submission, 'link_flair_text') else None,\n",
    "                    'nsfw': submission.over_18 if hasattr(submission, 'over_18') else False\n",
    "                }\n",
    "                \n",
    "                posts.append(post_data)\n",
    "                \n",
    "                if (i + 1) % 50 == 0:\n",
    "                    print(f\"     Processed {i + 1} posts...\")\n",
    "                    time.sleep(0.2)  # Rate limiting\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"     Error processing post {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"     ‚úÖ Collected {len(posts)} posts using {method}\")\n",
    "        return posts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"     ‚ùå Error with {method} method: {e}\")\n",
    "        return []\n",
    "\n",
    "def collect_category_data(category_name, category_config, max_posts_per_category=2000):\n",
    "    \"\"\"\n",
    "    Collect data for a specific category\n",
    "    \"\"\"\n",
    "    print(f\"\\nüéØ === COLLECTING {category_name.upper().replace('_', ' ')} DATA ===\")\n",
    "    category_posts = []\n",
    "    \n",
    "    subreddits = category_config['subreddits']\n",
    "    search_queries = category_config['search_queries']\n",
    "    methods = category_config['methods']\n",
    "    \n",
    "    # Limit search queries to avoid too many API calls\n",
    "    limited_queries = search_queries[:3]  # Use only first 3 queries\n",
    "    posts_per_subreddit = max_posts_per_category // len(subreddits)\n",
    "    \n",
    "    for subreddit in subreddits:\n",
    "        print(f\"\\n  üìç Processing r/{subreddit}...\")\n",
    "        subreddit_posts = []\n",
    "        \n",
    "        try:\n",
    "            # Try search method with limited queries\n",
    "            if 'search' in methods and limited_queries:\n",
    "                for query in limited_queries:\n",
    "                    if len(subreddit_posts) >= posts_per_subreddit:\n",
    "                        break\n",
    "                    \n",
    "                    search_posts = collect_reddit_posts(\n",
    "                        subreddit, query, limit=100, method=\"search\", topic_category=category_name\n",
    "                    )\n",
    "                    if search_posts:\n",
    "                        subreddit_posts.extend(search_posts)\n",
    "                        print(f\"     Added {len(search_posts)} posts from search: '{query}'\")\n",
    "                    time.sleep(1)  # Rate limiting between queries\n",
    "            \n",
    "            # Try hot posts method\n",
    "            if 'hot' in methods and len(subreddit_posts) < posts_per_subreddit:\n",
    "                time.sleep(1)\n",
    "                hot_posts = collect_reddit_posts(\n",
    "                    subreddit, None, limit=80, method=\"hot\", topic_category=category_name\n",
    "                )\n",
    "                if hot_posts:\n",
    "                    subreddit_posts.extend(hot_posts)\n",
    "                    print(f\"     Added {len(hot_posts)} posts from hot\")\n",
    "            \n",
    "            # Try new posts method\n",
    "            if 'new' in methods and len(subreddit_posts) < posts_per_subreddit:\n",
    "                time.sleep(1)\n",
    "                new_posts = collect_reddit_posts(\n",
    "                    subreddit, None, limit=80, method=\"new\", topic_category=category_name\n",
    "                )\n",
    "                if new_posts:\n",
    "                    subreddit_posts.extend(new_posts)\n",
    "                    print(f\"     Added {len(new_posts)} posts from new\")\n",
    "            \n",
    "            category_posts.extend(subreddit_posts)\n",
    "            print(f\"  ‚úÖ r/{subreddit}: {len(subreddit_posts)} posts total\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error with r/{subreddit}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        time.sleep(3)  # Rate limiting between subreddits\n",
    "    \n",
    "    return category_posts\n",
    "\n",
    "def remove_duplicates(all_posts):\n",
    "    \"\"\"Remove duplicate posts based on ID\"\"\"\n",
    "    seen_ids = set()\n",
    "    unique_posts = []\n",
    "    \n",
    "    for post in all_posts:\n",
    "        if post['id'] not in seen_ids:\n",
    "            unique_posts.append(post)\n",
    "            seen_ids.add(post['id'])\n",
    "    \n",
    "    return unique_posts\n",
    "\n",
    "def save_category_json(posts, category_name, output_dir=\"reddit_data\"):\n",
    "    \"\"\"Save category posts to separate JSON file\"\"\"\n",
    "    if not posts:\n",
    "        print(f\"  ‚ùå No posts to save for {category_name}!\")\n",
    "        return False\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    try:\n",
    "        # Create comprehensive data structure for this category\n",
    "        output_data = {\n",
    "            \"metadata\": {\n",
    "                \"category\": category_name.replace('_', ' '),\n",
    "                \"total_posts\": len(posts),\n",
    "                \"collection_date\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"methods_used\": list(set([post.get('method_collected', 'unknown') for post in posts])),\n",
    "                \"subreddits\": list(set([post['subreddit'] for post in posts])),\n",
    "                \"date_range\": {\n",
    "                    \"earliest\": min([post['created'] for post in posts]),\n",
    "                    \"latest\": max([post['created'] for post in posts])\n",
    "                },\n",
    "                \"text_statistics\": {\n",
    "                    \"posts_with_text\": len([p for p in posts if p['text'] and not p['text'].startswith('[Title only]') and not p['text'].startswith('Link post:')]),\n",
    "                    \"average_score\": sum([p['score'] for p in posts]) / len(posts),\n",
    "                    \"total_comments\": sum([p['comments'] for p in posts])\n",
    "                }\n",
    "            },\n",
    "            \"posts\": posts\n",
    "        }\n",
    "        \n",
    "        # Save to category-specific file\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"{output_dir}/{category_name}_{timestamp}.json\"\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"  ‚úÖ {category_name} data saved to: {filename}\")\n",
    "        print(f\"     Posts: {len(posts)}\")\n",
    "        print(f\"     Subreddits: {len(output_data['metadata']['subreddits'])}\")\n",
    "        return filename\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error saving {category_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def display_category_summary(posts, category_name):\n",
    "    \"\"\"Display summary for a category\"\"\"\n",
    "    if not posts:\n",
    "        return\n",
    "    \n",
    "    posts_with_text = [p for p in posts if p['text'] and not p['text'].startswith('[Title only]') and not p['text'].startswith('Link post:')]\n",
    "    \n",
    "    print(f\"\\n  üìä {category_name.replace('_', ' ').upper()} SUMMARY:\")\n",
    "    print(f\"     Total posts: {len(posts)}\")\n",
    "    print(f\"     Posts with text: {len(posts_with_text)}\")\n",
    "    print(f\"     Text ratio: {len(posts_with_text)/len(posts)*100:.1f}%\")\n",
    "    print(f\"     Subreddits: {len(set([p['subreddit'] for p in posts]))}\")\n",
    "    print(f\"     Average score: {sum([p['score'] for p in posts]) / len(posts):.1f}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT COMPREHENSIVE DATA COLLECTION\")\n",
    "    print(\"17 Categories - Separate JSON Files\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Test Reddit connection\n",
    "    try:\n",
    "        print(\"üîå Testing Reddit connection...\")\n",
    "        test_sub = reddit.subreddit('python')\n",
    "        print(f\"‚úÖ Connected! Test subreddit: {test_sub.display_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Reddit connection failed: {e}\")\n",
    "        print(\"Check your Reddit API credentials!\")\n",
    "        exit()\n",
    "    \n",
    "    # Create summary for all categories\n",
    "    collection_summary = {\n",
    "        \"collection_date\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"categories_collected\": [],\n",
    "        \"total_posts\": 0,\n",
    "        \"files_created\": []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìã CATEGORIES TO COLLECT ({len(CATEGORIES_CONFIG)}):\")\n",
    "    for i, category in enumerate(CATEGORIES_CONFIG.keys(), 1):\n",
    "        print(f\"  {i:2d}. {category.replace('_', ' ')}\")\n",
    "    \n",
    "    # Collect data for each category\n",
    "    for i, (category_name, category_config) in enumerate(CATEGORIES_CONFIG.items(), 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CATEGORY {i}/{len(CATEGORIES_CONFIG)}: {category_name.replace('_', ' ').upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            # Collect category data\n",
    "            category_posts = collect_category_data(category_name, category_config, max_posts_per_category=1500)\n",
    "            \n",
    "            if category_posts:\n",
    "                # Remove duplicates\n",
    "                unique_posts = remove_duplicates(category_posts)\n",
    "                print(f\"\\n  üìä Removed {len(category_posts) - len(unique_posts)} duplicates\")\n",
    "                \n",
    "                # Save to separate JSON file\n",
    "                filename = save_category_json(unique_posts, category_name)\n",
    "                \n",
    "                if filename:\n",
    "                    # Update summary\n",
    "                    collection_summary[\"categories_collected\"].append(category_name)\n",
    "                    collection_summary[\"total_posts\"] += len(unique_posts)\n",
    "                    collection_summary[\"files_created\"].append(filename)\n",
    "                    \n",
    "                    # Display summary\n",
    "                    display_category_summary(unique_posts, category_name)\n",
    "                \n",
    "            else:\n",
    "                print(f\"  ‚ùå No data collected for {category_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error processing {category_name}: {e}\")\n",
    "        \n",
    "        # Wait between categories to be respectful\n",
    "        if i < len(CATEGORIES_CONFIG):\n",
    "            wait_time = 30  # 30 seconds between categories\n",
    "            print(f\"\\n  ‚è≥ Waiting {wait_time} seconds before next category...\")\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    # Save collection summary\n",
    "    try:\n",
    "        summary_file = f\"reddit_data/collection_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(collection_summary, f, indent=2, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving summary: {e}\")\n",
    "    \n",
    "    # Final results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üéâ COLLECTION COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Categories processed: {len(collection_summary['categories_collected'])}/{len(CATEGORIES_CONFIG)}\")\n",
    "    print(f\"Total posts collected: {collection_summary['total_posts']:,}\")\n",
    "    print(f\"Files created: {len(collection_summary['files_created'])}\")\n",
    "    print(f\"\\nüìÅ All files saved in: reddit_data/\")\n",
    "    print(f\"üìä Collection summary: {summary_file}\")\n",
    "    \n",
    "    if collection_summary['categories_collected']:\n",
    "        print(f\"\\n‚úÖ Successfully collected data for:\")\n",
    "        for category in collection_summary['categories_collected']:\n",
    "            print(f\"   ‚Ä¢ {category.replace('_', ' ')}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Data collection completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa4351e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Using cached praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting prawcore<3,>=2.4 (from praw)\n",
      "  Using cached prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting update_checker>=0.18 (from praw)\n",
      "  Using cached update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\yagne\\miniconda3\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\yagne\\miniconda3\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yagne\\miniconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yagne\\miniconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yagne\\miniconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yagne\\miniconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.12.14)\n",
      "Using cached praw-7.8.1-py3-none-any.whl (189 kB)\n",
      "Using cached prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
      "Using cached update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Installing collected packages: update_checker, prawcore, praw\n",
      "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"
     ]
    }
   ],
   "source": [
    "! pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e68c378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ REDDIT COMPREHENSIVE DATA COLLECTION\n",
      "17 Categories - Separate JSON Files\n",
      "================================================================================\n",
      "üîå Testing Reddit connection...\n",
      "‚úÖ Connected! Test subreddit: python\n",
      "\n",
      "üìã CATEGORIES TO COLLECT (17):\n",
      "   1. Politics\n",
      "   2. Economy Business Finance\n",
      "   3. Science Technology\n",
      "   4. Health\n",
      "   5. Sport\n",
      "   6. Arts Culture Entertainment Media\n",
      "   7. Society\n",
      "   8. Human Interest\n",
      "   9. Crime Law Justice\n",
      "  10. Conflict War Peace\n",
      "  11. Disaster Accident Emergency\n",
      "  12. Environment\n",
      "  13. Education\n",
      "  14. Labour\n",
      "  15. Lifestyle Leisure\n",
      "  16. Weather\n",
      "  17. Religion\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 1/17: POLITICS\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING POLITICS DATA ===\n",
      "\n",
      "  üìç Processing r/politics...\n",
      "  üîç Method: SEARCH from r/politics\n",
      "     Query: 'politics'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'politics'\n",
      "  üîç Method: SEARCH from r/politics\n",
      "     Query: 'election'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'election'\n",
      "  üîç Method: SEARCH from r/politics\n",
      "     Query: 'government'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'government'\n",
      "  ‚úÖ r/politics: 300 posts total\n",
      "\n",
      "  üìç Processing r/PoliticalDiscussion...\n",
      "  üîç Method: SEARCH from r/PoliticalDiscussion\n",
      "     Query: 'politics'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'politics'\n",
      "  üîç Method: SEARCH from r/PoliticalDiscussion\n",
      "     Query: 'election'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'election'\n",
      "  üîç Method: SEARCH from r/PoliticalDiscussion\n",
      "     Query: 'government'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'government'\n",
      "  ‚úÖ r/PoliticalDiscussion: 300 posts total\n",
      "\n",
      "  üìç Processing r/Ask_Politics...\n",
      "  üîç Method: SEARCH from r/Ask_Politics\n",
      "     Query: 'politics'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 88 posts using search\n",
      "     Added 88 posts from search: 'politics'\n",
      "  üîç Method: SEARCH from r/Ask_Politics\n",
      "     Query: 'election'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 73 posts using search\n",
      "     Added 73 posts from search: 'election'\n",
      "  üîç Method: SEARCH from r/Ask_Politics\n",
      "     Query: 'government'\n",
      "     ‚úÖ Collected 30 posts using search\n",
      "     Added 30 posts from search: 'government'\n",
      "  üîç Method: HOT from r/Ask_Politics\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/Ask_Politics\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/Ask_Politics: 351 posts total\n",
      "\n",
      "  üìç Processing r/neutralpolitics...\n",
      "  üîç Method: SEARCH from r/neutralpolitics\n",
      "     Query: 'politics'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 60 posts using search\n",
      "     Added 60 posts from search: 'politics'\n",
      "  üîç Method: SEARCH from r/neutralpolitics\n",
      "     Query: 'election'\n",
      "     ‚úÖ Collected 36 posts using search\n",
      "     Added 36 posts from search: 'election'\n",
      "  üîç Method: SEARCH from r/neutralpolitics\n",
      "     Query: 'government'\n",
      "     ‚úÖ Collected 33 posts using search\n",
      "     Added 33 posts from search: 'government'\n",
      "  üîç Method: HOT from r/neutralpolitics\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/neutralpolitics\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/neutralpolitics: 289 posts total\n",
      "\n",
      "  üìç Processing r/worldpolitics...\n",
      "  üîç Method: SEARCH from r/worldpolitics\n",
      "     Query: 'politics'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/worldpolitics\n",
      "     Query: 'election'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/worldpolitics\n",
      "     Query: 'government'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: HOT from r/worldpolitics\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/worldpolitics\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/worldpolitics: 160 posts total\n",
      "\n",
      "  üìä Removed 445 duplicates\n",
      "  ‚úÖ Politics data saved to: C:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\data\\raw\\social_Media_data\\reddit\\Politics.json\n",
      "     Posts: 955\n",
      "     Subreddits: 5\n",
      "\n",
      "  üìä POLITICS SUMMARY:\n",
      "     Total posts: 955\n",
      "     Posts with text: 580\n",
      "     Text ratio: 60.7%\n",
      "     Subreddits: 5\n",
      "     Average score: 4878.8\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 2/17: ECONOMY BUSINESS FINANCE\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING ECONOMY BUSINESS FINANCE DATA ===\n",
      "\n",
      "  üìç Processing r/Economics...\n",
      "  üîç Method: SEARCH from r/Economics\n",
      "     Query: 'economy'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'economy'\n",
      "  üîç Method: SEARCH from r/Economics\n",
      "     Query: 'business'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'business'\n",
      "  üîç Method: SEARCH from r/Economics\n",
      "     Query: 'finance'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 65 posts using search\n",
      "     Added 65 posts from search: 'finance'\n",
      "  ‚úÖ r/Economics: 265 posts total\n",
      "\n",
      "  üìç Processing r/business...\n",
      "  üîç Method: SEARCH from r/business\n",
      "     Query: 'economy'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 60 posts using search\n",
      "     Added 60 posts from search: 'economy'\n",
      "  üîç Method: SEARCH from r/business\n",
      "     Query: 'business'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'business'\n",
      "  üîç Method: SEARCH from r/business\n",
      "     Query: 'finance'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'finance'\n",
      "  ‚úÖ r/business: 260 posts total\n",
      "\n",
      "  üìç Processing r/stocks...\n",
      "  üîç Method: SEARCH from r/stocks\n",
      "     Query: 'economy'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'economy'\n",
      "  üîç Method: SEARCH from r/stocks\n",
      "     Query: 'business'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'business'\n",
      "  üîç Method: SEARCH from r/stocks\n",
      "     Query: 'finance'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'finance'\n",
      "  ‚úÖ r/stocks: 300 posts total\n",
      "\n",
      "  üìç Processing r/investing...\n",
      "  üîç Method: SEARCH from r/investing\n",
      "     Query: 'economy'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'economy'\n",
      "  üîç Method: SEARCH from r/investing\n",
      "     Query: 'business'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'business'\n",
      "  üîç Method: SEARCH from r/investing\n",
      "     Query: 'finance'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'finance'\n",
      "  ‚úÖ r/investing: 300 posts total\n",
      "\n",
      "  üìç Processing r/personalfinance...\n",
      "  üîç Method: SEARCH from r/personalfinance\n",
      "     Query: 'economy'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'economy'\n",
      "  üîç Method: SEARCH from r/personalfinance\n",
      "     Query: 'business'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'business'\n",
      "  üîç Method: SEARCH from r/personalfinance\n",
      "     Query: 'finance'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'finance'\n",
      "  ‚úÖ r/personalfinance: 300 posts total\n",
      "\n",
      "  üìç Processing r/economy...\n",
      "  üîç Method: SEARCH from r/economy\n",
      "     Query: 'economy'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'economy'\n",
      "  üîç Method: SEARCH from r/economy\n",
      "     Query: 'business'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'business'\n",
      "  üîç Method: SEARCH from r/economy\n",
      "     Query: 'finance'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'finance'\n",
      "  ‚úÖ r/economy: 300 posts total\n",
      "\n",
      "  üìç Processing r/finance...\n",
      "  üîç Method: SEARCH from r/finance\n",
      "     Query: 'economy'\n",
      "     ‚úÖ Collected 10 posts using search\n",
      "     Added 10 posts from search: 'economy'\n",
      "  üîç Method: SEARCH from r/finance\n",
      "     Query: 'business'\n",
      "     ‚úÖ Collected 6 posts using search\n",
      "     Added 6 posts from search: 'business'\n",
      "  üîç Method: SEARCH from r/finance\n",
      "     Query: 'finance'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'finance'\n",
      "  üîç Method: HOT from r/finance\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/finance\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/finance: 276 posts total\n",
      "\n",
      "  üìä Removed 196 duplicates\n",
      "  ‚úÖ Economy_Business_Finance data saved to: C:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\data\\raw\\social_Media_data\\reddit\\Economy_Business_Finance.json\n",
      "     Posts: 1805\n",
      "     Subreddits: 7\n",
      "\n",
      "  üìä ECONOMY BUSINESS FINANCE SUMMARY:\n",
      "     Total posts: 1805\n",
      "     Posts with text: 1096\n",
      "     Text ratio: 60.7%\n",
      "     Subreddits: 7\n",
      "     Average score: 1132.9\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 3/17: SCIENCE TECHNOLOGY\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING SCIENCE TECHNOLOGY DATA ===\n",
      "\n",
      "  üìç Processing r/technology...\n",
      "  üîç Method: SEARCH from r/technology\n",
      "     Query: 'technology'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'technology'\n",
      "  üîç Method: SEARCH from r/technology\n",
      "     Query: 'science'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'science'\n",
      "  ‚úÖ r/technology: 200 posts total\n",
      "\n",
      "  üìç Processing r/science...\n",
      "  üîç Method: SEARCH from r/science\n",
      "     Query: 'technology'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 76 posts using search\n",
      "     Added 76 posts from search: 'technology'\n",
      "  üîç Method: SEARCH from r/science\n",
      "     Query: 'science'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/science\n",
      "     Query: 'innovation'\n",
      "     ‚úÖ Collected 40 posts using search\n",
      "     Added 40 posts from search: 'innovation'\n",
      "  üîç Method: HOT from r/science\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/science: 196 posts total\n",
      "\n",
      "  üìç Processing r/gadgets...\n",
      "  üîç Method: SEARCH from r/gadgets\n",
      "     Query: 'technology'\n",
      "     ‚úÖ Collected 8 posts using search\n",
      "     Added 8 posts from search: 'technology'\n",
      "  üîç Method: SEARCH from r/gadgets\n",
      "     Query: 'science'\n",
      "     ‚úÖ Collected 7 posts using search\n",
      "     Added 7 posts from search: 'science'\n",
      "  üîç Method: SEARCH from r/gadgets\n",
      "     Query: 'innovation'\n",
      "     ‚úÖ Collected 11 posts using search\n",
      "     Added 11 posts from search: 'innovation'\n",
      "  üîç Method: HOT from r/gadgets\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/gadgets\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/gadgets: 186 posts total\n",
      "\n",
      "  üìç Processing r/tech...\n",
      "  üîç Method: SEARCH from r/tech\n",
      "     Query: 'technology'\n",
      "     ‚úÖ Collected 26 posts using search\n",
      "     Added 26 posts from search: 'technology'\n",
      "  üîç Method: SEARCH from r/tech\n",
      "     Query: 'science'\n",
      "     ‚úÖ Collected 40 posts using search\n",
      "     Added 40 posts from search: 'science'\n",
      "  üîç Method: SEARCH from r/tech\n",
      "     Query: 'innovation'\n",
      "     ‚úÖ Collected 15 posts using search\n",
      "     Added 15 posts from search: 'innovation'\n",
      "  üîç Method: HOT from r/tech\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/tech\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/tech: 241 posts total\n",
      "\n",
      "  üìç Processing r/MachineLearning...\n",
      "  üîç Method: SEARCH from r/MachineLearning\n",
      "     Query: 'technology'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 57 posts using search\n",
      "     Added 57 posts from search: 'technology'\n",
      "  üîç Method: SEARCH from r/MachineLearning\n",
      "     Query: 'science'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'science'\n",
      "  üîç Method: SEARCH from r/MachineLearning\n",
      "     Query: 'innovation'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 87 posts using search\n",
      "     Added 87 posts from search: 'innovation'\n",
      "  ‚úÖ r/MachineLearning: 244 posts total\n",
      "\n",
      "  üìç Processing r/artificial...\n",
      "  üîç Method: SEARCH from r/artificial\n",
      "     Query: 'technology'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'technology'\n",
      "  üîç Method: SEARCH from r/artificial\n",
      "     Query: 'science'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'science'\n",
      "  ‚úÖ r/artificial: 200 posts total\n",
      "\n",
      "  üìç Processing r/programming...\n",
      "  üîç Method: SEARCH from r/programming\n",
      "     Query: 'technology'\n",
      "     ‚úÖ Collected 39 posts using search\n",
      "     Added 39 posts from search: 'technology'\n",
      "  üîç Method: SEARCH from r/programming\n",
      "     Query: 'science'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 60 posts using search\n",
      "     Added 60 posts from search: 'science'\n",
      "  üîç Method: SEARCH from r/programming\n",
      "     Query: 'innovation'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: HOT from r/programming\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/programming\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/programming: 259 posts total\n",
      "\n",
      "  üìç Processing r/computers...\n",
      "  üîç Method: SEARCH from r/computers\n",
      "     Query: 'technology'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'technology'\n",
      "  üîç Method: SEARCH from r/computers\n",
      "     Query: 'science'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'science'\n",
      "  ‚úÖ r/computers: 200 posts total\n",
      "\n",
      "  üìä Removed 269 duplicates\n",
      "  ‚úÖ Science_Technology data saved to: C:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\data\\raw\\social_Media_data\\reddit\\Science_Technology.json\n",
      "     Posts: 1457\n",
      "     Subreddits: 8\n",
      "\n",
      "  üìä SCIENCE TECHNOLOGY SUMMARY:\n",
      "     Total posts: 1457\n",
      "     Posts with text: 415\n",
      "     Text ratio: 28.5%\n",
      "     Subreddits: 8\n",
      "     Average score: 2347.0\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 4/17: HEALTH\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING HEALTH DATA ===\n",
      "\n",
      "  üìç Processing r/Health...\n",
      "  üîç Method: SEARCH from r/Health\n",
      "     Query: 'health'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'health'\n",
      "  üîç Method: SEARCH from r/Health\n",
      "     Query: 'medicine'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 60 posts using search\n",
      "     Added 60 posts from search: 'medicine'\n",
      "  üîç Method: SEARCH from r/Health\n",
      "     Query: 'medical'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'medical'\n",
      "  ‚úÖ r/Health: 260 posts total\n",
      "\n",
      "  üìç Processing r/medicine...\n",
      "  üîç Method: SEARCH from r/medicine\n",
      "     Query: 'health'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'health'\n",
      "  üîç Method: SEARCH from r/medicine\n",
      "     Query: 'medicine'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'medicine'\n",
      "  üîç Method: SEARCH from r/medicine\n",
      "     Query: 'medical'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'medical'\n",
      "  ‚úÖ r/medicine: 300 posts total\n",
      "\n",
      "  üìç Processing r/medical...\n",
      "  üîç Method: SEARCH from r/medical\n",
      "     Query: 'health'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/medical\n",
      "     Query: 'medicine'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/medical\n",
      "     Query: 'medical'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: HOT from r/medical\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/medical\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/medical: 160 posts total\n",
      "\n",
      "  üìç Processing r/healthcare...\n",
      "  üîç Method: SEARCH from r/healthcare\n",
      "     Query: 'health'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'health'\n",
      "  üîç Method: SEARCH from r/healthcare\n",
      "     Query: 'medicine'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 90 posts using search\n",
      "     Added 90 posts from search: 'medicine'\n",
      "  üîç Method: SEARCH from r/healthcare\n",
      "     Query: 'medical'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'medical'\n",
      "  ‚úÖ r/healthcare: 290 posts total\n",
      "\n",
      "  üìç Processing r/nutrition...\n",
      "  üîç Method: SEARCH from r/nutrition\n",
      "     Query: 'health'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'health'\n",
      "  üîç Method: SEARCH from r/nutrition\n",
      "     Query: 'medicine'\n",
      "     ‚úÖ Collected 29 posts using search\n",
      "     Added 29 posts from search: 'medicine'\n",
      "  üîç Method: SEARCH from r/nutrition\n",
      "     Query: 'medical'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 66 posts using search\n",
      "     Added 66 posts from search: 'medical'\n",
      "  üîç Method: HOT from r/nutrition\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/nutrition: 275 posts total\n",
      "\n",
      "  üìç Processing r/fitness...\n",
      "  üîç Method: SEARCH from r/fitness\n",
      "     Query: 'health'\n",
      "     ‚úÖ Collected 1 posts using search\n",
      "     Added 1 posts from search: 'health'\n",
      "  üîç Method: SEARCH from r/fitness\n",
      "     Query: 'medicine'\n",
      "     ‚úÖ Collected 2 posts using search\n",
      "     Added 2 posts from search: 'medicine'\n",
      "  üîç Method: SEARCH from r/fitness\n",
      "     Query: 'medical'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'medical'\n",
      "  üîç Method: HOT from r/fitness\n",
      "     ‚úÖ Collected 43 posts using hot\n",
      "     Added 43 posts from hot\n",
      "  üîç Method: NEW from r/fitness\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/fitness: 226 posts total\n",
      "\n",
      "  üìç Processing r/mentalhealth...\n",
      "  üîç Method: SEARCH from r/mentalhealth\n",
      "     Query: 'health'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'health'\n",
      "  üîç Method: SEARCH from r/mentalhealth\n",
      "     Query: 'medicine'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'medicine'\n",
      "  üîç Method: SEARCH from r/mentalhealth\n",
      "     Query: 'medical'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'medical'\n",
      "  ‚úÖ r/mentalhealth: 300 posts total\n",
      "\n",
      "  üìä Removed 204 duplicates\n",
      "  ‚úÖ Health data saved to: C:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\data\\raw\\social_Media_data\\reddit\\Health.json\n",
      "     Posts: 1607\n",
      "     Subreddits: 7\n",
      "\n",
      "  üìä HEALTH SUMMARY:\n",
      "     Total posts: 1607\n",
      "     Posts with text: 1164\n",
      "     Text ratio: 72.4%\n",
      "     Subreddits: 7\n",
      "     Average score: 167.6\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 5/17: SPORT\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING SPORT DATA ===\n",
      "\n",
      "  üìç Processing r/sports...\n",
      "  üîç Method: SEARCH from r/sports\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  üîç Method: SEARCH from r/sports\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'football'\n",
      "  ‚úÖ r/sports: 200 posts total\n",
      "\n",
      "  üìç Processing r/nfl...\n",
      "  üîç Method: SEARCH from r/nfl\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  üîç Method: SEARCH from r/nfl\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'football'\n",
      "  ‚úÖ r/nfl: 200 posts total\n",
      "\n",
      "  üìç Processing r/nba...\n",
      "  üîç Method: SEARCH from r/nba\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  üîç Method: SEARCH from r/nba\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'football'\n",
      "  ‚úÖ r/nba: 200 posts total\n",
      "\n",
      "  üìç Processing r/soccer...\n",
      "  üîç Method: SEARCH from r/soccer\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  üîç Method: SEARCH from r/soccer\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'football'\n",
      "  ‚úÖ r/soccer: 200 posts total\n",
      "\n",
      "  üìç Processing r/baseball...\n",
      "  üîç Method: SEARCH from r/baseball\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  üîç Method: SEARCH from r/baseball\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'football'\n",
      "  ‚úÖ r/baseball: 200 posts total\n",
      "\n",
      "  üìç Processing r/hockey...\n",
      "  üîç Method: SEARCH from r/hockey\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  üîç Method: SEARCH from r/hockey\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'football'\n",
      "  ‚úÖ r/hockey: 200 posts total\n",
      "\n",
      "  üìç Processing r/olympics...\n",
      "  üîç Method: SEARCH from r/olympics\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  üîç Method: SEARCH from r/olympics\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 58 posts using search\n",
      "     Added 58 posts from search: 'football'\n",
      "  üîç Method: SEARCH from r/olympics\n",
      "     Query: 'basketball'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 53 posts using search\n",
      "     Added 53 posts from search: 'basketball'\n",
      "  ‚úÖ r/olympics: 211 posts total\n",
      "\n",
      "  üìç Processing r/football...\n",
      "  üîç Method: SEARCH from r/football\n",
      "     Query: 'sports'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sports'\n",
      "  üîç Method: SEARCH from r/football\n",
      "     Query: 'football'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'football'\n",
      "  ‚úÖ r/football: 200 posts total\n",
      "\n",
      "  üìä Removed 50 duplicates\n",
      "  ‚úÖ Sport data saved to: C:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\data\\raw\\social_Media_data\\reddit\\Sport.json\n",
      "     Posts: 1561\n",
      "     Subreddits: 8\n",
      "\n",
      "  üìä SPORT SUMMARY:\n",
      "     Total posts: 1561\n",
      "     Posts with text: 420\n",
      "     Text ratio: 26.9%\n",
      "     Subreddits: 8\n",
      "     Average score: 3213.5\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 6/17: ARTS CULTURE ENTERTAINMENT MEDIA\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING ARTS CULTURE ENTERTAINMENT MEDIA DATA ===\n",
      "\n",
      "  üìç Processing r/movies...\n",
      "  üîç Method: SEARCH from r/movies\n",
      "     Query: 'movies'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'movies'\n",
      "  üîç Method: SEARCH from r/movies\n",
      "     Query: 'music'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'music'\n",
      "  ‚úÖ r/movies: 200 posts total\n",
      "\n",
      "  üìç Processing r/music...\n",
      "  üîç Method: SEARCH from r/music\n",
      "     Query: 'movies'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'movies'\n",
      "  üîç Method: SEARCH from r/music\n",
      "     Query: 'music'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'music'\n",
      "  ‚úÖ r/music: 200 posts total\n",
      "\n",
      "  üìç Processing r/art...\n",
      "  üîç Method: SEARCH from r/art\n",
      "     Query: 'movies'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 74 posts using search\n",
      "     Added 74 posts from search: 'movies'\n",
      "  üîç Method: SEARCH from r/art\n",
      "     Query: 'music'\n",
      "     ‚úÖ Collected 43 posts using search\n",
      "     Added 43 posts from search: 'music'\n",
      "  üîç Method: SEARCH from r/art\n",
      "     Query: 'art'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'art'\n",
      "  ‚úÖ r/art: 217 posts total\n",
      "\n",
      "  üìç Processing r/books...\n",
      "  üîç Method: SEARCH from r/books\n",
      "     Query: 'movies'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'movies'\n",
      "  üîç Method: SEARCH from r/books\n",
      "     Query: 'music'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 70 posts using search\n",
      "     Added 70 posts from search: 'music'\n",
      "  üîç Method: SEARCH from r/books\n",
      "     Query: 'art'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'art'\n",
      "  ‚úÖ r/books: 270 posts total\n",
      "\n",
      "  üìç Processing r/television...\n",
      "  üîç Method: SEARCH from r/television\n",
      "     Query: 'movies'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'movies'\n",
      "  üîç Method: SEARCH from r/television\n",
      "     Query: 'music'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'music'\n",
      "  ‚úÖ r/television: 200 posts total\n",
      "\n",
      "  üìç Processing r/entertainment...\n",
      "  üîç Method: SEARCH from r/entertainment\n",
      "     Query: 'movies'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'movies'\n",
      "  üîç Method: SEARCH from r/entertainment\n",
      "     Query: 'music'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'music'\n",
      "  ‚úÖ r/entertainment: 200 posts total\n",
      "\n",
      "  üìç Processing r/culture...\n",
      "  üîç Method: SEARCH from r/culture\n",
      "     Query: 'movies'\n",
      "     ‚úÖ Collected 14 posts using search\n",
      "     Added 14 posts from search: 'movies'\n",
      "  üîç Method: SEARCH from r/culture\n",
      "     Query: 'music'\n",
      "     ‚úÖ Collected 30 posts using search\n",
      "     Added 30 posts from search: 'music'\n",
      "  üîç Method: SEARCH from r/culture\n",
      "     Query: 'art'\n",
      "     ‚úÖ Collected 32 posts using search\n",
      "     Added 32 posts from search: 'art'\n",
      "  üîç Method: HOT from r/culture\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/culture\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/culture: 236 posts total\n",
      "\n",
      "  üìç Processing r/media...\n",
      "  üîç Method: SEARCH from r/media\n",
      "     Query: 'movies'\n",
      "     ‚ùå Error with search method: received 404 HTTP response\n",
      "  üîç Method: SEARCH from r/media\n",
      "     Query: 'music'\n",
      "     ‚ùå Error with search method: received 404 HTTP response\n",
      "  üîç Method: SEARCH from r/media\n",
      "     Query: 'art'\n",
      "     ‚ùå Error with search method: received 404 HTTP response\n",
      "  üîç Method: HOT from r/media\n",
      "     ‚ùå Error with hot method: received 404 HTTP response\n",
      "  üîç Method: NEW from r/media\n",
      "     ‚ùå Error with new method: received 404 HTTP response\n",
      "  ‚úÖ r/media: 0 posts total\n",
      "\n",
      "  üìä Removed 119 duplicates\n",
      "  ‚úÖ Arts_Culture_Entertainment_Media data saved to: C:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\data\\raw\\social_Media_data\\reddit\\Arts_Culture_Entertainment_Media.json\n",
      "     Posts: 1404\n",
      "     Subreddits: 7\n",
      "\n",
      "  üìä ARTS CULTURE ENTERTAINMENT MEDIA SUMMARY:\n",
      "     Total posts: 1404\n",
      "     Posts with text: 554\n",
      "     Text ratio: 39.5%\n",
      "     Subreddits: 7\n",
      "     Average score: 2409.0\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 7/17: SOCIETY\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING SOCIETY DATA ===\n",
      "\n",
      "  üìç Processing r/sociology...\n",
      "  üîç Method: SEARCH from r/sociology\n",
      "     Query: 'society'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'society'\n",
      "  üîç Method: SEARCH from r/sociology\n",
      "     Query: 'social'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'social'\n",
      "  üîç Method: SEARCH from r/sociology\n",
      "     Query: 'community'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 65 posts using search\n",
      "     Added 65 posts from search: 'community'\n",
      "  ‚úÖ r/sociology: 265 posts total\n",
      "\n",
      "  üìç Processing r/society...\n",
      "  üîç Method: SEARCH from r/society\n",
      "     Query: 'society'\n",
      "     ‚ùå Error with search method: received 403 HTTP response\n",
      "  üîç Method: SEARCH from r/society\n",
      "     Query: 'social'\n",
      "     ‚ùå Error with search method: received 403 HTTP response\n",
      "  üîç Method: SEARCH from r/society\n",
      "     Query: 'community'\n",
      "     ‚ùå Error with search method: received 403 HTTP response\n",
      "  üîç Method: HOT from r/society\n",
      "     ‚ùå Error with hot method: received 403 HTTP response\n",
      "  üîç Method: NEW from r/society\n",
      "     ‚ùå Error with new method: received 403 HTTP response\n",
      "  ‚úÖ r/society: 0 posts total\n",
      "\n",
      "  üìç Processing r/social...\n",
      "  üîç Method: SEARCH from r/social\n",
      "     Query: 'society'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/social\n",
      "     Query: 'social'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/social\n",
      "     Query: 'community'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: HOT from r/social\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/social\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/social: 160 posts total\n",
      "\n",
      "  üìç Processing r/community...\n",
      "  üîç Method: SEARCH from r/community\n",
      "     Query: 'society'\n",
      "     ‚úÖ Collected 3 posts using search\n",
      "     Added 3 posts from search: 'society'\n",
      "  üîç Method: SEARCH from r/community\n",
      "     Query: 'social'\n",
      "     ‚úÖ Collected 19 posts using search\n",
      "     Added 19 posts from search: 'social'\n",
      "  üîç Method: SEARCH from r/community\n",
      "     Query: 'community'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'community'\n",
      "  üîç Method: HOT from r/community\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/community\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/community: 282 posts total\n",
      "\n",
      "  üìç Processing r/TrueReddit...\n",
      "  üîç Method: SEARCH from r/TrueReddit\n",
      "     Query: 'society'\n",
      "     ‚úÖ Collected 10 posts using search\n",
      "     Added 10 posts from search: 'society'\n",
      "  üîç Method: SEARCH from r/TrueReddit\n",
      "     Query: 'social'\n",
      "     ‚úÖ Collected 41 posts using search\n",
      "     Added 41 posts from search: 'social'\n",
      "  üîç Method: SEARCH from r/TrueReddit\n",
      "     Query: 'community'\n",
      "     ‚úÖ Collected 4 posts using search\n",
      "     Added 4 posts from search: 'community'\n",
      "  üîç Method: HOT from r/TrueReddit\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/TrueReddit\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/TrueReddit: 215 posts total\n",
      "\n",
      "  üìç Processing r/news...\n",
      "  üîç Method: SEARCH from r/news\n",
      "     Query: 'society'\n",
      "     ‚úÖ Collected 26 posts using search\n",
      "     Added 26 posts from search: 'society'\n",
      "  üîç Method: SEARCH from r/news\n",
      "     Query: 'social'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using search\n",
      "     Added 80 posts from search: 'social'\n",
      "  üîç Method: SEARCH from r/news\n",
      "     Query: 'community'\n",
      "     ‚úÖ Collected 18 posts using search\n",
      "     Added 18 posts from search: 'community'\n",
      "  üîç Method: HOT from r/news\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/news\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/news: 284 posts total\n",
      "\n",
      "  üìä Removed 355 duplicates\n",
      "  ‚úÖ Society data saved to: C:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\data\\raw\\social_Media_data\\reddit\\Society.json\n",
      "     Posts: 851\n",
      "     Subreddits: 5\n",
      "\n",
      "  üìä SOCIETY SUMMARY:\n",
      "     Total posts: 851\n",
      "     Posts with text: 339\n",
      "     Text ratio: 39.8%\n",
      "     Subreddits: 5\n",
      "     Average score: 2416.4\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 8/17: HUMAN INTEREST\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING HUMAN INTEREST DATA ===\n",
      "\n",
      "  üìç Processing r/UpliftingNews...\n",
      "  üîç Method: SEARCH from r/UpliftingNews\n",
      "     Query: 'human interest'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/UpliftingNews\n",
      "     Query: 'inspiring'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 50 posts using search\n",
      "     Added 50 posts from search: 'inspiring'\n",
      "  üîç Method: SEARCH from r/UpliftingNews\n",
      "     Query: 'heartwarming'\n",
      "     ‚úÖ Collected 26 posts using search\n",
      "     Added 26 posts from search: 'heartwarming'\n",
      "  üîç Method: HOT from r/UpliftingNews\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/UpliftingNews\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/UpliftingNews: 236 posts total\n",
      "\n",
      "  üìç Processing r/MadeMeSmile...\n",
      "  üîç Method: SEARCH from r/MadeMeSmile\n",
      "     Query: 'human interest'\n",
      "     ‚úÖ Collected 6 posts using search\n",
      "     Added 6 posts from search: 'human interest'\n",
      "  üîç Method: SEARCH from r/MadeMeSmile\n",
      "     Query: 'inspiring'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'inspiring'\n",
      "  üîç Method: SEARCH from r/MadeMeSmile\n",
      "     Query: 'heartwarming'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'heartwarming'\n",
      "  üîç Method: HOT from r/MadeMeSmile\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/MadeMeSmile: 286 posts total\n",
      "\n",
      "  üìç Processing r/happy...\n",
      "  üîç Method: SEARCH from r/happy\n",
      "     Query: 'human interest'\n",
      "     ‚úÖ Collected 1 posts using search\n",
      "     Added 1 posts from search: 'human interest'\n",
      "  üîç Method: SEARCH from r/happy\n",
      "     Query: 'inspiring'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 64 posts using search\n",
      "     Added 64 posts from search: 'inspiring'\n",
      "  üîç Method: SEARCH from r/happy\n",
      "     Query: 'heartwarming'\n",
      "     ‚úÖ Collected 11 posts using search\n",
      "     Added 11 posts from search: 'heartwarming'\n",
      "  üîç Method: HOT from r/happy\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/happy\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/happy: 236 posts total\n",
      "\n",
      "  üìç Processing r/wholesome...\n",
      "  üîç Method: SEARCH from r/wholesome\n",
      "     Query: 'human interest'\n",
      "     ‚úÖ Collected 1 posts using search\n",
      "     Added 1 posts from search: 'human interest'\n",
      "  üîç Method: SEARCH from r/wholesome\n",
      "     Query: 'inspiring'\n",
      "     ‚úÖ Collected 21 posts using search\n",
      "     Added 21 posts from search: 'inspiring'\n",
      "  üîç Method: SEARCH from r/wholesome\n",
      "     Query: 'heartwarming'\n",
      "     ‚úÖ Collected 29 posts using search\n",
      "     Added 29 posts from search: 'heartwarming'\n",
      "  üîç Method: HOT from r/wholesome\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/wholesome\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/wholesome: 211 posts total\n",
      "\n",
      "  üìç Processing r/HumansBeingBros...\n",
      "  üîç Method: SEARCH from r/HumansBeingBros\n",
      "     Query: 'human interest'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/HumansBeingBros\n",
      "     Query: 'inspiring'\n",
      "     ‚úÖ Collected 5 posts using search\n",
      "     Added 5 posts from search: 'inspiring'\n",
      "  üîç Method: SEARCH from r/HumansBeingBros\n",
      "     Query: 'heartwarming'\n",
      "     ‚úÖ Collected 12 posts using search\n",
      "     Added 12 posts from search: 'heartwarming'\n",
      "  üîç Method: HOT from r/HumansBeingBros\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/HumansBeingBros\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/HumansBeingBros: 177 posts total\n",
      "\n",
      "  üìç Processing r/todayilearned...\n",
      "  üîç Method: SEARCH from r/todayilearned\n",
      "     Query: 'human interest'\n",
      "     ‚úÖ Collected 10 posts using search\n",
      "     Added 10 posts from search: 'human interest'\n",
      "  üîç Method: SEARCH from r/todayilearned\n",
      "     Query: 'inspiring'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'inspiring'\n",
      "  üîç Method: SEARCH from r/todayilearned\n",
      "     Query: 'heartwarming'\n",
      "     ‚úÖ Collected 5 posts using search\n",
      "     Added 5 posts from search: 'heartwarming'\n",
      "  üîç Method: HOT from r/todayilearned\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/todayilearned\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/todayilearned: 275 posts total\n",
      "\n",
      "  üìä Removed 411 duplicates\n",
      "  ‚úÖ Human_Interest data saved to: C:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\data\\raw\\social_Media_data\\reddit\\Human_Interest.json\n",
      "     Posts: 1010\n",
      "     Subreddits: 6\n",
      "\n",
      "  üìä HUMAN INTEREST SUMMARY:\n",
      "     Total posts: 1010\n",
      "     Posts with text: 119\n",
      "     Text ratio: 11.8%\n",
      "     Subreddits: 6\n",
      "     Average score: 8996.5\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 9/17: CRIME LAW JUSTICE\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING CRIME LAW JUSTICE DATA ===\n",
      "\n",
      "  üìç Processing r/law...\n",
      "  üîç Method: SEARCH from r/law\n",
      "     Query: 'crime'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'crime'\n",
      "  üîç Method: SEARCH from r/law\n",
      "     Query: 'law'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'law'\n",
      "  üîç Method: SEARCH from r/law\n",
      "     Query: 'justice'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'justice'\n",
      "  ‚úÖ r/law: 300 posts total\n",
      "\n",
      "  üìç Processing r/legaladvice...\n",
      "  üîç Method: SEARCH from r/legaladvice\n",
      "     Query: 'crime'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'crime'\n",
      "  üîç Method: SEARCH from r/legaladvice\n",
      "     Query: 'law'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'law'\n",
      "  üîç Method: SEARCH from r/legaladvice\n",
      "     Query: 'justice'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'justice'\n",
      "  ‚úÖ r/legaladvice: 300 posts total\n",
      "\n",
      "  üìç Processing r/UnresolvedMysteries...\n",
      "  üîç Method: SEARCH from r/UnresolvedMysteries\n",
      "     Query: 'crime'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'crime'\n",
      "  üîç Method: SEARCH from r/UnresolvedMysteries\n",
      "     Query: 'law'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'law'\n",
      "  üîç Method: SEARCH from r/UnresolvedMysteries\n",
      "     Query: 'justice'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'justice'\n",
      "  ‚úÖ r/UnresolvedMysteries: 300 posts total\n",
      "\n",
      "  üìç Processing r/TrueCrime...\n",
      "  üîç Method: SEARCH from r/TrueCrime\n",
      "     Query: 'crime'\n",
      "     ‚úÖ Collected 32 posts using search\n",
      "     Added 32 posts from search: 'crime'\n",
      "  üîç Method: SEARCH from r/TrueCrime\n",
      "     Query: 'law'\n",
      "     ‚úÖ Collected 8 posts using search\n",
      "     Added 8 posts from search: 'law'\n",
      "  üîç Method: SEARCH from r/TrueCrime\n",
      "     Query: 'justice'\n",
      "     ‚úÖ Collected 8 posts using search\n",
      "     Added 8 posts from search: 'justice'\n",
      "  üîç Method: HOT from r/TrueCrime\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/TrueCrime\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/TrueCrime: 208 posts total\n",
      "\n",
      "  üìç Processing r/justice...\n",
      "  üîç Method: SEARCH from r/justice\n",
      "     Query: 'crime'\n",
      "     ‚ùå Error with search method: received 403 HTTP response\n",
      "  üîç Method: SEARCH from r/justice\n",
      "     Query: 'law'\n",
      "     ‚ùå Error with search method: received 403 HTTP response\n",
      "  üîç Method: SEARCH from r/justice\n",
      "     Query: 'justice'\n",
      "     ‚ùå Error with search method: received 403 HTTP response\n",
      "  üîç Method: HOT from r/justice\n",
      "     ‚ùå Error with hot method: received 403 HTTP response\n",
      "  üîç Method: NEW from r/justice\n",
      "     ‚ùå Error with new method: received 403 HTTP response\n",
      "  ‚úÖ r/justice: 0 posts total\n",
      "\n",
      "  üìç Processing r/police...\n",
      "  üîç Method: SEARCH from r/police\n",
      "     Query: 'crime'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'crime'\n",
      "  üîç Method: SEARCH from r/police\n",
      "     Query: 'law'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'law'\n",
      "  üîç Method: SEARCH from r/police\n",
      "     Query: 'justice'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'justice'\n",
      "  ‚úÖ r/police: 300 posts total\n",
      "\n",
      "  üìä Removed 233 duplicates\n",
      "  ‚úÖ Crime_Law_Justice data saved to: C:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\data\\raw\\social_Media_data\\reddit\\Crime_Law_Justice.json\n",
      "     Posts: 1175\n",
      "     Subreddits: 5\n",
      "\n",
      "  üìä CRIME LAW JUSTICE SUMMARY:\n",
      "     Total posts: 1175\n",
      "     Posts with text: 832\n",
      "     Text ratio: 70.8%\n",
      "     Subreddits: 5\n",
      "     Average score: 3555.4\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 10/17: CONFLICT WAR PEACE\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING CONFLICT WAR PEACE DATA ===\n",
      "\n",
      "  üìç Processing r/worldnews...\n",
      "  üîç Method: SEARCH from r/worldnews\n",
      "     Query: 'war'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'war'\n",
      "  üîç Method: SEARCH from r/worldnews\n",
      "     Query: 'conflict'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'conflict'\n",
      "  üîç Method: SEARCH from r/worldnews\n",
      "     Query: 'military'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'military'\n",
      "  ‚úÖ r/worldnews: 300 posts total\n",
      "\n",
      "  üìç Processing r/geopolitics...\n",
      "  üîç Method: SEARCH from r/geopolitics\n",
      "     Query: 'war'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'war'\n",
      "  üîç Method: SEARCH from r/geopolitics\n",
      "     Query: 'conflict'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'conflict'\n",
      "  üîç Method: SEARCH from r/geopolitics\n",
      "     Query: 'military'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'military'\n",
      "  ‚úÖ r/geopolitics: 300 posts total\n",
      "\n",
      "  üìç Processing r/Military...\n",
      "  üîç Method: SEARCH from r/Military\n",
      "     Query: 'war'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'war'\n",
      "  üîç Method: SEARCH from r/Military\n",
      "     Query: 'conflict'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'conflict'\n",
      "  üîç Method: SEARCH from r/Military\n",
      "     Query: 'military'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'military'\n",
      "  ‚úÖ r/Military: 300 posts total\n",
      "\n",
      "  üìç Processing r/CredibleDefense...\n",
      "  üîç Method: SEARCH from r/CredibleDefense\n",
      "     Query: 'war'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'war'\n",
      "  üîç Method: SEARCH from r/CredibleDefense\n",
      "     Query: 'conflict'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'conflict'\n",
      "  üîç Method: SEARCH from r/CredibleDefense\n",
      "     Query: 'military'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 93 posts using search\n",
      "     Added 93 posts from search: 'military'\n",
      "  üîç Method: HOT from r/CredibleDefense\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/CredibleDefense: 373 posts total\n",
      "\n",
      "  üìç Processing r/conflict...\n",
      "  üîç Method: SEARCH from r/conflict\n",
      "     Query: 'war'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/conflict\n",
      "     Query: 'conflict'\n",
      "     ‚úÖ Collected 2 posts using search\n",
      "     Added 2 posts from search: 'conflict'\n",
      "  üîç Method: SEARCH from r/conflict\n",
      "     Query: 'military'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: HOT from r/conflict\n",
      "     ‚úÖ Collected 2 posts using hot\n",
      "     Added 2 posts from hot\n",
      "  üîç Method: NEW from r/conflict\n",
      "     ‚úÖ Collected 2 posts using new\n",
      "     Added 2 posts from new\n",
      "  ‚úÖ r/conflict: 6 posts total\n",
      "\n",
      "  üìä Removed 148 duplicates\n",
      "  ‚úÖ Conflict_War_Peace data saved to: C:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\data\\raw\\social_Media_data\\reddit\\Conflict_War_Peace.json\n",
      "     Posts: 1131\n",
      "     Subreddits: 5\n",
      "\n",
      "  üìä CONFLICT WAR PEACE SUMMARY:\n",
      "     Total posts: 1131\n",
      "     Posts with text: 343\n",
      "     Text ratio: 30.3%\n",
      "     Subreddits: 5\n",
      "     Average score: 3452.5\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 11/17: DISASTER ACCIDENT EMERGENCY\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING DISASTER ACCIDENT EMERGENCY DATA ===\n",
      "\n",
      "  üìç Processing r/worldnews...\n",
      "  üîç Method: SEARCH from r/worldnews\n",
      "     Query: 'disaster'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'disaster'\n",
      "  üîç Method: SEARCH from r/worldnews\n",
      "     Query: 'emergency'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'emergency'\n",
      "  üîç Method: SEARCH from r/worldnews\n",
      "     Query: 'accident'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 93 posts using search\n",
      "     Added 93 posts from search: 'accident'\n",
      "  üîç Method: HOT from r/worldnews\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/worldnews\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/worldnews: 453 posts total\n",
      "\n",
      "  üìç Processing r/news...\n",
      "  üîç Method: SEARCH from r/news\n",
      "     Query: 'disaster'\n",
      "     ‚úÖ Collected 25 posts using search\n",
      "     Added 25 posts from search: 'disaster'\n",
      "  üîç Method: SEARCH from r/news\n",
      "     Query: 'emergency'\n",
      "     ‚úÖ Collected 37 posts using search\n",
      "     Added 37 posts from search: 'emergency'\n",
      "  üîç Method: SEARCH from r/news\n",
      "     Query: 'accident'\n",
      "     ‚úÖ Collected 37 posts using search\n",
      "     Added 37 posts from search: 'accident'\n",
      "  üîç Method: HOT from r/news\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/news\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/news: 259 posts total\n",
      "\n",
      "  üìç Processing r/CatastrophicFailure...\n",
      "  üîç Method: SEARCH from r/CatastrophicFailure\n",
      "     Query: 'disaster'\n",
      "     ‚úÖ Collected 31 posts using search\n",
      "     Added 31 posts from search: 'disaster'\n",
      "  üîç Method: SEARCH from r/CatastrophicFailure\n",
      "     Query: 'emergency'\n",
      "     ‚úÖ Collected 15 posts using search\n",
      "     Added 15 posts from search: 'emergency'\n",
      "  üîç Method: SEARCH from r/CatastrophicFailure\n",
      "     Query: 'accident'\n",
      "     ‚úÖ Collected 34 posts using search\n",
      "     Added 34 posts from search: 'accident'\n",
      "  üîç Method: HOT from r/CatastrophicFailure\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/CatastrophicFailure\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/CatastrophicFailure: 240 posts total\n",
      "\n",
      "  üìç Processing r/emergencyservices...\n",
      "  üîç Method: SEARCH from r/emergencyservices\n",
      "     Query: 'disaster'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: SEARCH from r/emergencyservices\n",
      "     Query: 'emergency'\n",
      "     ‚úÖ Collected 2 posts using search\n",
      "     Added 2 posts from search: 'emergency'\n",
      "  üîç Method: SEARCH from r/emergencyservices\n",
      "     Query: 'accident'\n",
      "     ‚úÖ Collected 1 posts using search\n",
      "     Added 1 posts from search: 'accident'\n",
      "  üîç Method: HOT from r/emergencyservices\n",
      "     ‚úÖ Collected 22 posts using hot\n",
      "     Added 22 posts from hot\n",
      "  üîç Method: NEW from r/emergencyservices\n",
      "     ‚úÖ Collected 22 posts using new\n",
      "     Added 22 posts from new\n",
      "  ‚úÖ r/emergencyservices: 47 posts total\n",
      "\n",
      "  üìä Removed 277 duplicates\n",
      "  ‚úÖ Disaster_Accident_Emergency data saved to: C:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\data\\raw\\social_Media_data\\reddit\\Disaster_Accident_Emergency.json\n",
      "     Posts: 722\n",
      "     Subreddits: 4\n",
      "\n",
      "  üìä DISASTER ACCIDENT EMERGENCY SUMMARY:\n",
      "     Total posts: 722\n",
      "     Posts with text: 16\n",
      "     Text ratio: 2.2%\n",
      "     Subreddits: 4\n",
      "     Average score: 3031.5\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 12/17: ENVIRONMENT\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING ENVIRONMENT DATA ===\n",
      "\n",
      "  üìç Processing r/environment...\n",
      "  üîç Method: SEARCH from r/environment\n",
      "     Query: 'environment'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'environment'\n",
      "  üîç Method: SEARCH from r/environment\n",
      "     Query: 'climate change'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'climate change'\n",
      "  üîç Method: SEARCH from r/environment\n",
      "     Query: 'sustainability'\n",
      "     ‚úÖ Collected 47 posts using search\n",
      "     Added 47 posts from search: 'sustainability'\n",
      "  üîç Method: HOT from r/environment\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/environment: 327 posts total\n",
      "\n",
      "  üìç Processing r/climatechange...\n",
      "  üîç Method: SEARCH from r/climatechange\n",
      "     Query: 'environment'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 77 posts using search\n",
      "     Added 77 posts from search: 'environment'\n",
      "  üîç Method: SEARCH from r/climatechange\n",
      "     Query: 'climate change'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'climate change'\n",
      "  üîç Method: SEARCH from r/climatechange\n",
      "     Query: 'sustainability'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 94 posts using search\n",
      "     Added 94 posts from search: 'sustainability'\n",
      "  ‚úÖ r/climatechange: 271 posts total\n",
      "\n",
      "  üìç Processing r/sustainability...\n",
      "  üîç Method: SEARCH from r/sustainability\n",
      "     Query: 'environment'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 61 posts using search\n",
      "     Added 61 posts from search: 'environment'\n",
      "  üîç Method: SEARCH from r/sustainability\n",
      "     Query: 'climate change'\n",
      "     ‚úÖ Collected 40 posts using search\n",
      "     Added 40 posts from search: 'climate change'\n",
      "  üîç Method: SEARCH from r/sustainability\n",
      "     Query: 'sustainability'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'sustainability'\n",
      "  üîç Method: HOT from r/sustainability\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/sustainability: 281 posts total\n",
      "\n",
      "  üìç Processing r/nature...\n",
      "  üîç Method: SEARCH from r/nature\n",
      "     Query: 'environment'\n",
      "     ‚úÖ Collected 10 posts using search\n",
      "     Added 10 posts from search: 'environment'\n",
      "  üîç Method: SEARCH from r/nature\n",
      "     Query: 'climate change'\n",
      "     ‚úÖ Collected 18 posts using search\n",
      "     Added 18 posts from search: 'climate change'\n",
      "  üîç Method: SEARCH from r/nature\n",
      "     Query: 'sustainability'\n",
      "     ‚úÖ Collected 5 posts using search\n",
      "     Added 5 posts from search: 'sustainability'\n",
      "  üîç Method: HOT from r/nature\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/nature\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/nature: 193 posts total\n",
      "\n",
      "  üìç Processing r/ecology...\n",
      "  üîç Method: SEARCH from r/ecology\n",
      "     Query: 'environment'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'environment'\n",
      "  üîç Method: SEARCH from r/ecology\n",
      "     Query: 'climate change'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 50 posts using search\n",
      "     Added 50 posts from search: 'climate change'\n",
      "  üîç Method: SEARCH from r/ecology\n",
      "     Query: 'sustainability'\n",
      "     ‚úÖ Collected 46 posts using search\n",
      "     Added 46 posts from search: 'sustainability'\n",
      "  üîç Method: HOT from r/ecology\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/ecology: 276 posts total\n",
      "\n",
      "  üìç Processing r/green...\n",
      "  üîç Method: SEARCH from r/green\n",
      "     Query: 'environment'\n",
      "     ‚úÖ Collected 13 posts using search\n",
      "     Added 13 posts from search: 'environment'\n",
      "  üîç Method: SEARCH from r/green\n",
      "     Query: 'climate change'\n",
      "     ‚úÖ Collected 16 posts using search\n",
      "     Added 16 posts from search: 'climate change'\n",
      "  üîç Method: SEARCH from r/green\n",
      "     Query: 'sustainability'\n",
      "     ‚úÖ Collected 29 posts using search\n",
      "     Added 29 posts from search: 'sustainability'\n",
      "  üîç Method: HOT from r/green\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/green\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/green: 218 posts total\n",
      "\n",
      "  üìä Removed 286 duplicates\n",
      "  ‚úÖ Environment data saved to: C:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\data\\raw\\social_Media_data\\reddit\\Environment.json\n",
      "     Posts: 1280\n",
      "     Subreddits: 6\n",
      "\n",
      "  üìä ENVIRONMENT SUMMARY:\n",
      "     Total posts: 1280\n",
      "     Posts with text: 518\n",
      "     Text ratio: 40.5%\n",
      "     Subreddits: 6\n",
      "     Average score: 348.1\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 13/17: EDUCATION\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING EDUCATION DATA ===\n",
      "\n",
      "  üìç Processing r/education...\n",
      "  üîç Method: SEARCH from r/education\n",
      "     Query: 'education'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'education'\n",
      "  üîç Method: SEARCH from r/education\n",
      "     Query: 'school'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'school'\n",
      "  üîç Method: SEARCH from r/education\n",
      "     Query: 'university'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'university'\n",
      "  ‚úÖ r/education: 300 posts total\n",
      "\n",
      "  üìç Processing r/teachers...\n",
      "  üîç Method: SEARCH from r/teachers\n",
      "     Query: 'education'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'education'\n",
      "  üîç Method: SEARCH from r/teachers\n",
      "     Query: 'school'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'school'\n",
      "  üîç Method: SEARCH from r/teachers\n",
      "     Query: 'university'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'university'\n",
      "  ‚úÖ r/teachers: 300 posts total\n",
      "\n",
      "  üìç Processing r/college...\n",
      "  üîç Method: SEARCH from r/college\n",
      "     Query: 'education'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'education'\n",
      "  üîç Method: SEARCH from r/college\n",
      "     Query: 'school'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'school'\n",
      "  üîç Method: SEARCH from r/college\n",
      "     Query: 'university'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'university'\n",
      "  ‚úÖ r/college: 300 posts total\n",
      "\n",
      "  üìç Processing r/university...\n",
      "  üîç Method: SEARCH from r/university\n",
      "     Query: 'education'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'education'\n",
      "  üîç Method: SEARCH from r/university\n",
      "     Query: 'school'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'school'\n",
      "  üîç Method: SEARCH from r/university\n",
      "     Query: 'university'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'university'\n",
      "  ‚úÖ r/university: 300 posts total\n",
      "\n",
      "  üìç Processing r/students...\n",
      "  üîç Method: SEARCH from r/students\n",
      "     Query: 'education'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'education'\n",
      "  üîç Method: SEARCH from r/students\n",
      "     Query: 'school'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'school'\n",
      "  üîç Method: SEARCH from r/students\n",
      "     Query: 'university'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'university'\n",
      "  ‚úÖ r/students: 300 posts total\n",
      "\n",
      "  üìç Processing r/Academia...\n",
      "  üîç Method: SEARCH from r/Academia\n",
      "     Query: 'education'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'education'\n",
      "  üîç Method: SEARCH from r/Academia\n",
      "     Query: 'school'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'school'\n",
      "  üîç Method: SEARCH from r/Academia\n",
      "     Query: 'university'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'university'\n",
      "  ‚úÖ r/Academia: 300 posts total\n",
      "\n",
      "  üìä Removed 129 duplicates\n",
      "  ‚úÖ Education data saved to: C:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\data\\raw\\social_Media_data\\reddit\\Education.json\n",
      "     Posts: 1671\n",
      "     Subreddits: 6\n",
      "\n",
      "  üìä EDUCATION SUMMARY:\n",
      "     Total posts: 1671\n",
      "     Posts with text: 1472\n",
      "     Text ratio: 88.1%\n",
      "     Subreddits: 6\n",
      "     Average score: 435.0\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 14/17: LABOUR\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING LABOUR DATA ===\n",
      "\n",
      "  üìç Processing r/jobs...\n",
      "  üîç Method: SEARCH from r/jobs\n",
      "     Query: 'jobs'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'jobs'\n",
      "  üîç Method: SEARCH from r/jobs\n",
      "     Query: 'employment'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'employment'\n",
      "  üîç Method: SEARCH from r/jobs\n",
      "     Query: 'labor'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'labor'\n",
      "  ‚úÖ r/jobs: 300 posts total\n",
      "\n",
      "  üìç Processing r/antiwork...\n",
      "  üîç Method: SEARCH from r/antiwork\n",
      "     Query: 'jobs'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'jobs'\n",
      "  üîç Method: SEARCH from r/antiwork\n",
      "     Query: 'employment'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'employment'\n",
      "  üîç Method: SEARCH from r/antiwork\n",
      "     Query: 'labor'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'labor'\n",
      "  ‚úÖ r/antiwork: 300 posts total\n",
      "\n",
      "  üìç Processing r/WorkReform...\n",
      "  üîç Method: SEARCH from r/WorkReform\n",
      "     Query: 'jobs'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'jobs'\n",
      "  üîç Method: SEARCH from r/WorkReform\n",
      "     Query: 'employment'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'employment'\n",
      "  üîç Method: SEARCH from r/WorkReform\n",
      "     Query: 'labor'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'labor'\n",
      "  ‚úÖ r/WorkReform: 300 posts total\n",
      "\n",
      "  üìç Processing r/labor...\n",
      "  üîç Method: SEARCH from r/labor\n",
      "     Query: 'jobs'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 70 posts using search\n",
      "     Added 70 posts from search: 'jobs'\n",
      "  üîç Method: SEARCH from r/labor\n",
      "     Query: 'employment'\n",
      "     ‚úÖ Collected 40 posts using search\n",
      "     Added 40 posts from search: 'employment'\n",
      "  üîç Method: SEARCH from r/labor\n",
      "     Query: 'labor'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'labor'\n",
      "  üîç Method: HOT from r/labor\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/labor: 290 posts total\n",
      "\n",
      "  üìç Processing r/union...\n",
      "  üîç Method: SEARCH from r/union\n",
      "     Query: 'jobs'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'jobs'\n",
      "  üîç Method: SEARCH from r/union\n",
      "     Query: 'employment'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'employment'\n",
      "  üîç Method: SEARCH from r/union\n",
      "     Query: 'labor'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'labor'\n",
      "  ‚úÖ r/union: 300 posts total\n",
      "\n",
      "  üìç Processing r/employment...\n",
      "  üîç Method: SEARCH from r/employment\n",
      "     Query: 'jobs'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 50 posts using search\n",
      "     Added 50 posts from search: 'jobs'\n",
      "  üîç Method: SEARCH from r/employment\n",
      "     Query: 'employment'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 75 posts using search\n",
      "     Added 75 posts from search: 'employment'\n",
      "  üîç Method: SEARCH from r/employment\n",
      "     Query: 'labor'\n",
      "     ‚úÖ Collected 0 posts using search\n",
      "  üîç Method: HOT from r/employment\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/employment\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/employment: 285 posts total\n",
      "\n",
      "  üìä Removed 308 duplicates\n",
      "  ‚úÖ Labour data saved to: C:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\data\\raw\\social_Media_data\\reddit\\Labour.json\n",
      "     Posts: 1467\n",
      "     Subreddits: 6\n",
      "\n",
      "  üìä LABOUR SUMMARY:\n",
      "     Total posts: 1467\n",
      "     Posts with text: 643\n",
      "     Text ratio: 43.8%\n",
      "     Subreddits: 6\n",
      "     Average score: 2959.9\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 15/17: LIFESTYLE LEISURE\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING LIFESTYLE LEISURE DATA ===\n",
      "\n",
      "  üìç Processing r/lifestyle...\n",
      "  üîç Method: SEARCH from r/lifestyle\n",
      "     Query: 'lifestyle'\n",
      "     ‚ùå Error with search method: received 403 HTTP response\n",
      "  üîç Method: SEARCH from r/lifestyle\n",
      "     Query: 'travel'\n",
      "     ‚ùå Error with search method: received 403 HTTP response\n",
      "  üîç Method: SEARCH from r/lifestyle\n",
      "     Query: 'food'\n",
      "     ‚ùå Error with search method: received 403 HTTP response\n",
      "  üîç Method: HOT from r/lifestyle\n",
      "     ‚ùå Error with hot method: received 403 HTTP response\n",
      "  üîç Method: NEW from r/lifestyle\n",
      "     ‚ùå Error with new method: received 403 HTTP response\n",
      "  ‚úÖ r/lifestyle: 0 posts total\n",
      "\n",
      "  üìç Processing r/travel...\n",
      "  üîç Method: SEARCH from r/travel\n",
      "     Query: 'lifestyle'\n",
      "     ‚úÖ Collected 48 posts using search\n",
      "     Added 48 posts from search: 'lifestyle'\n",
      "  üîç Method: SEARCH from r/travel\n",
      "     Query: 'travel'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'travel'\n",
      "  üîç Method: SEARCH from r/travel\n",
      "     Query: 'food'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'food'\n",
      "  ‚úÖ r/travel: 248 posts total\n",
      "\n",
      "  üìç Processing r/food...\n",
      "  üîç Method: SEARCH from r/food\n",
      "     Query: 'lifestyle'\n",
      "     ‚úÖ Collected 2 posts using search\n",
      "     Added 2 posts from search: 'lifestyle'\n",
      "  üîç Method: SEARCH from r/food\n",
      "     Query: 'travel'\n",
      "     ‚úÖ Collected 41 posts using search\n",
      "     Added 41 posts from search: 'travel'\n",
      "  üîç Method: SEARCH from r/food\n",
      "     Query: 'food'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'food'\n",
      "  üîç Method: HOT from r/food\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/food: 223 posts total\n",
      "\n",
      "  üìç Processing r/cooking...\n",
      "  üîç Method: SEARCH from r/cooking\n",
      "     Query: 'lifestyle'\n",
      "     ‚úÖ Collected 31 posts using search\n",
      "     Added 31 posts from search: 'lifestyle'\n",
      "  üîç Method: SEARCH from r/cooking\n",
      "     Query: 'travel'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'travel'\n",
      "  üîç Method: SEARCH from r/cooking\n",
      "     Query: 'food'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'food'\n",
      "  ‚úÖ r/cooking: 231 posts total\n",
      "\n",
      "  üìç Processing r/DIY...\n",
      "  üîç Method: SEARCH from r/DIY\n",
      "     Query: 'lifestyle'\n",
      "     ‚úÖ Collected 7 posts using search\n",
      "     Added 7 posts from search: 'lifestyle'\n",
      "  üîç Method: SEARCH from r/DIY\n",
      "     Query: 'travel'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'travel'\n",
      "  üîç Method: SEARCH from r/DIY\n",
      "     Query: 'food'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'food'\n",
      "  ‚úÖ r/DIY: 207 posts total\n",
      "\n",
      "  üìç Processing r/hobbies...\n",
      "  üîç Method: SEARCH from r/hobbies\n",
      "     Query: 'lifestyle'\n",
      "     ‚úÖ Collected 12 posts using search\n",
      "     Added 12 posts from search: 'lifestyle'\n",
      "  üîç Method: SEARCH from r/hobbies\n",
      "     Query: 'travel'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 71 posts using search\n",
      "     Added 71 posts from search: 'travel'\n",
      "  üîç Method: SEARCH from r/hobbies\n",
      "     Query: 'food'\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 62 posts using search\n",
      "     Added 62 posts from search: 'food'\n",
      "  üîç Method: HOT from r/hobbies\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  ‚úÖ r/hobbies: 225 posts total\n",
      "\n",
      "  üìç Processing r/fashion...\n",
      "  üîç Method: SEARCH from r/fashion\n",
      "     Query: 'lifestyle'\n",
      "     ‚úÖ Collected 2 posts using search\n",
      "     Added 2 posts from search: 'lifestyle'\n",
      "  üîç Method: SEARCH from r/fashion\n",
      "     Query: 'travel'\n",
      "     ‚úÖ Collected 15 posts using search\n",
      "     Added 15 posts from search: 'travel'\n",
      "  üîç Method: SEARCH from r/fashion\n",
      "     Query: 'food'\n",
      "     ‚úÖ Collected 13 posts using search\n",
      "     Added 13 posts from search: 'food'\n",
      "  üîç Method: HOT from r/fashion\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using hot\n",
      "     Added 80 posts from hot\n",
      "  üîç Method: NEW from r/fashion\n",
      "     Processed 50 posts...\n",
      "     ‚úÖ Collected 80 posts using new\n",
      "     Added 80 posts from new\n",
      "  ‚úÖ r/fashion: 190 posts total\n",
      "\n",
      "  üìç Processing r/gardening...\n",
      "  üîç Method: SEARCH from r/gardening\n",
      "     Query: 'lifestyle'\n",
      "     ‚úÖ Collected 15 posts using search\n",
      "     Added 15 posts from search: 'lifestyle'\n",
      "  üîç Method: SEARCH from r/gardening\n",
      "     Query: 'travel'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'travel'\n",
      "  üîç Method: SEARCH from r/gardening\n",
      "     Query: 'food'\n",
      "     Processed 50 posts...\n",
      "     Processed 100 posts...\n",
      "     ‚úÖ Collected 100 posts using search\n",
      "     Added 100 posts from search: 'food'\n",
      "  ‚úÖ r/gardening: 215 posts total\n",
      "\n",
      "  üìä Removed 99 duplicates\n",
      "  ‚úÖ Lifestyle_Leisure data saved to: C:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\data\\raw\\social_Media_data\\reddit\\Lifestyle_Leisure.json\n",
      "     Posts: 1440\n",
      "     Subreddits: 7\n",
      "\n",
      "  üìä LIFESTYLE LEISURE SUMMARY:\n",
      "     Total posts: 1440\n",
      "     Posts with text: 773\n",
      "     Text ratio: 53.7%\n",
      "     Subreddits: 7\n",
      "     Average score: 885.3\n",
      "\n",
      "  ‚è≥ Waiting 30 seconds before next category...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 389\u001b[0m\n\u001b[0;32m    387\u001b[0m         wait_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m  \u001b[38;5;66;03m# 30 seconds between categories\u001b[39;00m\n\u001b[0;32m    388\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  ‚è≥ Waiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwait_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds before next category...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 389\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;66;03m# Save collection summary (also updated path)\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"xeV3sLrkl4xGyM-ScCBkSA\",\n",
    "    client_secret=\"sGrJQoAWabMp3hVcAXTxocV3gkCXoQ\",\n",
    "    user_agent=\"HeadlineHive\"\n",
    ")\n",
    "\n",
    "# Comprehensive category configuration\n",
    "CATEGORIES_CONFIG = {\n",
    "    'Politics': {\n",
    "        'subreddits': ['politics', 'PoliticalDiscussion', 'Ask_Politics', 'neutralpolitics', 'worldpolitics'],\n",
    "        'search_queries': ['politics', 'election', 'government', 'congress', 'senate', 'political'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Economy_Business_Finance': {\n",
    "        'subreddits': ['Economics', 'business', 'stocks', 'investing', 'personalfinance', 'economy', 'finance'],\n",
    "        'search_queries': ['economy', 'business', 'finance', 'stock market', 'investment', 'banking', 'GDP'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Science_Technology': {\n",
    "        'subreddits': ['technology', 'science', 'gadgets', 'tech', 'MachineLearning', 'artificial', 'programming', 'computers'],\n",
    "        'search_queries': ['technology', 'science', 'innovation', 'research', 'AI', 'machine learning', 'tech'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Health': {\n",
    "        'subreddits': ['Health', 'medicine', 'medical', 'healthcare', 'nutrition', 'fitness', 'mentalhealth'],\n",
    "        'search_queries': ['health', 'medicine', 'medical', 'healthcare', 'disease', 'treatment', 'wellness'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Sport': {\n",
    "        'subreddits': ['sports', 'nfl', 'nba', 'soccer', 'baseball', 'hockey', 'olympics', 'football'],\n",
    "        'search_queries': ['sports', 'football', 'basketball', 'soccer', 'baseball', 'olympics', 'athletics'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Arts_Culture_Entertainment_Media': {\n",
    "        'subreddits': ['movies', 'music', 'art', 'books', 'television', 'entertainment', 'culture', 'media'],\n",
    "        'search_queries': ['movies', 'music', 'art', 'books', 'entertainment', 'culture', 'media', 'celebrity'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Society': {\n",
    "        'subreddits': ['sociology', 'society', 'social', 'community', 'TrueReddit', 'news'],\n",
    "        'search_queries': ['society', 'social', 'community', 'culture', 'demographics', 'population'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Human_Interest': {\n",
    "        'subreddits': ['UpliftingNews', 'MadeMeSmile', 'happy', 'wholesome', 'HumansBeingBros', 'todayilearned'],\n",
    "        'search_queries': ['human interest', 'inspiring', 'heartwarming', 'amazing', 'incredible', 'touching'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Crime_Law_Justice': {\n",
    "        'subreddits': ['law', 'legaladvice', 'UnresolvedMysteries', 'TrueCrime', 'justice', 'police'],\n",
    "        'search_queries': ['crime', 'law', 'justice', 'court', 'legal', 'police', 'arrest', 'trial'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Conflict_War_Peace': {\n",
    "        'subreddits': ['worldnews', 'geopolitics', 'Military', 'CredibleDefense', 'conflict'],\n",
    "        'search_queries': ['war', 'conflict', 'military', 'peace', 'international', 'defense', 'security'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Disaster_Accident_Emergency': {\n",
    "        'subreddits': ['worldnews', 'news', 'CatastrophicFailure', 'emergencyservices'],\n",
    "        'search_queries': ['disaster', 'emergency', 'accident', 'earthquake', 'flood', 'fire', 'rescue'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Environment': {\n",
    "        'subreddits': ['environment', 'climatechange', 'sustainability', 'nature', 'ecology', 'green'],\n",
    "        'search_queries': ['environment', 'climate change', 'sustainability', 'ecology', 'pollution', 'conservation'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Education': {\n",
    "        'subreddits': ['education', 'teachers', 'college', 'university', 'students', 'Academia'],\n",
    "        'search_queries': ['education', 'school', 'university', 'college', 'learning', 'academic', 'student'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Labour': {\n",
    "        'subreddits': ['jobs', 'antiwork', 'WorkReform', 'labor', 'union', 'employment'],\n",
    "        'search_queries': ['jobs', 'employment', 'labor', 'union', 'workplace', 'workers', 'career'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Lifestyle_Leisure': {\n",
    "        'subreddits': ['lifestyle', 'travel', 'food', 'cooking', 'DIY', 'hobbies', 'fashion', 'gardening'],\n",
    "        'search_queries': ['lifestyle', 'travel', 'food', 'cooking', 'hobbies', 'leisure', 'fashion'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Weather': {\n",
    "        'subreddits': ['weather', 'climate', 'meteorology', 'TropicalWeather'],\n",
    "        'search_queries': ['weather', 'climate', 'temperature', 'storm', 'hurricane', 'meteorology'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    },\n",
    "    'Religion': {\n",
    "        'subreddits': ['religion', 'Christianity', 'islam', 'Judaism', 'Buddhism', 'Hinduism', 'atheism'],\n",
    "        'search_queries': ['religion', 'faith', 'spiritual', 'church', 'religious', 'belief', 'worship'],\n",
    "        'methods': ['search', 'hot', 'new']\n",
    "    }\n",
    "}\n",
    "\n",
    "def collect_reddit_posts(subreddit_name, search_query=None, limit=300, method=\"search\", topic_category=\"general\"):\n",
    "    \"\"\"\n",
    "    Collect Reddit data with proper text extraction\n",
    "    \"\"\"\n",
    "    print(f\"  üîç Method: {method.upper()} from r/{subreddit_name}\")\n",
    "    if search_query:\n",
    "        print(f\"     Query: '{search_query}'\")\n",
    "    \n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "    \n",
    "    try:\n",
    "        # Choose method\n",
    "        if method == \"search\" and search_query:\n",
    "            submissions = subreddit.search(search_query, limit=limit, time_filter='year')\n",
    "        elif method == \"hot\":\n",
    "            submissions = subreddit.hot(limit=limit)\n",
    "        elif method == \"new\":\n",
    "            submissions = subreddit.new(limit=limit)\n",
    "        elif method == \"top\":\n",
    "            submissions = subreddit.top(limit=limit, time_filter='month')\n",
    "        else:\n",
    "            print(f\"     Unknown method: {method}\")\n",
    "            return []\n",
    "        \n",
    "        for i, submission in enumerate(submissions):\n",
    "            try:\n",
    "                # Extract text content properly\n",
    "                text_content = \"\"\n",
    "                \n",
    "                # For self posts (text posts)\n",
    "                if submission.is_self and submission.selftext:\n",
    "                    text_content = submission.selftext.strip()\n",
    "                \n",
    "                # For link posts, we might want the URL or title as content\n",
    "                elif not submission.is_self:\n",
    "                    text_content = f\"Link post: {submission.url}\"\n",
    "                \n",
    "                # Sometimes selftext exists but is empty/whitespace\n",
    "                if not text_content and hasattr(submission, 'selftext'):\n",
    "                    if submission.selftext and submission.selftext.strip():\n",
    "                        text_content = submission.selftext.strip()\n",
    "                \n",
    "                # If still no content, use title as fallback\n",
    "                if not text_content:\n",
    "                    text_content = f\"[Title only] {submission.title}\"\n",
    "                \n",
    "                post_data = {\n",
    "                    'id': submission.id,\n",
    "                    'title': submission.title if submission.title else \"No title\",\n",
    "                    'text': text_content,\n",
    "                    'author': str(submission.author) if submission.author else '[deleted]',\n",
    "                    'subreddit': submission.subreddit.display_name,\n",
    "                    'score': submission.score if submission.score else 0,\n",
    "                    'upvote_ratio': getattr(submission, 'upvote_ratio', 0),\n",
    "                    'comments': submission.num_comments if submission.num_comments else 0,\n",
    "                    'url': submission.url if submission.url else \"\",\n",
    "                    'is_self': submission.is_self,\n",
    "                    'created': datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'permalink': f\"https://reddit.com{submission.permalink}\",\n",
    "                    'method_collected': method,\n",
    "                    'category': topic_category,\n",
    "                    'flair': submission.link_flair_text if hasattr(submission, 'link_flair_text') else None,\n",
    "                    'nsfw': submission.over_18 if hasattr(submission, 'over_18') else False\n",
    "                }\n",
    "                \n",
    "                posts.append(post_data)\n",
    "                \n",
    "                if (i + 1) % 50 == 0:\n",
    "                    print(f\"     Processed {i + 1} posts...\")\n",
    "                    time.sleep(0.2)  # Rate limiting\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"     Error processing post {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"     ‚úÖ Collected {len(posts)} posts using {method}\")\n",
    "        return posts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"     ‚ùå Error with {method} method: {e}\")\n",
    "        return []\n",
    "\n",
    "def collect_category_data(category_name, category_config, max_posts_per_category=2000):\n",
    "    \"\"\"\n",
    "    Collect data for a specific category\n",
    "    \"\"\"\n",
    "    print(f\"\\nüéØ === COLLECTING {category_name.upper().replace('_', ' ')} DATA ===\")\n",
    "    category_posts = []\n",
    "    \n",
    "    subreddits = category_config['subreddits']\n",
    "    search_queries = category_config['search_queries']\n",
    "    methods = category_config['methods']\n",
    "    \n",
    "    # Limit search queries to avoid too many API calls\n",
    "    limited_queries = search_queries[:3]  # Use only first 3 queries\n",
    "    posts_per_subreddit = max_posts_per_category // len(subreddits)\n",
    "    \n",
    "    for subreddit in subreddits:\n",
    "        print(f\"\\n  üìç Processing r/{subreddit}...\")\n",
    "        subreddit_posts = []\n",
    "        \n",
    "        try:\n",
    "            # Try search method with limited queries\n",
    "            if 'search' in methods and limited_queries:\n",
    "                for query in limited_queries:\n",
    "                    if len(subreddit_posts) >= posts_per_subreddit:\n",
    "                        break\n",
    "                    \n",
    "                    search_posts = collect_reddit_posts(\n",
    "                        subreddit, query, limit=100, method=\"search\", topic_category=category_name\n",
    "                    )\n",
    "                    if search_posts:\n",
    "                        subreddit_posts.extend(search_posts)\n",
    "                        print(f\"     Added {len(search_posts)} posts from search: '{query}'\")\n",
    "                    time.sleep(1)  # Rate limiting between queries\n",
    "            \n",
    "            # Try hot posts method\n",
    "            if 'hot' in methods and len(subreddit_posts) < posts_per_subreddit:\n",
    "                time.sleep(1)\n",
    "                hot_posts = collect_reddit_posts(\n",
    "                    subreddit, None, limit=80, method=\"hot\", topic_category=category_name\n",
    "                )\n",
    "                if hot_posts:\n",
    "                    subreddit_posts.extend(hot_posts)\n",
    "                    print(f\"     Added {len(hot_posts)} posts from hot\")\n",
    "            \n",
    "            # Try new posts method\n",
    "            if 'new' in methods and len(subreddit_posts) < posts_per_subreddit:\n",
    "                time.sleep(1)\n",
    "                new_posts = collect_reddit_posts(\n",
    "                    subreddit, None, limit=80, method=\"new\", topic_category=category_name\n",
    "                )\n",
    "                if new_posts:\n",
    "                    subreddit_posts.extend(new_posts)\n",
    "                    print(f\"     Added {len(new_posts)} posts from new\")\n",
    "            \n",
    "            category_posts.extend(subreddit_posts)\n",
    "            print(f\"  ‚úÖ r/{subreddit}: {len(subreddit_posts)} posts total\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error with r/{subreddit}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        time.sleep(3)  # Rate limiting between subreddits\n",
    "    \n",
    "    return category_posts\n",
    "\n",
    "def remove_duplicates(all_posts):\n",
    "    \"\"\"Remove duplicate posts based on ID\"\"\"\n",
    "    seen_ids = set()\n",
    "    unique_posts = []\n",
    "    \n",
    "    for post in all_posts:\n",
    "        if post['id'] not in seen_ids:\n",
    "            unique_posts.append(post)\n",
    "            seen_ids.add(post['id'])\n",
    "    \n",
    "    return unique_posts\n",
    "\n",
    "def save_category_json(posts, category_name, output_dir=r\"C:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\data\\raw\\social_Media_data\\reddit\"):\n",
    "    \"\"\"Save category posts to separate JSON file\"\"\"\n",
    "    if not posts:\n",
    "        print(f\"  ‚ùå No posts to save for {category_name}!\")\n",
    "        return False\n",
    "    \n",
    "    # Create output directory if it doesn't exist (including parent directories)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    try:\n",
    "        # Create comprehensive data structure for this category\n",
    "        output_data = {\n",
    "            \"metadata\": {\n",
    "                \"category\": category_name.replace('_', ' '),\n",
    "                \"total_posts\": len(posts),\n",
    "                \"collection_date\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"methods_used\": list(set([post.get('method_collected', 'unknown') for post in posts])),\n",
    "                \"subreddits\": list(set([post['subreddit'] for post in posts])),\n",
    "                \"date_range\": {\n",
    "                    \"earliest\": min([post['created'] for post in posts]),\n",
    "                    \"latest\": max([post['created'] for post in posts])\n",
    "                },\n",
    "                \"text_statistics\": {\n",
    "                    \"posts_with_text\": len([p for p in posts if p['text'] and not p['text'].startswith('[Title only]') and not p['text'].startswith('Link post:')]),\n",
    "                    \"average_score\": sum([p['score'] for p in posts]) / len(posts),\n",
    "                    \"total_comments\": sum([p['comments'] for p in posts])\n",
    "                }\n",
    "            },\n",
    "            \"posts\": posts\n",
    "        }\n",
    "        \n",
    "        # Save to category-specific file WITHOUT timestamp\n",
    "        filename = os.path.join(output_dir, f\"{category_name}.json\")\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"  ‚úÖ {category_name} data saved to: {filename}\")\n",
    "        print(f\"     Posts: {len(posts)}\")\n",
    "        print(f\"     Subreddits: {len(output_data['metadata']['subreddits'])}\")\n",
    "        return filename\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error saving {category_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def display_category_summary(posts, category_name):\n",
    "    \"\"\"Display summary for a category\"\"\"\n",
    "    if not posts:\n",
    "        return\n",
    "    \n",
    "    posts_with_text = [p for p in posts if p['text'] and not p['text'].startswith('[Title only]') and not p['text'].startswith('Link post:')]\n",
    "    \n",
    "    print(f\"\\n  üìä {category_name.replace('_', ' ').upper()} SUMMARY:\")\n",
    "    print(f\"     Total posts: {len(posts)}\")\n",
    "    print(f\"     Posts with text: {len(posts_with_text)}\")\n",
    "    print(f\"     Text ratio: {len(posts_with_text)/len(posts)*100:.1f}%\")\n",
    "    print(f\"     Subreddits: {len(set([p['subreddit'] for p in posts]))}\")\n",
    "    print(f\"     Average score: {sum([p['score'] for p in posts]) / len(posts):.1f}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT COMPREHENSIVE DATA COLLECTION\")\n",
    "    print(\"17 Categories - Separate JSON Files\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Test Reddit connection\n",
    "    try:\n",
    "        print(\"üîå Testing Reddit connection...\")\n",
    "        test_sub = reddit.subreddit('python')\n",
    "        print(f\"‚úÖ Connected! Test subreddit: {test_sub.display_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Reddit connection failed: {e}\")\n",
    "        print(\"Check your Reddit API credentials!\")\n",
    "        exit()\n",
    "    \n",
    "    # Create summary for all categories\n",
    "    collection_summary = {\n",
    "        \"collection_date\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"categories_collected\": [],\n",
    "        \"total_posts\": 0,\n",
    "        \"files_created\": []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìã CATEGORIES TO COLLECT ({len(CATEGORIES_CONFIG)}):\")\n",
    "    for i, category in enumerate(CATEGORIES_CONFIG.keys(), 1):\n",
    "        print(f\"  {i:2d}. {category.replace('_', ' ')}\")\n",
    "    \n",
    "    # Collect data for each category\n",
    "    for i, (category_name, category_config) in enumerate(CATEGORIES_CONFIG.items(), 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CATEGORY {i}/{len(CATEGORIES_CONFIG)}: {category_name.replace('_', ' ').upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            # Collect category data\n",
    "            category_posts = collect_category_data(category_name, category_config, max_posts_per_category=1500)\n",
    "            \n",
    "            if category_posts:\n",
    "                # Remove duplicates\n",
    "                unique_posts = remove_duplicates(category_posts)\n",
    "                print(f\"\\n  üìä Removed {len(category_posts) - len(unique_posts)} duplicates\")\n",
    "                \n",
    "                # Save to separate JSON file\n",
    "                filename = save_category_json(unique_posts, category_name)\n",
    "                \n",
    "                if filename:\n",
    "                    # Update summary\n",
    "                    collection_summary[\"categories_collected\"].append(category_name)\n",
    "                    collection_summary[\"total_posts\"] += len(unique_posts)\n",
    "                    collection_summary[\"files_created\"].append(filename)\n",
    "                    \n",
    "                    # Display summary\n",
    "                    display_category_summary(unique_posts, category_name)\n",
    "                \n",
    "            else:\n",
    "                print(f\"  ‚ùå No data collected for {category_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error processing {category_name}: {e}\")\n",
    "        \n",
    "        # Wait between categories to be respectful\n",
    "        if i < len(CATEGORIES_CONFIG):\n",
    "            wait_time = 30  # 30 seconds between categories\n",
    "            print(f\"\\n  ‚è≥ Waiting {wait_time} seconds before next category...\")\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    # Save collection summary (also updated path)\n",
    "    try:\n",
    "        summary_file = fr\"C:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\data\\raw\\social_Media_data\\reddit\\collection_summary.json\"\n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(collection_summary, f, indent=2, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving summary: {e}\")\n",
    "    \n",
    "    # Final results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üéâ COLLECTION COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Categories processed: {len(collection_summary['categories_collected'])}/{len(CATEGORIES_CONFIG)}\")\n",
    "    print(f\"Total posts collected: {collection_summary['total_posts']:,}\")\n",
    "    print(f\"Files created: {len(collection_summary['files_created'])}\")\n",
    "    print(f\"\\nüìÅ All files saved in: C:\\\\Users\\\\yagne\\\\OneDrive\\\\Desktop\\\\project s\\\\Co-projects\\\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\\\data\\\\raw\\\\social_Media_data\\\\reddit\\\\\")\n",
    "    print(f\"üìä Collection summary: {summary_file}\")\n",
    "    \n",
    "    if collection_summary['categories_collected']:\n",
    "        print(f\"\\n‚úÖ Successfully collected data for:\")\n",
    "        for category in collection_summary['categories_collected']:\n",
    "            print(f\"   ‚Ä¢ {category.replace('_', ' ')}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Data collection completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "960cf762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ REDDIT 3-YEAR HISTORICAL DATA COLLECTION\n",
      "Enhanced for comprehensive historical coverage\n",
      "================================================================================\n",
      "üîå Testing Reddit connection...\n",
      "‚úÖ Connected! Test subreddit: python\n",
      "\n",
      "üìã CATEGORIES TO COLLECT (3-YEAR HISTORICAL DATA):\n",
      "   1. Politics\n",
      "   2. Economy Business Finance\n",
      "   3. Science Technology\n",
      "   4. Health\n",
      "   5. Sport\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 1/5: POLITICS\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING 3-YEAR POLITICS DATA ===\n",
      "\n",
      "  üìç Processing r/politics (3-year historical data)...\n",
      "  üîç Method: SEARCH from r/politics\n",
      "     Query: 'politics'\n",
      "     üìÖ Collecting from time period: month\n",
      "        Processed 100 posts from month...\n",
      "        Processed 200 posts from month...\n",
      "        ‚úÖ Collected 248 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 247 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 100 posts from all...\n",
      "        ‚úÖ Collected 19 posts from all\n",
      "     üìä Total collected: 514 posts\n",
      "     Added 514 posts from search: 'politics'\n",
      "  üîç Method: SEARCH from r/politics\n",
      "     Query: 'election'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 85 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 245 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 32 posts from all\n",
      "     üìä Total collected: 362 posts\n",
      "     Added 362 posts from search: 'election'\n",
      "  ‚úÖ r/politics: 876 posts total\n",
      "\n",
      "  üìç Processing r/PoliticalDiscussion (3-year historical data)...\n",
      "  üîç Method: SEARCH from r/PoliticalDiscussion\n",
      "     Query: 'politics'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 35 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 234 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 100 posts from all...\n",
      "        Processed 200 posts from all...\n",
      "        ‚úÖ Collected 39 posts from all\n",
      "     üìä Total collected: 308 posts\n",
      "     Added 308 posts from search: 'politics'\n",
      "  üîç Method: SEARCH from r/PoliticalDiscussion\n",
      "     Query: 'election'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 24 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 216 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 55 posts from all\n",
      "     üìä Total collected: 295 posts\n",
      "     Added 295 posts from search: 'election'\n",
      "  üîç Method: SEARCH from r/PoliticalDiscussion\n",
      "     Query: 'government'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 25 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 227 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 47 posts from all\n",
      "     üìä Total collected: 299 posts\n",
      "     Added 299 posts from search: 'government'\n",
      "  ‚úÖ r/PoliticalDiscussion: 902 posts total\n",
      "\n",
      "  üìç Processing r/Ask_Politics (3-year historical data)...\n",
      "  üîç Method: SEARCH from r/Ask_Politics\n",
      "     Query: 'politics'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 1 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 62 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 7 posts from all\n",
      "     üìä Total collected: 70 posts\n",
      "     Added 70 posts from search: 'politics'\n",
      "  üîç Method: SEARCH from r/Ask_Politics\n",
      "     Query: 'election'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 1 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 67 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 15 posts from all\n",
      "     üìä Total collected: 83 posts\n",
      "     Added 83 posts from search: 'election'\n",
      "  üîç Method: SEARCH from r/Ask_Politics\n",
      "     Query: 'government'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 0 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 25 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 6 posts from all\n",
      "     üìä Total collected: 31 posts\n",
      "     Added 31 posts from search: 'government'\n",
      "  üîç Method: SEARCH from r/Ask_Politics\n",
      "     Query: 'congress'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 0 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 17 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 11 posts from all\n",
      "     üìä Total collected: 28 posts\n",
      "     Added 28 posts from search: 'congress'\n",
      "  üîç Method: SEARCH from r/Ask_Politics\n",
      "     Query: 'senate'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 0 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 18 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 200 posts from all...\n",
      "        ‚úÖ Collected 10 posts from all\n",
      "     üìä Total collected: 28 posts\n",
      "     Added 28 posts from search: 'senate'\n",
      "  üîç Method: TOP from r/Ask_Politics\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 1 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        Processed 300 posts from year...\n",
      "        ‚úÖ Collected 301 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 50 posts from all\n",
      "     üìä Total collected: 352 posts\n",
      "     Added 352 posts from top posts\n",
      "  ‚úÖ r/Ask_Politics: 592 posts total\n",
      "\n",
      "  üìç Processing r/neutralpolitics (3-year historical data)...\n",
      "  üîç Method: SEARCH from r/neutralpolitics\n",
      "     Query: 'politics'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 6 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 59 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 6 posts from all\n",
      "     üìä Total collected: 71 posts\n",
      "     Added 71 posts from search: 'politics'\n",
      "  üîç Method: SEARCH from r/neutralpolitics\n",
      "     Query: 'election'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 0 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 34 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 100 posts from all...\n",
      "        ‚úÖ Collected 17 posts from all\n",
      "     üìä Total collected: 51 posts\n",
      "     Added 51 posts from search: 'election'\n",
      "  üîç Method: SEARCH from r/neutralpolitics\n",
      "     Query: 'government'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 3 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 33 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 13 posts from all\n",
      "     üìä Total collected: 49 posts\n",
      "     Added 49 posts from search: 'government'\n",
      "  üîç Method: SEARCH from r/neutralpolitics\n",
      "     Query: 'congress'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 0 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 20 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 100 posts from all...\n",
      "        ‚úÖ Collected 21 posts from all\n",
      "     üìä Total collected: 41 posts\n",
      "     Added 41 posts from search: 'congress'\n",
      "  üîç Method: SEARCH from r/neutralpolitics\n",
      "     Query: 'senate'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 0 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 18 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 14 posts from all\n",
      "     üìä Total collected: 32 posts\n",
      "     Added 32 posts from search: 'senate'\n",
      "  üîç Method: TOP from r/neutralpolitics\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 9 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        ‚úÖ Collected 119 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 300 posts from all...\n",
      "        Processed 800 posts from all...\n",
      "        ‚úÖ Collected 66 posts from all\n",
      "     üìä Total collected: 194 posts\n",
      "     Added 194 posts from top posts\n",
      "  ‚úÖ r/neutralpolitics: 438 posts total\n",
      "\n",
      "  üìç Processing r/worldpolitics (3-year historical data)...\n",
      "  üîç Method: SEARCH from r/worldpolitics\n",
      "     Query: 'politics'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 0 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 0 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 0 posts from all\n",
      "     üìä Total collected: 0 posts\n",
      "  üîç Method: SEARCH from r/worldpolitics\n",
      "     Query: 'election'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 0 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 0 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 0 posts from all\n",
      "     üìä Total collected: 0 posts\n",
      "  üîç Method: SEARCH from r/worldpolitics\n",
      "     Query: 'government'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 0 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 0 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 0 posts from all\n",
      "     üìä Total collected: 0 posts\n",
      "  üîç Method: SEARCH from r/worldpolitics\n",
      "     Query: 'congress'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 0 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 0 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 0 posts from all\n",
      "     üìä Total collected: 0 posts\n",
      "  üîç Method: SEARCH from r/worldpolitics\n",
      "     Query: 'senate'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 0 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 0 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 0 posts from all\n",
      "     üìä Total collected: 0 posts\n",
      "  üîç Method: TOP from r/worldpolitics\n",
      "     üìÖ Collecting from time period: month\n",
      "        Processed 100 posts from month...\n",
      "        Processed 200 posts from month...\n",
      "        ‚úÖ Collected 264 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        Processed 300 posts from year...\n",
      "        Processed 400 posts from year...\n",
      "        Processed 500 posts from year...\n",
      "        Processed 600 posts from year...\n",
      "        Processed 700 posts from year...\n",
      "        Processed 800 posts from year...\n",
      "        ‚úÖ Collected 800 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 900 posts from all...\n",
      "        ‚úÖ Collected 11 posts from all\n",
      "     üìä Total collected: 1075 posts\n",
      "     Added 1075 posts from top posts\n",
      "  ‚úÖ r/worldpolitics: 1075 posts total\n",
      "\n",
      "  üìç Processing r/Conservative (3-year historical data)...\n",
      "  üîç Method: SEARCH from r/Conservative\n",
      "     Query: 'politics'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 62 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 236 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 100 posts from all...\n",
      "        Processed 200 posts from all...\n",
      "        ‚úÖ Collected 72 posts from all\n",
      "     üìä Total collected: 370 posts\n",
      "     Added 370 posts from search: 'politics'\n",
      "  üîç Method: SEARCH from r/Conservative\n",
      "     Query: 'election'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 34 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 234 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 81 posts from all\n",
      "     üìä Total collected: 349 posts\n",
      "     Added 349 posts from search: 'election'\n",
      "  üîç Method: SEARCH from r/Conservative\n",
      "     Query: 'government'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 35 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 226 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 92 posts from all\n",
      "     üìä Total collected: 353 posts\n",
      "     Added 353 posts from search: 'government'\n",
      "  ‚úÖ r/Conservative: 1072 posts total\n",
      "\n",
      "  üìç Processing r/Liberal (3-year historical data)...\n",
      "  üîç Method: SEARCH from r/Liberal\n",
      "     Query: 'politics'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 4 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        ‚úÖ Collected 127 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 100 posts from all...\n",
      "        ‚úÖ Collected 113 posts from all\n",
      "     üìä Total collected: 244 posts\n",
      "     Added 244 posts from search: 'politics'\n",
      "  üîç Method: SEARCH from r/Liberal\n",
      "     Query: 'election'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 1 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        ‚úÖ Collected 160 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 200 posts from all...\n",
      "        ‚úÖ Collected 92 posts from all\n",
      "     üìä Total collected: 253 posts\n",
      "     Added 253 posts from search: 'election'\n",
      "  üîç Method: SEARCH from r/Liberal\n",
      "     Query: 'government'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 3 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        ‚úÖ Collected 107 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 100 posts from all...\n",
      "        ‚úÖ Collected 102 posts from all\n",
      "     üìä Total collected: 212 posts\n",
      "     Added 212 posts from search: 'government'\n",
      "  üîç Method: SEARCH from r/Liberal\n",
      "     Query: 'congress'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 2 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 38 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 100 posts from all...\n",
      "        ‚úÖ Collected 62 posts from all\n",
      "     üìä Total collected: 102 posts\n",
      "     Added 102 posts from search: 'congress'\n",
      "  ‚úÖ r/Liberal: 811 posts total\n",
      "\n",
      "  üìç Processing r/moderatepolitics (3-year historical data)...\n",
      "  üîç Method: SEARCH from r/moderatepolitics\n",
      "     Query: 'politics'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 10 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        ‚úÖ Collected 138 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 200 posts from all...\n",
      "        ‚úÖ Collected 122 posts from all\n",
      "     üìä Total collected: 270 posts\n",
      "     Added 270 posts from search: 'politics'\n",
      "  üîç Method: SEARCH from r/moderatepolitics\n",
      "     Query: 'election'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 4 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        ‚úÖ Collected 141 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 200 posts from all...\n",
      "        ‚úÖ Collected 121 posts from all\n",
      "     üìä Total collected: 266 posts\n",
      "     Added 266 posts from search: 'election'\n",
      "  üîç Method: SEARCH from r/moderatepolitics\n",
      "     Query: 'government'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 6 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 77 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 200 posts from all...\n",
      "        ‚úÖ Collected 94 posts from all\n",
      "     üìä Total collected: 177 posts\n",
      "     Added 177 posts from search: 'government'\n",
      "  üîç Method: SEARCH from r/moderatepolitics\n",
      "     Query: 'congress'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 1 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 40 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 90 posts from all\n",
      "     üìä Total collected: 131 posts\n",
      "     Added 131 posts from search: 'congress'\n",
      "  ‚úÖ r/moderatepolitics: 844 posts total\n",
      "\n",
      "  üìç Processing r/PoliticalHumor (3-year historical data)...\n",
      "  üîç Method: SEARCH from r/PoliticalHumor\n",
      "     Query: 'politics'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 23 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 240 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 100 posts from all...\n",
      "        Processed 200 posts from all...\n",
      "        ‚úÖ Collected 154 posts from all\n",
      "     üìä Total collected: 417 posts\n",
      "     Added 417 posts from search: 'politics'\n",
      "  üîç Method: SEARCH from r/PoliticalHumor\n",
      "     Query: 'election'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 12 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 229 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 100 posts from all...\n",
      "        ‚úÖ Collected 185 posts from all\n",
      "     üìä Total collected: 426 posts\n",
      "     Added 426 posts from search: 'election'\n",
      "  ‚úÖ r/PoliticalHumor: 843 posts total\n",
      "\n",
      "  üìç Processing r/uspolitics (3-year historical data)...\n",
      "  üîç Method: SEARCH from r/uspolitics\n",
      "     Query: 'politics'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 20 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 244 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 92 posts from all\n",
      "     üìä Total collected: 356 posts\n",
      "     Added 356 posts from search: 'politics'\n",
      "  üîç Method: SEARCH from r/uspolitics\n",
      "     Query: 'election'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 24 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 237 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 100 posts from all...\n",
      "        Processed 200 posts from all...\n",
      "        ‚úÖ Collected 91 posts from all\n",
      "     üìä Total collected: 352 posts\n",
      "     Added 352 posts from search: 'election'\n",
      "  üîç Method: SEARCH from r/uspolitics\n",
      "     Query: 'government'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 16 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 241 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 200 posts from all...\n",
      "        ‚úÖ Collected 115 posts from all\n",
      "     üìä Total collected: 372 posts\n",
      "     Added 372 posts from search: 'government'\n",
      "  ‚úÖ r/uspolitics: 1080 posts total\n",
      "\n",
      "  üìä Removed 2012 duplicates\n",
      "  ‚úÖ Politics 3-year data saved to: C:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\data\\raw\\social_Media_data\\reddit\\Politics_3year.json\n",
      "     Posts: 6521\n",
      "     Date range: 2022-08-25 to 2025-08-23\n",
      "     Span: 1094 days\n",
      "\n",
      "  ‚è≥ Waiting 60 seconds before next category...\n",
      "\n",
      "================================================================================\n",
      "CATEGORY 2/5: ECONOMY BUSINESS FINANCE\n",
      "================================================================================\n",
      "\n",
      "üéØ === COLLECTING 3-YEAR ECONOMY BUSINESS FINANCE DATA ===\n",
      "\n",
      "  üìç Processing r/Economics (3-year historical data)...\n",
      "  üîç Method: SEARCH from r/Economics\n",
      "     Query: 'economy'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 68 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 244 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 100 posts from all...\n",
      "        ‚úÖ Collected 117 posts from all\n",
      "     üìä Total collected: 429 posts\n",
      "     Added 429 posts from search: 'economy'\n",
      "  üîç Method: SEARCH from r/Economics\n",
      "     Query: 'business'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 9 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        ‚úÖ Collected 138 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 100 posts from all...\n",
      "        ‚úÖ Collected 114 posts from all\n",
      "     üìä Total collected: 261 posts\n",
      "     Added 261 posts from search: 'business'\n",
      "  üîç Method: SEARCH from r/Economics\n",
      "     Query: 'finance'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 3 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 48 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 100 posts from all...\n",
      "        ‚úÖ Collected 62 posts from all\n",
      "     üìä Total collected: 113 posts\n",
      "     Added 113 posts from search: 'finance'\n",
      "  ‚úÖ r/Economics: 803 posts total\n",
      "\n",
      "  üìç Processing r/business (3-year historical data)...\n",
      "  üîç Method: SEARCH from r/business\n",
      "     Query: 'economy'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 4 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 45 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 100 posts from all...\n",
      "        ‚úÖ Collected 33 posts from all\n",
      "     üìä Total collected: 82 posts\n",
      "     Added 82 posts from search: 'economy'\n",
      "  üîç Method: SEARCH from r/business\n",
      "     Query: 'business'\n",
      "     üìÖ Collecting from time period: month\n",
      "        Processed 100 posts from month...\n",
      "        Processed 200 posts from month...\n",
      "        ‚úÖ Collected 247 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 248 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 200 posts from all...\n",
      "        ‚úÖ Collected 135 posts from all\n",
      "     üìä Total collected: 630 posts\n",
      "     Added 630 posts from search: 'business'\n",
      "  üîç Method: SEARCH from r/business\n",
      "     Query: 'finance'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 13 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        ‚úÖ Collected 149 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 100 posts from all...\n",
      "        ‚úÖ Collected 50 posts from all\n",
      "     üìä Total collected: 212 posts\n",
      "     Added 212 posts from search: 'finance'\n",
      "  ‚úÖ r/business: 924 posts total\n",
      "\n",
      "  üìç Processing r/stocks (3-year historical data)...\n",
      "  üîç Method: SEARCH from r/stocks\n",
      "     Query: 'economy'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 44 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 244 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 100 posts from all...\n",
      "        Processed 200 posts from all...\n",
      "        ‚úÖ Collected 150 posts from all\n",
      "     üìä Total collected: 438 posts\n",
      "     Added 438 posts from search: 'economy'\n",
      "  üîç Method: SEARCH from r/stocks\n",
      "     Query: 'business'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 99 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 247 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 200 posts from all...\n",
      "        ‚úÖ Collected 134 posts from all\n",
      "     üìä Total collected: 480 posts\n",
      "     Added 480 posts from search: 'business'\n",
      "  ‚úÖ r/stocks: 918 posts total\n",
      "\n",
      "  üìç Processing r/investing (3-year historical data)...\n",
      "  üîç Method: SEARCH from r/investing\n",
      "     Query: 'economy'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 34 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 240 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 52 posts from all\n",
      "     üìä Total collected: 326 posts\n",
      "     Added 326 posts from search: 'economy'\n",
      "  üîç Method: SEARCH from r/investing\n",
      "     Query: 'business'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 60 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 244 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 200 posts from all...\n",
      "        ‚úÖ Collected 32 posts from all\n",
      "     üìä Total collected: 336 posts\n",
      "     Added 336 posts from search: 'business'\n",
      "  üîç Method: SEARCH from r/investing\n",
      "     Query: 'finance'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 40 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 246 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 57 posts from all\n",
      "     üìä Total collected: 343 posts\n",
      "     Added 343 posts from search: 'finance'\n",
      "  ‚úÖ r/investing: 1005 posts total\n",
      "\n",
      "  üìç Processing r/personalfinance (3-year historical data)...\n",
      "  üîç Method: SEARCH from r/personalfinance\n",
      "     Query: 'economy'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 28 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 243 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 60 posts from all\n",
      "     üìä Total collected: 331 posts\n",
      "     Added 331 posts from search: 'economy'\n",
      "  üîç Method: SEARCH from r/personalfinance\n",
      "     Query: 'business'\n",
      "     üìÖ Collecting from time period: month\n",
      "        Processed 100 posts from month...\n",
      "        ‚úÖ Collected 166 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 236 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 11 posts from all\n",
      "     üìä Total collected: 413 posts\n",
      "     Added 413 posts from search: 'business'\n",
      "  ‚úÖ r/personalfinance: 744 posts total\n",
      "\n",
      "  üìç Processing r/economy (3-year historical data)...\n",
      "  üîç Method: SEARCH from r/economy\n",
      "     Query: 'economy'\n",
      "     üìÖ Collecting from time period: month\n",
      "        Processed 100 posts from month...\n",
      "        Processed 200 posts from month...\n",
      "        ‚úÖ Collected 250 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 243 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 100 posts from all...\n",
      "        ‚úÖ Collected 110 posts from all\n",
      "     üìä Total collected: 603 posts\n",
      "     Added 603 posts from search: 'economy'\n",
      "  üîç Method: SEARCH from r/economy\n",
      "     Query: 'business'\n",
      "     üìÖ Collecting from time period: month\n",
      "        Processed 100 posts from month...\n",
      "        ‚úÖ Collected 121 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 248 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 100 posts from all...\n",
      "        ‚úÖ Collected 148 posts from all\n",
      "     üìä Total collected: 517 posts\n",
      "     Added 517 posts from search: 'business'\n",
      "  ‚úÖ r/economy: 1120 posts total\n",
      "\n",
      "  üìç Processing r/finance (3-year historical data)...\n",
      "  üîç Method: SEARCH from r/finance\n",
      "     Query: 'economy'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 2 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 8 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 17 posts from all\n",
      "     üìä Total collected: 27 posts\n",
      "     Added 27 posts from search: 'economy'\n",
      "  üîç Method: SEARCH from r/finance\n",
      "     Query: 'business'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 0 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 1 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 8 posts from all\n",
      "     üìä Total collected: 9 posts\n",
      "     Added 9 posts from search: 'business'\n",
      "  üîç Method: SEARCH from r/finance\n",
      "     Query: 'finance'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 23 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        Processed 200 posts from year...\n",
      "        ‚úÖ Collected 224 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 100 posts from all...\n",
      "        ‚úÖ Collected 72 posts from all\n",
      "     üìä Total collected: 319 posts\n",
      "     Added 319 posts from search: 'finance'\n",
      "  üîç Method: SEARCH from r/finance\n",
      "     Query: 'stock market'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 2 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 5 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 9 posts from all\n",
      "     üìä Total collected: 16 posts\n",
      "     Added 16 posts from search: 'stock market'\n",
      "  üîç Method: SEARCH from r/finance\n",
      "     Query: 'investment'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 0 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 3 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 15 posts from all\n",
      "     üìä Total collected: 18 posts\n",
      "     Added 18 posts from search: 'investment'\n",
      "  üîç Method: TOP from r/finance\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 22 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        Processed 100 posts from year...\n",
      "        ‚úÖ Collected 129 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        Processed 100 posts from all...\n",
      "        ‚úÖ Collected 248 posts from all\n",
      "     üìä Total collected: 399 posts\n",
      "     Added 399 posts from top posts\n",
      "  ‚úÖ r/finance: 788 posts total\n",
      "\n",
      "  üìç Processing r/SecurityAnalysis (3-year historical data)...\n",
      "  üîç Method: SEARCH from r/SecurityAnalysis\n",
      "     Query: 'economy'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 0 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 8 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 21 posts from all\n",
      "     üìä Total collected: 29 posts\n",
      "     Added 29 posts from search: 'economy'\n",
      "  üîç Method: SEARCH from r/SecurityAnalysis\n",
      "     Query: 'business'\n",
      "     üìÖ Collecting from time period: month\n",
      "        ‚úÖ Collected 0 posts from month\n",
      "     üìÖ Collecting from time period: year\n",
      "        ‚úÖ Collected 24 posts from year\n",
      "     üìÖ Collecting from time period: all\n",
      "        ‚úÖ Collected 27 posts from all\n",
      "     üìä Total collected: 51 posts\n",
      "     Added 51 posts from search: 'business'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 347\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;66;03m# Collect 3-year category data\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     category_posts \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_comprehensive_category_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategory_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_posts_per_category\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8000\u001b[39;49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m category_posts:\n\u001b[0;32m    352\u001b[0m         \u001b[38;5;66;03m# Enhanced duplicate removal\u001b[39;00m\n\u001b[0;32m    353\u001b[0m         unique_posts \u001b[38;5;241m=\u001b[39m remove_duplicates_enhanced(category_posts)\n",
      "Cell \u001b[1;32mIn[2], line 182\u001b[0m, in \u001b[0;36mcollect_comprehensive_category_data\u001b[1;34m(category_name, category_config, max_posts_per_category)\u001b[0m\n\u001b[0;32m    180\u001b[0m             subreddit_posts\u001b[38;5;241m.\u001b[39mextend(search_posts)\n\u001b[0;32m    181\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m     Added \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(search_posts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m posts from search: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 182\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Rate limiting between queries\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# Method 2: Top posts across time periods\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m methods \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subreddit_posts) \u001b[38;5;241m<\u001b[39m posts_per_subreddit:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"xeV3sLrkl4xGyM-ScCBkSA\",\n",
    "    client_secret=\"sGrJQoAWabMp3hVcAXTxocV3gkCXoQ\",\n",
    "    user_agent=\"HeadlineHive\"\n",
    ")\n",
    "\n",
    "# Enhanced category configuration with more subreddits for better historical coverage\n",
    "CATEGORIES_CONFIG = {\n",
    "    'Politics': {\n",
    "        'subreddits': ['politics', 'PoliticalDiscussion', 'Ask_Politics', 'neutralpolitics', 'worldpolitics', \n",
    "                      'Conservative', 'Liberal', 'moderatepolitics', 'PoliticalHumor', 'uspolitics'],\n",
    "        'search_queries': ['politics', 'election', 'government', 'congress', 'senate', 'political', 'vote', 'policy'],\n",
    "        'methods': ['search', 'top']\n",
    "    },\n",
    "    'Economy_Business_Finance': {\n",
    "        'subreddits': ['Economics', 'business', 'stocks', 'investing', 'personalfinance', 'economy', 'finance',\n",
    "                      'SecurityAnalysis', 'ValueInvesting', 'financialindependence', 'entrepreneur'],\n",
    "        'search_queries': ['economy', 'business', 'finance', 'stock market', 'investment', 'banking', 'GDP', 'recession'],\n",
    "        'methods': ['search', 'top']\n",
    "    },\n",
    "    'Science_Technology': {\n",
    "        'subreddits': ['technology', 'science', 'gadgets', 'tech', 'MachineLearning', 'artificial', 'programming', \n",
    "                      'computers', 'singularity', 'Futurology', 'innovation', 'Physics', 'Biology'],\n",
    "        'search_queries': ['technology', 'science', 'innovation', 'research', 'AI', 'machine learning', 'tech', 'breakthrough'],\n",
    "        'methods': ['search', 'top']\n",
    "    },\n",
    "    'Health': {\n",
    "        'subreddits': ['Health', 'medicine', 'medical', 'healthcare', 'nutrition', 'fitness', 'mentalhealth',\n",
    "                      'Coronavirus', 'COVID19', 'publichealth', 'MedicalNews'],\n",
    "        'search_queries': ['health', 'medicine', 'medical', 'healthcare', 'disease', 'treatment', 'wellness', 'vaccine'],\n",
    "        'methods': ['search', 'top']\n",
    "    },\n",
    "    'Sport': {\n",
    "        'subreddits': ['sports', 'nfl', 'nba', 'soccer', 'baseball', 'hockey', 'olympics', 'football',\n",
    "                      'MMA', 'tennis', 'golf', 'worldcup'],\n",
    "        'search_queries': ['sports', 'football', 'basketball', 'soccer', 'baseball', 'olympics', 'athletics', 'championship'],\n",
    "        'methods': ['search', 'top']\n",
    "    },\n",
    "    # Add other categories similarly...\n",
    "}\n",
    "\n",
    "def collect_historical_posts(subreddit_name, search_query=None, method=\"top\", topic_category=\"general\", time_periods=None):\n",
    "    \"\"\"\n",
    "    Enhanced function to collect historical Reddit data using multiple time periods\n",
    "    \"\"\"\n",
    "    print(f\"  üîç Method: {method.upper()} from r/{subreddit_name}\")\n",
    "    if search_query:\n",
    "        print(f\"     Query: '{search_query}'\")\n",
    "    \n",
    "    if not time_periods:\n",
    "        # Default time periods for 3 years of data\n",
    "        time_periods = ['month', 'year', 'all']\n",
    "    \n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    all_posts = []\n",
    "    \n",
    "    for time_period in time_periods:\n",
    "        print(f\"     üìÖ Collecting from time period: {time_period}\")\n",
    "        posts = []\n",
    "        \n",
    "        try:\n",
    "            if method == \"search\" and search_query:\n",
    "                # Search with different time filters\n",
    "                if time_period == 'month':\n",
    "                    submissions = subreddit.search(search_query, limit=500, time_filter='month', sort='top')\n",
    "                elif time_period == 'year':\n",
    "                    submissions = subreddit.search(search_query, limit=800, time_filter='year', sort='top')\n",
    "                else:  # 'all'\n",
    "                    submissions = subreddit.search(search_query, limit=1000, time_filter='all', sort='top')\n",
    "                    \n",
    "            elif method == \"top\":\n",
    "                # Get top posts from different time periods\n",
    "                if time_period == 'month':\n",
    "                    submissions = subreddit.top(limit=500, time_filter='month')\n",
    "                elif time_period == 'year':\n",
    "                    submissions = subreddit.top(limit=800, time_filter='year')\n",
    "                else:  # 'all'\n",
    "                    submissions = subreddit.top(limit=1000, time_filter='all')\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            for i, submission in enumerate(submissions):\n",
    "                try:\n",
    "                    # Check if post is from the last 3 years\n",
    "                    post_date = datetime.fromtimestamp(submission.created_utc)\n",
    "                    three_years_ago = datetime.now() - timedelta(days=3*365)\n",
    "                    \n",
    "                    if post_date < three_years_ago:\n",
    "                        continue  # Skip posts older than 3 years\n",
    "                    \n",
    "                    # Extract text content\n",
    "                    text_content = \"\"\n",
    "                    if submission.is_self and submission.selftext:\n",
    "                        text_content = submission.selftext.strip()\n",
    "                    elif not submission.is_self:\n",
    "                        text_content = f\"Link post: {submission.url}\"\n",
    "                    \n",
    "                    if not text_content:\n",
    "                        text_content = f\"[Title only] {submission.title}\"\n",
    "                    \n",
    "                    post_data = {\n",
    "                        'id': submission.id,\n",
    "                        'title': submission.title if submission.title else \"No title\",\n",
    "                        'text': text_content,\n",
    "                        'author': str(submission.author) if submission.author else '[deleted]',\n",
    "                        'subreddit': submission.subreddit.display_name,\n",
    "                        'score': submission.score if submission.score else 0,\n",
    "                        'upvote_ratio': getattr(submission, 'upvote_ratio', 0),\n",
    "                        'comments': submission.num_comments if submission.num_comments else 0,\n",
    "                        'url': submission.url if submission.url else \"\",\n",
    "                        'is_self': submission.is_self,\n",
    "                        'created': post_date.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        'created_utc': submission.created_utc,\n",
    "                        'permalink': f\"https://reddit.com{submission.permalink}\",\n",
    "                        'method_collected': f\"{method}_{time_period}\",\n",
    "                        'category': topic_category,\n",
    "                        'flair': submission.link_flair_text if hasattr(submission, 'link_flair_text') else None,\n",
    "                        'nsfw': submission.over_18 if hasattr(submission, 'over_18') else False,\n",
    "                        'time_period': time_period\n",
    "                    }\n",
    "                    \n",
    "                    posts.append(post_data)\n",
    "                    \n",
    "                    if (i + 1) % 100 == 0:\n",
    "                        print(f\"        Processed {i + 1} posts from {time_period}...\")\n",
    "                        time.sleep(0.3)  # Rate limiting\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"        Error processing post {i}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"        ‚úÖ Collected {len(posts)} posts from {time_period}\")\n",
    "            all_posts.extend(posts)\n",
    "            time.sleep(2)  # Rate limiting between time periods\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"        ‚ùå Error with {method} method for {time_period}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"     üìä Total collected: {len(all_posts)} posts\")\n",
    "    return all_posts\n",
    "\n",
    "def collect_comprehensive_category_data(category_name, category_config, max_posts_per_category=5000):\n",
    "    \"\"\"\n",
    "    Enhanced category collection for 3 years of data\n",
    "    \"\"\"\n",
    "    print(f\"\\nüéØ === COLLECTING 3-YEAR {category_name.upper().replace('_', ' ')} DATA ===\")\n",
    "    category_posts = []\n",
    "    \n",
    "    subreddits = category_config['subreddits']\n",
    "    search_queries = category_config['search_queries'][:5]  # Use first 5 queries\n",
    "    methods = category_config['methods']\n",
    "    \n",
    "    posts_per_subreddit = max_posts_per_category // len(subreddits)\n",
    "    \n",
    "    for subreddit in subreddits:\n",
    "        print(f\"\\n  üìç Processing r/{subreddit} (3-year historical data)...\")\n",
    "        subreddit_posts = []\n",
    "        \n",
    "        try:\n",
    "            # Method 1: Search with multiple queries across time periods\n",
    "            if 'search' in methods and search_queries:\n",
    "                for query in search_queries:\n",
    "                    if len(subreddit_posts) >= posts_per_subreddit:\n",
    "                        break\n",
    "                    \n",
    "                    search_posts = collect_historical_posts(\n",
    "                        subreddit, query, method=\"search\", topic_category=category_name,\n",
    "                        time_periods=['month', 'year', 'all']\n",
    "                    )\n",
    "                    if search_posts:\n",
    "                        subreddit_posts.extend(search_posts)\n",
    "                        print(f\"     Added {len(search_posts)} posts from search: '{query}'\")\n",
    "                    time.sleep(3)  # Rate limiting between queries\n",
    "            \n",
    "            # Method 2: Top posts across time periods\n",
    "            if 'top' in methods and len(subreddit_posts) < posts_per_subreddit:\n",
    "                time.sleep(3)\n",
    "                top_posts = collect_historical_posts(\n",
    "                    subreddit, None, method=\"top\", topic_category=category_name,\n",
    "                    time_periods=['month', 'year', 'all']\n",
    "                )\n",
    "                if top_posts:\n",
    "                    subreddit_posts.extend(top_posts)\n",
    "                    print(f\"     Added {len(top_posts)} posts from top posts\")\n",
    "            \n",
    "            category_posts.extend(subreddit_posts)\n",
    "            print(f\"  ‚úÖ r/{subreddit}: {len(subreddit_posts)} posts total\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error with r/{subreddit}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        time.sleep(5)  # Longer rate limiting between subreddits for historical data\n",
    "    \n",
    "    return category_posts\n",
    "\n",
    "def analyze_date_distribution(posts):\n",
    "    \"\"\"\n",
    "    Analyze the date distribution of collected posts\n",
    "    \"\"\"\n",
    "    if not posts:\n",
    "        return {}\n",
    "    \n",
    "    # Group posts by year and month\n",
    "    date_dist = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for post in posts:\n",
    "        try:\n",
    "            post_date = datetime.strptime(post['created'], '%Y-%m-%d %H:%M:%S')\n",
    "            year = post_date.year\n",
    "            month = post_date.month\n",
    "            date_dist[year][month] += 1\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return dict(date_dist)\n",
    "\n",
    "def save_enhanced_category_json(posts, category_name, output_dir=r\"C:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\data\\raw\\social_Media_data\\reddit\"):\n",
    "    \"\"\"\n",
    "    Enhanced save function with date analysis\n",
    "    \"\"\"\n",
    "    if not posts:\n",
    "        print(f\"  ‚ùå No posts to save for {category_name}!\")\n",
    "        return False\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    try:\n",
    "        # Analyze date distribution\n",
    "        date_distribution = analyze_date_distribution(posts)\n",
    "        \n",
    "        # Calculate date range\n",
    "        dates = [datetime.strptime(p['created'], '%Y-%m-%d %H:%M:%S') for p in posts if p['created']]\n",
    "        earliest_date = min(dates) if dates else None\n",
    "        latest_date = max(dates) if dates else None\n",
    "        \n",
    "        # Create comprehensive data structure\n",
    "        output_data = {\n",
    "            \"metadata\": {\n",
    "                \"category\": category_name.replace('_', ' '),\n",
    "                \"total_posts\": len(posts),\n",
    "                \"collection_date\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"methods_used\": list(set([post.get('method_collected', 'unknown') for post in posts])),\n",
    "                \"subreddits\": list(set([post['subreddit'] for post in posts])),\n",
    "                \"date_range\": {\n",
    "                    \"earliest\": earliest_date.strftime('%Y-%m-%d %H:%M:%S') if earliest_date else None,\n",
    "                    \"latest\": latest_date.strftime('%Y-%m-%d %H:%M:%S') if latest_date else None,\n",
    "                    \"span_days\": (latest_date - earliest_date).days if earliest_date and latest_date else 0\n",
    "                },\n",
    "                \"date_distribution\": date_distribution,\n",
    "                \"text_statistics\": {\n",
    "                    \"posts_with_text\": len([p for p in posts if p['text'] and not p['text'].startswith('[Title only]') and not p['text'].startswith('Link post:')]),\n",
    "                    \"average_score\": sum([p['score'] for p in posts]) / len(posts),\n",
    "                    \"total_comments\": sum([p['comments'] for p in posts])\n",
    "                },\n",
    "                \"time_period_breakdown\": {\n",
    "                    period: len([p for p in posts if p.get('time_period') == period])\n",
    "                    for period in ['month', 'year', 'all']\n",
    "                }\n",
    "            },\n",
    "            \"posts\": sorted(posts, key=lambda x: x.get('created_utc', 0), reverse=True)  # Sort by date\n",
    "        }\n",
    "        \n",
    "        filename = os.path.join(output_dir, f\"{category_name}_3year.json\")\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"  ‚úÖ {category_name} 3-year data saved to: {filename}\")\n",
    "        print(f\"     Posts: {len(posts)}\")\n",
    "        print(f\"     Date range: {earliest_date.strftime('%Y-%m-%d') if earliest_date else 'N/A'} to {latest_date.strftime('%Y-%m-%d') if latest_date else 'N/A'}\")\n",
    "        print(f\"     Span: {(latest_date - earliest_date).days if earliest_date and latest_date else 0} days\")\n",
    "        return filename\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error saving {category_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def remove_duplicates_enhanced(all_posts):\n",
    "    \"\"\"Enhanced duplicate removal with date preference\"\"\"\n",
    "    seen_ids = {}\n",
    "    unique_posts = []\n",
    "    \n",
    "    for post in all_posts:\n",
    "        post_id = post['id']\n",
    "        if post_id not in seen_ids:\n",
    "            unique_posts.append(post)\n",
    "            seen_ids[post_id] = post\n",
    "        else:\n",
    "            # Keep the post with more recent collection or better data\n",
    "            existing = seen_ids[post_id]\n",
    "            if post.get('score', 0) > existing.get('score', 0):\n",
    "                # Replace with higher scored version\n",
    "                idx = unique_posts.index(existing)\n",
    "                unique_posts[idx] = post\n",
    "                seen_ids[post_id] = post\n",
    "    \n",
    "    return unique_posts\n",
    "\n",
    "# Main execution for 3-year data collection\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ REDDIT 3-YEAR HISTORICAL DATA COLLECTION\")\n",
    "    print(\"Enhanced for comprehensive historical coverage\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Test Reddit connection\n",
    "    try:\n",
    "        print(\"üîå Testing Reddit connection...\")\n",
    "        test_sub = reddit.subreddit('python')\n",
    "        print(f\"‚úÖ Connected! Test subreddit: {test_sub.display_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Reddit connection failed: {e}\")\n",
    "        print(\"Check your Reddit API credentials!\")\n",
    "        exit()\n",
    "    \n",
    "    # Collection summary\n",
    "    collection_summary = {\n",
    "        \"collection_date\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"target_timespan\": \"3 years\",\n",
    "        \"categories_collected\": [],\n",
    "        \"total_posts\": 0,\n",
    "        \"files_created\": []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìã CATEGORIES TO COLLECT (3-YEAR HISTORICAL DATA):\")\n",
    "    for i, category in enumerate(CATEGORIES_CONFIG.keys(), 1):\n",
    "        print(f\"  {i:2d}. {category.replace('_', ' ')}\")\n",
    "    \n",
    "    # Collect data for each category\n",
    "    for i, (category_name, category_config) in enumerate(CATEGORIES_CONFIG.items(), 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CATEGORY {i}/{len(CATEGORIES_CONFIG)}: {category_name.replace('_', ' ').upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            # Collect 3-year category data\n",
    "            category_posts = collect_comprehensive_category_data(\n",
    "                category_name, category_config, max_posts_per_category=8000\n",
    "            )\n",
    "            \n",
    "            if category_posts:\n",
    "                # Enhanced duplicate removal\n",
    "                unique_posts = remove_duplicates_enhanced(category_posts)\n",
    "                print(f\"\\n  üìä Removed {len(category_posts) - len(unique_posts)} duplicates\")\n",
    "                \n",
    "                # Save to file\n",
    "                filename = save_enhanced_category_json(unique_posts, category_name)\n",
    "                \n",
    "                if filename:\n",
    "                    collection_summary[\"categories_collected\"].append(category_name)\n",
    "                    collection_summary[\"total_posts\"] += len(unique_posts)\n",
    "                    collection_summary[\"files_created\"].append(filename)\n",
    "                \n",
    "            else:\n",
    "                print(f\"  ‚ùå No data collected for {category_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error processing {category_name}: {e}\")\n",
    "        \n",
    "        # Longer wait between categories for historical data\n",
    "        if i < len(CATEGORIES_CONFIG):\n",
    "            wait_time = 60  # 60 seconds between categories\n",
    "            print(f\"\\n  ‚è≥ Waiting {wait_time} seconds before next category...\")\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üéâ 3-YEAR DATA COLLECTION COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Categories processed: {len(collection_summary['categories_collected'])}/{len(CATEGORIES_CONFIG)}\")\n",
    "    print(f\"Total posts collected: {collection_summary['total_posts']:,}\")\n",
    "    print(f\"Target timespan: 3 years of historical data\")\n",
    "    \n",
    "    # Save summary\n",
    "    summary_file = r\"C:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\data\\raw\\social_Media_data\\reddit\\3year_collection_summary.json\"\n",
    "    try:\n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(collection_summary, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"üìä Collection summary saved: {summary_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving summary: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8548ba7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
