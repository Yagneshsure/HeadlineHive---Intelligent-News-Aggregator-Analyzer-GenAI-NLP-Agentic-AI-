{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed2954c",
   "metadata": {},
   "source": [
    "# Twitter scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eb5bc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22AI%20OR%20Artificial%20Intelligence%20since%3A2025-08-15%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D: blocked (404)\n",
      "4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22AI%20OR%20Artificial%20Intelligence%20since%3A2025-08-15%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up.\n",
      "Errors: blocked (404), blocked (404), blocked (404), blocked (404)\n"
     ]
    },
    {
     "ename": "ScraperException",
     "evalue": "4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22AI%20OR%20Artificial%20Intelligence%20since%3A2025-08-15%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mScraperException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(tweets)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m df_twitter \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_twitter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAI OR Artificial Intelligence since:2025-08-15\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_twitter\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m, in \u001b[0;36mscrape_twitter\u001b[1;34m(query, limit)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mscrape_twitter\u001b[39m(query, limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m      5\u001b[0m     tweets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, tweet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sntwitter\u001b[38;5;241m.\u001b[39mTwitterSearchScraper(query)\u001b[38;5;241m.\u001b[39mget_items()):\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m limit:\n\u001b[0;32m      8\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\snscrape\\modules\\twitter.py:1763\u001b[0m, in \u001b[0;36mTwitterSearchScraper.get_items\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1760\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m'\u001b[39m: variables, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m: features}\n\u001b[0;32m   1761\u001b[0m paginationParams \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m'\u001b[39m: paginationVariables, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m: features}\n\u001b[1;32m-> 1763\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_api_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline\u001b[39m\u001b[38;5;124m'\u001b[39m, _TwitterAPIType\u001b[38;5;241m.\u001b[39mGRAPHQL, params, paginationParams, cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor, instructionsPath \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_by_raw_query\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_timeline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m   1764\u001b[0m \t\u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graphql_timeline_instructions_to_tweets(obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_by_raw_query\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_timeline\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeline\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\snscrape\\modules\\twitter.py:915\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._iter_api_data\u001b[1;34m(self, endpoint, apiType, params, paginationParams, cursor, direction, instructionsPath)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetrieving scroll page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcursor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 915\u001b[0m \tobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_api_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreqParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    916\u001b[0m \t\u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    918\u001b[0m \t\u001b[38;5;66;03m# No data format test, just a hard and loud crash if anything's wrong :-)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\snscrape\\modules\\twitter.py:886\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._get_api_data\u001b[1;34m(self, endpoint, apiType, params, instructionsPath)\u001b[0m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m apiType \u001b[38;5;129;01mis\u001b[39;00m _TwitterAPIType\u001b[38;5;241m.\u001b[39mGRAPHQL:\n\u001b[0;32m    885\u001b[0m \tparams \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39murlencode({k: json\u001b[38;5;241m.\u001b[39mdumps(v, separators \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems()}, quote_via \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39mquote)\n\u001b[1;32m--> 886\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apiHeaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponseOkCallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfunctools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_api_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39m_snscrapeObj\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\snscrape\\base.py:275\u001b[0m, in \u001b[0;36mScraper._get\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 275\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\snscrape\\base.py:271\u001b[0m, in \u001b[0;36mScraper._request\u001b[1;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[0m\n\u001b[0;32m    269\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39mfatal(msg)\n\u001b[0;32m    270\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39mfatal(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mErrors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(errors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 271\u001b[0m \t\u001b[38;5;28;01mraise\u001b[39;00m ScraperException(msg)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReached unreachable code\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mScraperException\u001b[0m: 4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22AI%20OR%20Artificial%20Intelligence%20since%3A2025-08-15%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up."
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_twitter(query, limit=50):\n",
    "    tweets = []\n",
    "    for i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
    "        if i >= limit:\n",
    "            break\n",
    "        tweets.append({\n",
    "            \"date\": tweet.date,\n",
    "            \"id\": tweet.id,\n",
    "            \"content\": tweet.content,\n",
    "            \"username\": tweet.user.username,\n",
    "            \"retweets\": tweet.retweetCount,\n",
    "            \"likes\": tweet.likeCount,\n",
    "        })\n",
    "    return pd.DataFrame(tweets)\n",
    "\n",
    "# Example usage\n",
    "df_twitter = scrape_twitter(\"AI OR Artificial Intelligence since:2025-08-15\")\n",
    "print(df_twitter.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82987048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snscrape"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached snscrape-0.7.0.20230622-py3-none-any.whl (74 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Collecting lxml\n",
      "  Using cached lxml-6.0.0-cp310-cp310-win_amd64.whl (4.0 MB)\n",
      "Collecting requests[socks]\n",
      "  Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Collecting typing-extensions>=4.0.0\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Using cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.4.3-cp310-cp310-win_amd64.whl (107 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, soupsieve, PySocks, lxml, idna, filelock, charset_normalizer, certifi, requests, beautifulsoup4, snscrape\n",
      "Successfully installed PySocks-1.7.1 beautifulsoup4-4.13.4 certifi-2025.8.3 charset_normalizer-3.4.3 filelock-3.19.1 idna-3.10 lxml-6.0.0 requests-2.32.5 snscrape-0.7.0.20230622 soupsieve-2.7 typing-extensions-4.14.1 urllib3-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install snscrape\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe0843b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FileFinder' object has no attribute 'find_module'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msnscrape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtwitter\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msntwitter\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([tweet\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m sntwitter\u001b[38;5;241m.\u001b[39mTwitterSearchScraper(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mget_items()])\n",
      "File \u001b[1;32mc:\\Users\\yagne\\miniconda3\\Lib\\site-packages\\snscrape\\modules\\__init__.py:17\u001b[0m\n\u001b[0;32m     13\u001b[0m \t\tmodule \u001b[38;5;241m=\u001b[39m importer\u001b[38;5;241m.\u001b[39mfind_module(moduleName)\u001b[38;5;241m.\u001b[39mload_module(moduleName)\n\u001b[0;32m     14\u001b[0m \t\t\u001b[38;5;28mglobals\u001b[39m()[moduleNameWithoutPrefix] \u001b[38;5;241m=\u001b[39m module\n\u001b[1;32m---> 17\u001b[0m \u001b[43m_import_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yagne\\miniconda3\\Lib\\site-packages\\snscrape\\modules\\__init__.py:13\u001b[0m, in \u001b[0;36m_import_modules\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m moduleNameWithoutPrefix \u001b[38;5;241m=\u001b[39m moduleName[prefixLen:]\n\u001b[0;32m     12\u001b[0m __all__\u001b[38;5;241m.\u001b[39mappend(moduleNameWithoutPrefix)\n\u001b[1;32m---> 13\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimporter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_module\u001b[49m(moduleName)\u001b[38;5;241m.\u001b[39mload_module(moduleName)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mglobals\u001b[39m()[moduleNameWithoutPrefix] \u001b[38;5;241m=\u001b[39m module\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FileFinder' object has no attribute 'find_module'"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([tweet.content for tweet in sntwitter.TwitterSearchScraper(\"AI\").get_items()])\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef307b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89be22de",
   "metadata": {},
   "source": [
    "# Reddit Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d0e87c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Politics from r/politics...\n",
      "Scraping Economy from r/economics...\n",
      "Scraping Health from r/health...\n",
      "Scraping Sport from r/sports...\n",
      "Scraping Technology from r/technology...\n",
      "Scraping Entertainment from r/movies...\n",
      "Scraping Society from r/worldnews...\n",
      "Saved scraping results to reddit_data.csv\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"xeV3sLrkl4xGyM-ScCBkSA\",\n",
    "    client_secret=\"sGrJQoAWabMp3hVcAXTxocV3gkCXoQ\",\n",
    "    user_agent=\"HeadlineHive\"\n",
    ")\n",
    "\n",
    "def scrape_reddit(subreddit, limit=100):\n",
    "    posts = []\n",
    "    for post in reddit.subreddit(subreddit).hot(limit=limit):\n",
    "        posts.append({\n",
    "            \"title\": post.title,\n",
    "            \"text\": post.selftext,\n",
    "            \"score\": post.score,\n",
    "            \"subreddit\": subreddit\n",
    "        })\n",
    "    return pd.DataFrame(posts)\n",
    "\n",
    "subs = {\n",
    "    \"Politics\": \"politics\",\n",
    "    \"Economy\": \"economics\",\n",
    "    \"Health\": \"health\",\n",
    "    \"Sport\": \"sports\",\n",
    "    \"Technology\": \"technology\",\n",
    "    \"Entertainment\": \"movies\",\n",
    "    \"Society\": \"worldnews\"\n",
    "}\n",
    "\n",
    "all_reddit = pd.DataFrame()\n",
    "for cat, sub in subs.items():\n",
    "    print(f\"Scraping {cat} from r/{sub}...\")\n",
    "    df = scrape_reddit(sub, limit=200)\n",
    "    df[\"category\"] = cat\n",
    "    all_reddit = pd.concat([all_reddit, df], ignore_index=True)\n",
    "\n",
    "all_reddit.to_csv(\"reddit_data.csv\", index=False)\n",
    "print(\"Saved scraping results to reddit_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc3073b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Reddit data collection with multiple methods...\n",
      "\n",
      "==================================================\n",
      "üîç Method 1: Searching r/politics for 'politics'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 100 posts...\n",
      "Collected 200 posts...\n",
      "‚úÖ Method 1 Success: 234 posts\n",
      "\n",
      "==================================================\n",
      "üî• Method 2: Getting ['hot', 'new', 'top'] posts from r/politics...\n",
      "Fetching hot posts...\n",
      "Got 200 hot posts\n",
      "Fetching new posts...\n",
      "Got 200 new posts\n",
      "Fetching top posts...\n",
      "Got 200 top posts\n",
      "‚úÖ Method 2 Success: 600 posts\n",
      "After keyword filtering: 44 posts\n",
      "\n",
      "==================================================\n",
      "üì° Method 3: Using Reddit JSON API for r/politics...\n",
      "Collected 25 posts via JSON API...\n",
      "Collected 50 posts via JSON API...\n",
      "Collected 75 posts via JSON API...\n",
      "Collected 100 posts via JSON API...\n",
      "Collected 125 posts via JSON API...\n",
      "Collected 150 posts via JSON API...\n",
      "Collected 175 posts via JSON API...\n",
      "Collected 200 posts via JSON API...\n",
      "‚úÖ Method 3 Success: 200 posts\n",
      "\n",
      "==================================================\n",
      "üéØ Method 4: Collecting from multiple subreddits...\n",
      "Processing r/politics...\n",
      "üîç Method 1: Searching r/politics for 'politics'...\n",
      "Collected 100 posts...\n",
      "Collected 200 posts...\n",
      "Got 200 posts from r/politics\n",
      "Processing r/PoliticalDiscussion...\n",
      "üîç Method 1: Searching r/PoliticalDiscussion for 'politics'...\n",
      "Collected 100 posts...\n",
      "Collected 200 posts...\n",
      "Got 200 posts from r/PoliticalDiscussion\n",
      "Processing r/neutralpolitics...\n",
      "üîç Method 1: Searching r/neutralpolitics for 'politics'...\n",
      "Collected 100 posts...\n",
      "Collected 200 posts...\n",
      "Got 200 posts from r/neutralpolitics\n",
      "Processing r/Ask_Politics...\n",
      "üîç Method 1: Searching r/Ask_Politics for 'politics'...\n",
      "Collected 100 posts...\n",
      "Collected 200 posts...\n",
      "Got 200 posts from r/Ask_Politics\n",
      "‚úÖ Method 4 Success: 800 posts from multiple subreddits\n",
      "\n",
      "üéâ FINAL RESULT: 1051 unique posts collected!\n",
      "Date range: 2012-06-01 05:28:02 to 2025-08-21 12:06:17\n",
      "üíæ Data saved to: reddit_politics_data_20250821_174542.csv\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas.io.formats.string'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 275\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;66;03m# Show sample\u001b[39;00m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSample data:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 275\u001b[0m     \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauthor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcreated_utc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚ùå All methods failed. This might be due to:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\pandas\\core\\frame.py:1214\u001b[0m, in \u001b[0;36m__repr__\u001b[1;34m(self)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\pandas\\core\\frame.py:1394\u001b[0m, in \u001b[0;36mto_string\u001b[1;34m(self, buf, columns, col_space, header, index, na_rep, formatters, float_format, sparsify, index_names, justify, max_rows, max_cols, show_dimensions, decimal, line_width, min_rows, max_colwidth, encoding)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\yagne\\OneDrive\\Desktop\\project s\\Co-projects\\HeadlineHive-Intelligent-News-Aggregator-Analyzer-GenAI-NLP-Agentic-AI-\\venv310\\lib\\site-packages\\pandas\\io\\formats\\format.py:959\u001b[0m, in \u001b[0;36mto_string\u001b[1;34m(self, buf, encoding, line_width)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas.io.formats.string'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import praw\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"xeV3sLrkl4xGyM-ScCBkSA\",\n",
    "    client_secret=\"sGrJQoAWabMp3hVcAXTxocV3gkCXoQ\",\n",
    "    user_agent=\"HeadlineHive\"\n",
    ")\n",
    "\n",
    "def method_1_praw_search(subreddit_name, query, limit=1000, time_filter='all'):\n",
    "    \"\"\"\n",
    "    Method 1: PRAW Search (Most Reliable)\n",
    "    Uses Reddit's official API through PRAW\n",
    "    \"\"\"\n",
    "    print(f\"üîç Method 1: Searching r/{subreddit_name} for '{query}'...\")\n",
    "    \n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "    \n",
    "    try:\n",
    "        # Search posts\n",
    "        for submission in subreddit.search(query, limit=limit, time_filter=time_filter):\n",
    "            posts.append({\n",
    "                \"id\": submission.id,\n",
    "                \"title\": submission.title,\n",
    "                \"selftext\": submission.selftext,\n",
    "                \"author\": str(submission.author) if submission.author else \"[deleted]\",\n",
    "                \"subreddit\": submission.subreddit.display_name,\n",
    "                \"created_utc\": pd.to_datetime(submission.created_utc, unit='s'),\n",
    "                \"url\": submission.url,\n",
    "                \"score\": submission.score,\n",
    "                \"num_comments\": submission.num_comments,\n",
    "                \"upvote_ratio\": submission.upvote_ratio,\n",
    "                \"is_self\": submission.is_self,\n",
    "                \"permalink\": f\"https://reddit.com{submission.permalink}\"\n",
    "            })\n",
    "            \n",
    "            if len(posts) % 100 == 0:\n",
    "                print(f\"Collected {len(posts)} posts...\")\n",
    "                time.sleep(0.1)  # Be gentle with API\n",
    "                \n",
    "        return pd.DataFrame(posts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"PRAW search error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def method_2_praw_hot_new_top(subreddit_name, categories=['hot', 'new', 'top'], limit_per_category=500):\n",
    "    \"\"\"\n",
    "    Method 2: PRAW Hot/New/Top Posts\n",
    "    Gets recent popular posts from specific categories\n",
    "    \"\"\"\n",
    "    print(f\"üî• Method 2: Getting {categories} posts from r/{subreddit_name}...\")\n",
    "    \n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    all_posts = []\n",
    "    \n",
    "    for category in categories:\n",
    "        print(f\"Fetching {category} posts...\")\n",
    "        posts = []\n",
    "        \n",
    "        try:\n",
    "            if category == 'hot':\n",
    "                submissions = subreddit.hot(limit=limit_per_category)\n",
    "            elif category == 'new':\n",
    "                submissions = subreddit.new(limit=limit_per_category)\n",
    "            elif category == 'top':\n",
    "                submissions = subreddit.top(time_filter='month', limit=limit_per_category)\n",
    "            elif category == 'rising':\n",
    "                submissions = subreddit.rising(limit=limit_per_category)\n",
    "            \n",
    "            for submission in submissions:\n",
    "                posts.append({\n",
    "                    \"id\": submission.id,\n",
    "                    \"title\": submission.title,\n",
    "                    \"selftext\": submission.selftext,\n",
    "                    \"author\": str(submission.author) if submission.author else \"[deleted]\",\n",
    "                    \"subreddit\": submission.subreddit.display_name,\n",
    "                    \"created_utc\": pd.to_datetime(submission.created_utc, unit='s'),\n",
    "                    \"url\": submission.url,\n",
    "                    \"score\": submission.score,\n",
    "                    \"num_comments\": submission.num_comments,\n",
    "                    \"upvote_ratio\": submission.upvote_ratio,\n",
    "                    \"category\": category,\n",
    "                    \"permalink\": f\"https://reddit.com{submission.permalink}\"\n",
    "                })\n",
    "                \n",
    "            print(f\"Got {len(posts)} {category} posts\")\n",
    "            all_posts.extend(posts)\n",
    "            time.sleep(1)  # Rate limiting\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {category} posts: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(all_posts)\n",
    "\n",
    "def method_3_reddit_json_api(subreddit_name, category='hot', limit=100):\n",
    "    \"\"\"\n",
    "    Method 3: Direct Reddit JSON API\n",
    "    Uses Reddit's JSON endpoints (no authentication required)\n",
    "    \"\"\"\n",
    "    print(f\"üì° Method 3: Using Reddit JSON API for r/{subreddit_name}...\")\n",
    "    \n",
    "    base_url = f\"https://www.reddit.com/r/{subreddit_name}/{category}.json\"\n",
    "    posts = []\n",
    "    after = None\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'HeadlineHive/1.0'\n",
    "    }\n",
    "    \n",
    "    while len(posts) < limit:\n",
    "        params = {'limit': 25}\n",
    "        if after:\n",
    "            params['after'] = after\n",
    "            \n",
    "        try:\n",
    "            response = requests.get(base_url, params=params, headers=headers)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                children = data['data']['children']\n",
    "                \n",
    "                if not children:\n",
    "                    break\n",
    "                \n",
    "                for child in children:\n",
    "                    post = child['data']\n",
    "                    posts.append({\n",
    "                        \"id\": post.get('id'),\n",
    "                        \"title\": post.get('title'),\n",
    "                        \"selftext\": post.get('selftext'),\n",
    "                        \"author\": post.get('author'),\n",
    "                        \"subreddit\": post.get('subreddit'),\n",
    "                        \"created_utc\": pd.to_datetime(post.get('created_utc'), unit='s'),\n",
    "                        \"url\": post.get('url'),\n",
    "                        \"score\": post.get('score'),\n",
    "                        \"num_comments\": post.get('num_comments'),\n",
    "                        \"upvote_ratio\": post.get('upvote_ratio'),\n",
    "                        \"permalink\": f\"https://reddit.com{post.get('permalink')}\"\n",
    "                    })\n",
    "                \n",
    "                after = data['data']['after']\n",
    "                print(f\"Collected {len(posts)} posts via JSON API...\")\n",
    "                \n",
    "                if not after:  # No more pages\n",
    "                    break\n",
    "                    \n",
    "                time.sleep(1)  # Rate limiting\n",
    "                \n",
    "            else:\n",
    "                print(f\"HTTP Error: {response.status_code}\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"JSON API error: {e}\")\n",
    "            break\n",
    "    \n",
    "    return pd.DataFrame(posts[:limit])\n",
    "\n",
    "def method_4_multiple_subreddits(subreddit_list, query, method='search'):\n",
    "    \"\"\"\n",
    "    Method 4: Collect from multiple subreddits\n",
    "    \"\"\"\n",
    "    print(f\"üéØ Method 4: Collecting from multiple subreddits...\")\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for sub in subreddit_list:\n",
    "        print(f\"Processing r/{sub}...\")\n",
    "        try:\n",
    "            if method == 'search':\n",
    "                df = method_1_praw_search(sub, query, limit=200)\n",
    "            elif method == 'hot':\n",
    "                df = method_2_praw_hot_new_top(sub, ['hot'], limit_per_category=200)\n",
    "                \n",
    "            if not df.empty:\n",
    "                all_data.append(df)\n",
    "                print(f\"Got {len(df)} posts from r/{sub}\")\n",
    "            \n",
    "            time.sleep(2)  # Be respectful between subreddits\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with r/{sub}: {e}\")\n",
    "    \n",
    "    if all_data:\n",
    "        return pd.concat(all_data, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def filter_by_keywords(df, keywords, column='title'):\n",
    "    \"\"\"\n",
    "    Filter dataframe by keywords in title or text\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    pattern = '|'.join(keywords)\n",
    "    mask = df[column].str.contains(pattern, case=False, na=False)\n",
    "    \n",
    "    if 'selftext' in df.columns:\n",
    "        selftext_mask = df['selftext'].str.contains(pattern, case=False, na=False)\n",
    "        mask = mask | selftext_mask\n",
    "    \n",
    "    return df[mask]\n",
    "\n",
    "# Example usage with multiple fallback methods\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting Reddit data collection with multiple methods...\\n\")\n",
    "    \n",
    "    # Try Method 1: PRAW Search (most reliable for keyword-based collection)\n",
    "    print(\"=\" * 50)\n",
    "    df1 = method_1_praw_search('politics', 'politics', limit=500)\n",
    "    \n",
    "    if not df1.empty:\n",
    "        print(f\"‚úÖ Method 1 Success: {len(df1)} posts\")\n",
    "    else:\n",
    "        print(\"‚ùå Method 1 failed\")\n",
    "    \n",
    "    # Try Method 2: Get hot/new/top posts from politics subreddit\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    df2 = method_2_praw_hot_new_top('politics', ['hot', 'new', 'top'], limit_per_category=200)\n",
    "    \n",
    "    if not df2.empty:\n",
    "        print(f\"‚úÖ Method 2 Success: {len(df2)} posts\")\n",
    "        # Filter for politics-related content\n",
    "        df2_filtered = filter_by_keywords(df2, ['politics', 'political', 'election', 'government', 'congress'])\n",
    "        print(f\"After keyword filtering: {len(df2_filtered)} posts\")\n",
    "    else:\n",
    "        print(\"‚ùå Method 2 failed\")\n",
    "    \n",
    "    # Try Method 3: Reddit JSON API\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    df3 = method_3_reddit_json_api('politics', 'hot', limit=200)\n",
    "    \n",
    "    if not df3.empty:\n",
    "        print(f\"‚úÖ Method 3 Success: {len(df3)} posts\")\n",
    "    else:\n",
    "        print(\"‚ùå Method 3 failed\")\n",
    "    \n",
    "    # Try Method 4: Multiple subreddits\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    political_subs = ['politics', 'PoliticalDiscussion', 'neutralpolitics', 'Ask_Politics']\n",
    "    df4 = method_4_multiple_subreddits(political_subs, 'politics', method='search')\n",
    "    \n",
    "    if not df4.empty:\n",
    "        print(f\"‚úÖ Method 4 Success: {len(df4)} posts from multiple subreddits\")\n",
    "    else:\n",
    "        print(\"‚ùå Method 4 failed\")\n",
    "    \n",
    "    # Combine all successful methods\n",
    "    successful_dfs = [df for df in [df1, df2_filtered if 'df2_filtered' in locals() else df2, df3, df4] if not df.empty]\n",
    "    \n",
    "    if successful_dfs:\n",
    "        final_df = pd.concat(successful_dfs, ignore_index=True)\n",
    "        # Remove duplicates based on post ID\n",
    "        final_df = final_df.drop_duplicates(subset=['id'], keep='first')\n",
    "        \n",
    "        print(f\"\\nüéâ FINAL RESULT: {len(final_df)} unique posts collected!\")\n",
    "        print(f\"Date range: {final_df['created_utc'].min()} to {final_df['created_utc'].max()}\")\n",
    "        \n",
    "        # Save results\n",
    "        output_file = f\"reddit_politics_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        final_df.to_csv(output_file, index=False)\n",
    "        print(f\"üíæ Data saved to: {output_file}\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(\"\\nSample data:\")\n",
    "        print(final_df[['title', 'author', 'score', 'created_utc']].head())\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ùå All methods failed. This might be due to:\")\n",
    "        print(\"- Reddit API rate limiting\")\n",
    "        print(\"- Network connectivity issues\") \n",
    "        print(\"- Reddit credentials issues\")\n",
    "        print(\"- Subreddit restrictions\")\n",
    "        \n",
    "        # Show troubleshooting steps\n",
    "        print(\"\\nüîß Troubleshooting steps:\")\n",
    "        print(\"1. Check your Reddit API credentials\")\n",
    "        print(\"2. Try again in a few minutes (rate limiting)\")\n",
    "        print(\"3. Test with a smaller limit\")\n",
    "        print(\"4. Check if the subreddit exists and is public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8182e1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Simple Reddit Data Collection\n",
      "==================================================\n",
      "Testing Reddit connection...\n",
      "‚úÖ Connected! Test subreddit: python\n",
      "\n",
      "üìç Method 1: Searching for politics posts...\n",
      "üîç Collecting posts from r/politics...\n",
      "Searching for: 'politics'\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "Collected 150 posts...\n",
      "Collected 200 posts...\n",
      "‚úÖ Collected 234 politics posts\n",
      "‚úÖ Data saved to reddit_politics_20250821_174915.csv\n",
      "‚úÖ JSON data saved to reddit_politics_20250821_174915.json\n",
      "\n",
      "üìã Sample of 3 posts:\n",
      "============================================================\n",
      "Post 1:\n",
      "  Title: DOJ Opens Door To Stripping Citizenship Over Politics...\n",
      "  Author: marji80\n",
      "  Score: 33598\n",
      "  Comments: 3300\n",
      "  Created: 2025-07-02 19:42:01\n",
      "  URL: https://talkingpointsmemo.com/news/doj-opens-door-to-strippi...\n",
      "----------------------------------------\n",
      "Post 2:\n",
      "  Title: Everybody Hates Trump Now | Six months after Trump bragged about a ‚Äúhistoric‚Äù re...\n",
      "  Author: Aggravating_Money992\n",
      "  Score: 17373\n",
      "  Comments: 1238\n",
      "  Created: 2025-07-31 16:41:20\n",
      "  URL: https://newrepublic.com/article/198624/everybody-hates-trump...\n",
      "----------------------------------------\n",
      "Post 3:\n",
      "  Title: Tim Walz: Trump Will Start Arresting Political Opponents...\n",
      "  Author: thedailybeast\n",
      "  Score: 58945\n",
      "  Comments: 4112\n",
      "  Created: 2025-03-20 20:30:01\n",
      "  URL: https://www.thedailybeast.com/tim-walz-trump-will-start-arre...\n",
      "----------------------------------------\n",
      "\n",
      "üìä Data Summary:\n",
      "Total posts: 234\n",
      "Average score: 22926.1\n",
      "Max score: 129734\n",
      "Unique authors: 172\n",
      "\n",
      "üìç Method 2: Trying other political subreddits...\n",
      "Trying r/PoliticalDiscussion...\n",
      "üîç Collecting posts from r/PoliticalDiscussion...\n",
      "Getting hot posts...\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "‚úÖ Added 100 posts from r/PoliticalDiscussion\n",
      "Trying r/Ask_Politics...\n",
      "üîç Collecting posts from r/Ask_Politics...\n",
      "Getting hot posts...\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "‚úÖ Added 100 posts from r/Ask_Politics\n",
      "Trying r/neutralpolitics...\n",
      "üîç Collecting posts from r/neutralpolitics...\n",
      "Getting hot posts...\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "‚úÖ Added 100 posts from r/neutralpolitics\n",
      "\n",
      "üéâ FINAL TOTAL: 534 posts collected!\n",
      "After removing duplicates: 534 unique posts\n",
      "‚úÖ Data saved to reddit_political_data_final_20250821_174930.csv\n",
      "‚úÖ JSON data saved to reddit_political_data_final_20250821_174930.json\n",
      "\n",
      "üéØ Data collection complete!\n",
      "CSV file: reddit_political_data_final_20250821_174930.csv\n",
      "JSON file: reddit_political_data_final_20250821_174930.json\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"xeV3sLrkl4xGyM-ScCBkSA\",\n",
    "    client_secret=\"sGrJQoAWabMp3hVcAXTxocV3gkCXoQ\",\n",
    "    user_agent=\"HeadlineHive\"\n",
    ")\n",
    "\n",
    "def collect_reddit_posts_simple(subreddit_name, search_query=None, limit=500):\n",
    "    \"\"\"\n",
    "    Simple Reddit data collection without pandas dependencies\n",
    "    \"\"\"\n",
    "    print(f\"üîç Collecting posts from r/{subreddit_name}...\")\n",
    "    \n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "    \n",
    "    try:\n",
    "        if search_query:\n",
    "            print(f\"Searching for: '{search_query}'\")\n",
    "            submissions = subreddit.search(search_query, limit=limit)\n",
    "        else:\n",
    "            print(\"Getting hot posts...\")\n",
    "            submissions = subreddit.hot(limit=limit)\n",
    "        \n",
    "        for i, submission in enumerate(submissions):\n",
    "            try:\n",
    "                post_data = {\n",
    "                    'id': submission.id,\n",
    "                    'title': submission.title,\n",
    "                    'text': submission.selftext,\n",
    "                    'author': str(submission.author) if submission.author else '[deleted]',\n",
    "                    'subreddit': submission.subreddit.display_name,\n",
    "                    'score': submission.score,\n",
    "                    'comments': submission.num_comments,\n",
    "                    'url': submission.url,\n",
    "                    'created': datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'permalink': f\"https://reddit.com{submission.permalink}\"\n",
    "                }\n",
    "                posts.append(post_data)\n",
    "                \n",
    "                if (i + 1) % 50 == 0:\n",
    "                    print(f\"Collected {i + 1} posts...\")\n",
    "                    time.sleep(0.5)  # Be nice to the API\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing post {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return posts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting posts: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_to_csv(posts, filename):\n",
    "    \"\"\"Save posts to CSV file\"\"\"\n",
    "    if not posts:\n",
    "        print(\"No posts to save!\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        fieldnames = posts[0].keys()\n",
    "        \n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for post in posts:\n",
    "                # Clean text fields to avoid CSV issues\n",
    "                clean_post = {}\n",
    "                for key, value in post.items():\n",
    "                    if isinstance(value, str):\n",
    "                        # Remove problematic characters\n",
    "                        clean_post[key] = value.replace('\\n', ' ').replace('\\r', ' ')[:1000]\n",
    "                    else:\n",
    "                        clean_post[key] = value\n",
    "                writer.writerow(clean_post)\n",
    "        \n",
    "        print(f\"‚úÖ Data saved to {filename}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving CSV: {e}\")\n",
    "        return False\n",
    "\n",
    "def save_to_json(posts, filename):\n",
    "    \"\"\"Save posts to JSON file\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(posts, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"‚úÖ JSON data saved to {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving JSON: {e}\")\n",
    "        return False\n",
    "\n",
    "def display_sample(posts, num_samples=3):\n",
    "    \"\"\"Display sample posts without pandas\"\"\"\n",
    "    if not posts:\n",
    "        print(\"No posts to display\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüìã Sample of {min(num_samples, len(posts))} posts:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, post in enumerate(posts[:num_samples]):\n",
    "        print(f\"Post {i+1}:\")\n",
    "        print(f\"  Title: {post['title'][:80]}...\")\n",
    "        print(f\"  Author: {post['author']}\")\n",
    "        print(f\"  Score: {post['score']}\")\n",
    "        print(f\"  Comments: {post['comments']}\")\n",
    "        print(f\"  Created: {post['created']}\")\n",
    "        print(f\"  URL: {post['url'][:60]}...\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Simple Reddit Data Collection\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test Reddit connection\n",
    "    try:\n",
    "        print(\"Testing Reddit connection...\")\n",
    "        test_sub = reddit.subreddit('python')\n",
    "        print(f\"‚úÖ Connected! Test subreddit: {test_sub.display_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Reddit connection failed: {e}\")\n",
    "        print(\"Check your Reddit API credentials!\")\n",
    "        exit()\n",
    "    \n",
    "    # Method 1: Search for politics posts\n",
    "    print(\"\\nüìç Method 1: Searching for politics posts...\")\n",
    "    politics_posts = collect_reddit_posts_simple('politics', 'politics', limit=300)\n",
    "    \n",
    "    if politics_posts:\n",
    "        print(f\"‚úÖ Collected {len(politics_posts)} politics posts\")\n",
    "        \n",
    "        # Save data\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        csv_file = f\"reddit_politics_{timestamp}.csv\"\n",
    "        json_file = f\"reddit_politics_{timestamp}.json\"\n",
    "        \n",
    "        save_to_csv(politics_posts, csv_file)\n",
    "        save_to_json(politics_posts, json_file)\n",
    "        \n",
    "        # Display sample\n",
    "        display_sample(politics_posts)\n",
    "        \n",
    "        # Basic stats\n",
    "        print(f\"\\nüìä Data Summary:\")\n",
    "        print(f\"Total posts: {len(politics_posts)}\")\n",
    "        \n",
    "        scores = [post['score'] for post in politics_posts]\n",
    "        if scores:\n",
    "            print(f\"Average score: {sum(scores) / len(scores):.1f}\")\n",
    "            print(f\"Max score: {max(scores)}\")\n",
    "        \n",
    "        authors = [post['author'] for post in politics_posts if post['author'] != '[deleted]']\n",
    "        unique_authors = len(set(authors))\n",
    "        print(f\"Unique authors: {unique_authors}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Failed to collect politics posts\")\n",
    "    \n",
    "    # Method 2: Try other political subreddits\n",
    "    print(\"\\nüìç Method 2: Trying other political subreddits...\")\n",
    "    other_subs = ['PoliticalDiscussion', 'Ask_Politics', 'neutralpolitics']\n",
    "    all_posts = politics_posts.copy() if politics_posts else []\n",
    "    \n",
    "    for sub in other_subs:\n",
    "        print(f\"Trying r/{sub}...\")\n",
    "        try:\n",
    "            sub_posts = collect_reddit_posts_simple(sub, None, limit=100)\n",
    "            if sub_posts:\n",
    "                all_posts.extend(sub_posts)\n",
    "                print(f\"‚úÖ Added {len(sub_posts)} posts from r/{sub}\")\n",
    "            time.sleep(2)  # Rate limiting\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed r/{sub}: {e}\")\n",
    "    \n",
    "    # Final results\n",
    "    if all_posts:\n",
    "        print(f\"\\nüéâ FINAL TOTAL: {len(all_posts)} posts collected!\")\n",
    "        \n",
    "        # Remove duplicates manually\n",
    "        seen_ids = set()\n",
    "        unique_posts = []\n",
    "        for post in all_posts:\n",
    "            if post['id'] not in seen_ids:\n",
    "                unique_posts.append(post)\n",
    "                seen_ids.add(post['id'])\n",
    "        \n",
    "        print(f\"After removing duplicates: {len(unique_posts)} unique posts\")\n",
    "        \n",
    "        # Save final dataset\n",
    "        final_csv = f\"reddit_political_data_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        final_json = f\"reddit_political_data_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        \n",
    "        save_to_csv(unique_posts, final_csv)\n",
    "        save_to_json(unique_posts, final_json)\n",
    "        \n",
    "        print(\"\\nüéØ Data collection complete!\")\n",
    "        print(f\"CSV file: {final_csv}\")\n",
    "        print(f\"JSON file: {final_json}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ùå No data collected. Possible issues:\")\n",
    "        print(\"- Reddit API credentials invalid\")\n",
    "        print(\"- Network connectivity problems\")\n",
    "        print(\"- Rate limiting (try again later)\")\n",
    "        print(\"- Subreddit access restrictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "853c1956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Simple Reddit Data Collection\n",
      "==================================================\n",
      "Testing Reddit connection...\n",
      "‚úÖ Connected! Test subreddit: python\n",
      "\n",
      "üìç Method 1: Searching for politics posts...\n",
      "üîç Collecting posts from r/politics...\n",
      "Searching for: 'politics'\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "Collected 150 posts...\n",
      "Collected 200 posts...\n",
      "‚úÖ Collected 234 politics posts\n",
      "‚úÖ JSON data saved to reddit_politics_20250821_180555.json\n",
      "\n",
      "üìã Sample of 3 posts:\n",
      "============================================================\n",
      "Post 1:\n",
      "  Title: DOJ Opens Door To Stripping Citizenship Over Politics...\n",
      "  Author: marji80\n",
      "  Score: 33606\n",
      "  Comments: 3300\n",
      "  Created: 2025-07-02 19:42:01\n",
      "  URL: https://talkingpointsmemo.com/news/doj-opens-door-to-strippi...\n",
      "----------------------------------------\n",
      "Post 2:\n",
      "  Title: Everybody Hates Trump Now | Six months after Trump bragged about a ‚Äúhistoric‚Äù re...\n",
      "  Author: Aggravating_Money992\n",
      "  Score: 17373\n",
      "  Comments: 1238\n",
      "  Created: 2025-07-31 16:41:20\n",
      "  URL: https://newrepublic.com/article/198624/everybody-hates-trump...\n",
      "----------------------------------------\n",
      "Post 3:\n",
      "  Title: Tim Walz: Trump Will Start Arresting Political Opponents...\n",
      "  Author: thedailybeast\n",
      "  Score: 58943\n",
      "  Comments: 4112\n",
      "  Created: 2025-03-20 20:30:01\n",
      "  URL: https://www.thedailybeast.com/tim-walz-trump-will-start-arre...\n",
      "----------------------------------------\n",
      "\n",
      "üìä Data Summary:\n",
      "Total posts: 234\n",
      "Average score: 22925.9\n",
      "Max score: 129732\n",
      "Unique authors: 172\n",
      "\n",
      "üìç Method 2: Trying other political subreddits...\n",
      "Trying r/PoliticalDiscussion...\n",
      "üîç Collecting posts from r/PoliticalDiscussion...\n",
      "Getting hot posts...\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "‚úÖ Added 100 posts from r/PoliticalDiscussion\n",
      "Trying r/Ask_Politics...\n",
      "üîç Collecting posts from r/Ask_Politics...\n",
      "Getting hot posts...\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "‚úÖ Added 100 posts from r/Ask_Politics\n",
      "Trying r/neutralpolitics...\n",
      "üîç Collecting posts from r/neutralpolitics...\n",
      "Getting hot posts...\n",
      "Collected 50 posts...\n",
      "Collected 100 posts...\n",
      "‚úÖ Added 100 posts from r/neutralpolitics\n",
      "\n",
      "üéâ FINAL TOTAL: 534 posts collected!\n",
      "After removing duplicates: 534 unique posts\n",
      "‚úÖ JSON data saved to reddit_political_data_final_20250821_180610.json\n",
      "\n",
      "üéØ Data collection complete!\n",
      "JSON file: reddit_political_data_final_20250821_180610.json\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"xeV3sLrkl4xGyM-ScCBkSA\",\n",
    "    client_secret=\"sGrJQoAWabMp3hVcAXTxocV3gkCXoQ\",\n",
    "    user_agent=\"HeadlineHive\"\n",
    ")\n",
    "\n",
    "def collect_reddit_posts_simple(subreddit_name, search_query=None, limit=500):\n",
    "    \"\"\"\n",
    "    Simple Reddit data collection without pandas dependencies\n",
    "    \"\"\"\n",
    "    print(f\"üîç Collecting posts from r/{subreddit_name}...\")\n",
    "    \n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "    \n",
    "    try:\n",
    "        if search_query:\n",
    "            print(f\"Searching for: '{search_query}'\")\n",
    "            submissions = subreddit.search(search_query, limit=limit)\n",
    "        else:\n",
    "            print(\"Getting hot posts...\")\n",
    "            submissions = subreddit.hot(limit=limit)\n",
    "        \n",
    "        for i, submission in enumerate(submissions):\n",
    "            try:\n",
    "                post_data = {\n",
    "                    'id': submission.id,\n",
    "                    'title': submission.title,\n",
    "                    'text': submission.selftext,\n",
    "                    'author': str(submission.author) if submission.author else '[deleted]',\n",
    "                    'subreddit': submission.subreddit.display_name,\n",
    "                    'score': submission.score,\n",
    "                    'comments': submission.num_comments,\n",
    "                    'url': submission.url,\n",
    "                    'created': datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'permalink': f\"https://reddit.com{submission.permalink}\"\n",
    "                }\n",
    "                posts.append(post_data)\n",
    "                \n",
    "                if (i + 1) % 50 == 0:\n",
    "                    print(f\"Collected {i + 1} posts...\")\n",
    "                    time.sleep(0.5)  # Be nice to the API\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing post {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return posts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting posts: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_to_csv(posts, filename):\n",
    "    \"\"\"Save posts to CSV file\"\"\"\n",
    "    if not posts:\n",
    "        print(\"No posts to save!\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        fieldnames = posts[0].keys()\n",
    "        \n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for post in posts:\n",
    "                # Clean text fields to avoid CSV issues\n",
    "                clean_post = {}\n",
    "                for key, value in post.items():\n",
    "                    if isinstance(value, str):\n",
    "                        # Remove problematic characters\n",
    "                        clean_post[key] = value.replace('\\n', ' ').replace('\\r', ' ')[:1000]\n",
    "                    else:\n",
    "                        clean_post[key] = value\n",
    "                writer.writerow(clean_post)\n",
    "        \n",
    "        print(f\"‚úÖ Data saved to {filename}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving CSV: {e}\")\n",
    "        return False\n",
    "\n",
    "def save_to_json(posts, filename):\n",
    "    \"\"\"Save posts to JSON file\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(posts, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"‚úÖ JSON data saved to {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving JSON: {e}\")\n",
    "        return False\n",
    "\n",
    "def display_sample(posts, num_samples=3):\n",
    "    \"\"\"Display sample posts without pandas\"\"\"\n",
    "    if not posts:\n",
    "        print(\"No posts to display\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüìã Sample of {min(num_samples, len(posts))} posts:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, post in enumerate(posts[:num_samples]):\n",
    "        print(f\"Post {i+1}:\")\n",
    "        print(f\"  Title: {post['title'][:80]}...\")\n",
    "        print(f\"  Author: {post['author']}\")\n",
    "        print(f\"  Score: {post['score']}\")\n",
    "        print(f\"  Comments: {post['comments']}\")\n",
    "        print(f\"  Created: {post['created']}\")\n",
    "        print(f\"  URL: {post['url'][:60]}...\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Simple Reddit Data Collection\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test Reddit connection\n",
    "    try:\n",
    "        print(\"Testing Reddit connection...\")\n",
    "        test_sub = reddit.subreddit('python')\n",
    "        print(f\"‚úÖ Connected! Test subreddit: {test_sub.display_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Reddit connection failed: {e}\")\n",
    "        print(\"Check your Reddit API credentials!\")\n",
    "        exit()\n",
    "    \n",
    "    # Method 1: Search for politics posts\n",
    "    print(\"\\nüìç Method 1: Searching for politics posts...\")\n",
    "    politics_posts = collect_reddit_posts_simple('politics', 'politics', limit=300)\n",
    "    \n",
    "    if politics_posts:\n",
    "        print(f\"‚úÖ Collected {len(politics_posts)} politics posts\")\n",
    "        \n",
    "        # Save data (JSON only)\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        json_file = f\"reddit_politics_{timestamp}.json\"\n",
    "        \n",
    "        save_to_json(politics_posts, json_file)\n",
    "        \n",
    "        # Display sample\n",
    "        display_sample(politics_posts)\n",
    "        \n",
    "        # Basic stats\n",
    "        print(f\"\\nüìä Data Summary:\")\n",
    "        print(f\"Total posts: {len(politics_posts)}\")\n",
    "        \n",
    "        scores = [post['score'] for post in politics_posts]\n",
    "        if scores:\n",
    "            print(f\"Average score: {sum(scores) / len(scores):.1f}\")\n",
    "            print(f\"Max score: {max(scores)}\")\n",
    "        \n",
    "        authors = [post['author'] for post in politics_posts if post['author'] != '[deleted]']\n",
    "        unique_authors = len(set(authors))\n",
    "        print(f\"Unique authors: {unique_authors}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Failed to collect politics posts\")\n",
    "    \n",
    "    # Method 2: Try other political subreddits\n",
    "    print(\"\\nüìç Method 2: Trying other political subreddits...\")\n",
    "    other_subs = ['PoliticalDiscussion', 'Ask_Politics', 'neutralpolitics']\n",
    "    all_posts = politics_posts.copy() if politics_posts else []\n",
    "    \n",
    "    for sub in other_subs:\n",
    "        print(f\"Trying r/{sub}...\")\n",
    "        try:\n",
    "            sub_posts = collect_reddit_posts_simple(sub, None, limit=100)\n",
    "            if sub_posts:\n",
    "                all_posts.extend(sub_posts)\n",
    "                print(f\"‚úÖ Added {len(sub_posts)} posts from r/{sub}\")\n",
    "            time.sleep(2)  # Rate limiting\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed r/{sub}: {e}\")\n",
    "    \n",
    "    # Final results\n",
    "    if all_posts:\n",
    "        print(f\"\\nüéâ FINAL TOTAL: {len(all_posts)} posts collected!\")\n",
    "        \n",
    "        # Remove duplicates manually\n",
    "        seen_ids = set()\n",
    "        unique_posts = []\n",
    "        for post in all_posts:\n",
    "            if post['id'] not in seen_ids:\n",
    "                unique_posts.append(post)\n",
    "                seen_ids.add(post['id'])\n",
    "        \n",
    "        print(f\"After removing duplicates: {len(unique_posts)} unique posts\")\n",
    "        \n",
    "        # Save final dataset (JSON only)\n",
    "        final_json = f\"reddit_political_data_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        \n",
    "        save_to_json(unique_posts, final_json)\n",
    "        \n",
    "        print(\"\\nüéØ Data collection complete!\")\n",
    "        print(f\"JSON file: {final_json}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ùå No data collected. Possible issues:\")\n",
    "        print(\"- Reddit API credentials invalid\")\n",
    "        print(\"- Network connectivity problems\")\n",
    "        print(\"- Rate limiting (try again later)\")\n",
    "        print(\"- Subreddit access restrictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93b78ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Reddit Data Collection - Single JSON Output\n",
      "============================================================\n",
      "üîå Testing Reddit connection...\n",
      "‚úÖ Connected! Test subreddit: python\n",
      "\n",
      "üéØ Starting comprehensive data collection...\n",
      "\n",
      "üìç Processing r/politics...\n",
      "üîç Method: SEARCH from r/politics\n",
      "   Query: 'politics'\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "   Processed 125 posts...\n",
      "   Processed 150 posts...\n",
      "   Processed 175 posts...\n",
      "   Processed 200 posts...\n",
      "‚úÖ Collected 200 posts using search\n",
      "   Added 200 posts from search\n",
      "üîç Method: HOT from r/politics\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "‚úÖ Collected 100 posts using hot\n",
      "   Added 100 posts from hot\n",
      "üîç Method: NEW from r/politics\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "‚úÖ Collected 100 posts using new\n",
      "   Added 100 posts from new\n",
      "\n",
      "üìç Processing r/PoliticalDiscussion...\n",
      "üîç Method: SEARCH from r/PoliticalDiscussion\n",
      "   Query: 'politics'\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "   Processed 125 posts...\n",
      "   Processed 150 posts...\n",
      "   Processed 175 posts...\n",
      "   Processed 200 posts...\n",
      "‚úÖ Collected 200 posts using search\n",
      "   Added 200 posts from search\n",
      "üîç Method: HOT from r/PoliticalDiscussion\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "‚úÖ Collected 100 posts using hot\n",
      "   Added 100 posts from hot\n",
      "üîç Method: NEW from r/PoliticalDiscussion\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "‚úÖ Collected 100 posts using new\n",
      "   Added 100 posts from new\n",
      "\n",
      "üìç Processing r/Ask_Politics...\n",
      "üîç Method: SEARCH from r/Ask_Politics\n",
      "   Query: 'politics'\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "   Processed 125 posts...\n",
      "   Processed 150 posts...\n",
      "   Processed 175 posts...\n",
      "   Processed 200 posts...\n",
      "‚úÖ Collected 200 posts using search\n",
      "   Added 200 posts from search\n",
      "üîç Method: HOT from r/Ask_Politics\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "‚úÖ Collected 100 posts using hot\n",
      "   Added 100 posts from hot\n",
      "üîç Method: NEW from r/Ask_Politics\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "‚úÖ Collected 100 posts using new\n",
      "   Added 100 posts from new\n",
      "\n",
      "üìç Processing r/neutralpolitics...\n",
      "üîç Method: SEARCH from r/neutralpolitics\n",
      "   Query: 'politics'\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "   Processed 125 posts...\n",
      "   Processed 150 posts...\n",
      "   Processed 175 posts...\n",
      "   Processed 200 posts...\n",
      "‚úÖ Collected 200 posts using search\n",
      "   Added 200 posts from search\n",
      "üîç Method: HOT from r/neutralpolitics\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "‚úÖ Collected 100 posts using hot\n",
      "   Added 100 posts from hot\n",
      "üîç Method: NEW from r/neutralpolitics\n",
      "   Processed 25 posts...\n",
      "   Processed 50 posts...\n",
      "   Processed 75 posts...\n",
      "   Processed 100 posts...\n",
      "‚úÖ Collected 100 posts using new\n",
      "   Added 100 posts from new\n",
      "\n",
      "üìä Processing 1600 total posts...\n",
      "After removing duplicates: 1219 unique posts\n",
      "Posts with text content: 868\n",
      "Posts without text content: 351\n",
      "‚úÖ All data saved to: reddit_political_data_complete_20250821_181052.json\n",
      "   Total posts: 1219\n",
      "\n",
      "üìã Sample of 5 posts:\n",
      "================================================================================\n",
      "\n",
      "Post 1:\n",
      "  ID: 1lpwwbr\n",
      "  Title: DOJ Opens Door To Stripping Citizenship Over Politics...\n",
      "  Author: marji80\n",
      "  Subreddit: r/politics\n",
      "  Score: 33601 | Comments: 3300\n",
      "  Method: search\n",
      "  Is Self Post: False\n",
      "  Text: Link post: https://talkingpointsmemo.com/news/doj-opens-door-to-stripping-citizenship-over-politics\n",
      "  Created: 2025-07-02 19:42:01\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Post 2:\n",
      "  ID: 1mdyoyp\n",
      "  Title: Everybody Hates Trump Now | Six months after Trump bragged about a ‚Äúhistoric‚Äù realignment, voters fr...\n",
      "  Author: Aggravating_Money992\n",
      "  Subreddit: r/politics\n",
      "  Score: 17367 | Comments: 1238\n",
      "  Method: search\n",
      "  Is Self Post: False\n",
      "  Text: Link post: https://newrepublic.com/article/198624/everybody-hates-trump-approval-rating-polls\n",
      "  Created: 2025-07-31 16:41:20\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Post 3:\n",
      "  ID: 1jfqvem\n",
      "  Title: Tim Walz: Trump Will Start Arresting Political Opponents...\n",
      "  Author: thedailybeast\n",
      "  Subreddit: r/politics\n",
      "  Score: 58941 | Comments: 4112\n",
      "  Method: search\n",
      "  Is Self Post: False\n",
      "  Text: Link post: https://www.thedailybeast.com/tim-walz-trump-will-start-arresting-political-opponents/\n",
      "  Created: 2025-03-20 20:30:01\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Post 4:\n",
      "  ID: 1m323ju\n",
      "  Title: Politics, not Performance Killed ‚ÄòThe Late Show‚Äô with Stephen Colbert...\n",
      "  Author: AlexandrTheTolerable\n",
      "  Subreddit: r/politics\n",
      "  Score: 12306 | Comments: 845\n",
      "  Method: search\n",
      "  Is Self Post: False\n",
      "  Text: Link post: https://www.rollingstone.com/tv-movies/tv-movie-features/late-show-with-stephen-colbert-ending-analysis-1235388587/\n",
      "  Created: 2025-07-18 18:47:27\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Post 5:\n",
      "  ID: 1k4pnor\n",
      "  Title: Musk wants to leave politics because he‚Äôs tired of ‚Äòattacks‚Äô from the left: report...\n",
      "  Author: BreakfastTop6899\n",
      "  Subreddit: r/politics\n",
      "  Score: 13091 | Comments: 1823\n",
      "  Method: search\n",
      "  Is Self Post: False\n",
      "  Text: Link post: https://www.independent.co.uk/news/world/americas/us-politics/elon-musk-donald-trump-doge-b2736753.html\n",
      "  Created: 2025-04-22 03:17:46\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìà Final Statistics:\n",
      "Total unique posts: 1219\n",
      "Posts with actual text: 868\n",
      "Subreddits covered: 4\n",
      "Methods used: {'new', 'hot', 'search'}\n",
      "Average text length: 1136 characters\n",
      "Longest text: 16093 characters\n",
      "\n",
      "üéâ Complete dataset saved to: reddit_political_data_complete_20250821_181052.json\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"xeV3sLrkl4xGyM-ScCBkSA\",\n",
    "    client_secret=\"sGrJQoAWabMp3hVcAXTxocV3gkCXoQ\",\n",
    "    user_agent=\"HeadlineHive\"\n",
    ")\n",
    "\n",
    "def collect_reddit_posts(subreddit_name, search_query=None, limit=500, method=\"search\"):\n",
    "    \"\"\"\n",
    "    Collect Reddit data with proper text extraction\n",
    "    \"\"\"\n",
    "    print(f\"üîç Method: {method.upper()} from r/{subreddit_name}\")\n",
    "    if search_query:\n",
    "        print(f\"   Query: '{search_query}'\")\n",
    "    \n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "    \n",
    "    try:\n",
    "        # Choose method\n",
    "        if method == \"search\" and search_query:\n",
    "            submissions = subreddit.search(search_query, limit=limit, time_filter='all')\n",
    "        elif method == \"hot\":\n",
    "            submissions = subreddit.hot(limit=limit)\n",
    "        elif method == \"new\":\n",
    "            submissions = subreddit.new(limit=limit)\n",
    "        elif method == \"top\":\n",
    "            submissions = subreddit.top(limit=limit, time_filter='month')\n",
    "        else:\n",
    "            print(f\"Unknown method: {method}\")\n",
    "            return []\n",
    "        \n",
    "        for i, submission in enumerate(submissions):\n",
    "            try:\n",
    "                # Extract text content properly\n",
    "                text_content = \"\"\n",
    "                \n",
    "                # For self posts (text posts)\n",
    "                if submission.is_self and submission.selftext:\n",
    "                    text_content = submission.selftext.strip()\n",
    "                \n",
    "                # For link posts, we might want the URL or title as content\n",
    "                elif not submission.is_self:\n",
    "                    text_content = f\"Link post: {submission.url}\"\n",
    "                \n",
    "                # Sometimes selftext exists but is empty/whitespace\n",
    "                if not text_content and hasattr(submission, 'selftext'):\n",
    "                    if submission.selftext and submission.selftext.strip():\n",
    "                        text_content = submission.selftext.strip()\n",
    "                \n",
    "                # If still no content, use title as fallback\n",
    "                if not text_content:\n",
    "                    text_content = f\"[Title only] {submission.title}\"\n",
    "                \n",
    "                post_data = {\n",
    "                    'id': submission.id,\n",
    "                    'title': submission.title if submission.title else \"No title\",\n",
    "                    'text': text_content,\n",
    "                    'author': str(submission.author) if submission.author else '[deleted]',\n",
    "                    'subreddit': submission.subreddit.display_name,\n",
    "                    'score': submission.score if submission.score else 0,\n",
    "                    'upvote_ratio': getattr(submission, 'upvote_ratio', 0),\n",
    "                    'comments': submission.num_comments if submission.num_comments else 0,\n",
    "                    'url': submission.url if submission.url else \"\",\n",
    "                    'is_self': submission.is_self,\n",
    "                    'created': datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'permalink': f\"https://reddit.com{submission.permalink}\",\n",
    "                    'method_collected': method,\n",
    "                    'flair': submission.link_flair_text if hasattr(submission, 'link_flair_text') else None,\n",
    "                    'nsfw': submission.over_18 if hasattr(submission, 'over_18') else False\n",
    "                }\n",
    "                \n",
    "                posts.append(post_data)\n",
    "                \n",
    "                if (i + 1) % 25 == 0:\n",
    "                    print(f\"   Processed {i + 1} posts...\")\n",
    "                    time.sleep(0.3)  # Rate limiting\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   Error processing post {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"‚úÖ Collected {len(posts)} posts using {method}\")\n",
    "        return posts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with {method} method: {e}\")\n",
    "        return []\n",
    "\n",
    "def remove_duplicates(all_posts):\n",
    "    \"\"\"Remove duplicate posts based on ID\"\"\"\n",
    "    seen_ids = set()\n",
    "    unique_posts = []\n",
    "    \n",
    "    for post in all_posts:\n",
    "        if post['id'] not in seen_ids:\n",
    "            unique_posts.append(post)\n",
    "            seen_ids.add(post['id'])\n",
    "    \n",
    "    return unique_posts\n",
    "\n",
    "def save_single_json(posts, filename):\n",
    "    \"\"\"Save all posts to a single JSON file\"\"\"\n",
    "    if not posts:\n",
    "        print(\"‚ùå No posts to save!\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Create comprehensive data structure\n",
    "        output_data = {\n",
    "            \"metadata\": {\n",
    "                \"total_posts\": len(posts),\n",
    "                \"collection_date\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"methods_used\": list(set([post.get('method_collected', 'unknown') for post in posts])),\n",
    "                \"subreddits\": list(set([post['subreddit'] for post in posts])),\n",
    "                \"date_range\": {\n",
    "                    \"earliest\": min([post['created'] for post in posts]),\n",
    "                    \"latest\": max([post['created'] for post in posts])\n",
    "                }\n",
    "            },\n",
    "            \"posts\": posts\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"‚úÖ All data saved to: {filename}\")\n",
    "        print(f\"   Total posts: {len(posts)}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving JSON: {e}\")\n",
    "        return False\n",
    "\n",
    "def display_sample_with_text(posts, num_samples=3):\n",
    "    \"\"\"Display sample posts showing text content\"\"\"\n",
    "    if not posts:\n",
    "        print(\"No posts to display\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüìã Sample of {min(num_samples, len(posts))} posts:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, post in enumerate(posts[:num_samples]):\n",
    "        print(f\"\\nPost {i+1}:\")\n",
    "        print(f\"  ID: {post['id']}\")\n",
    "        print(f\"  Title: {post['title'][:100]}...\")\n",
    "        print(f\"  Author: {post['author']}\")\n",
    "        print(f\"  Subreddit: r/{post['subreddit']}\")\n",
    "        print(f\"  Score: {post['score']} | Comments: {post['comments']}\")\n",
    "        print(f\"  Method: {post['method_collected']}\")\n",
    "        print(f\"  Is Self Post: {post['is_self']}\")\n",
    "        print(f\"  Text Preview: {post['text'][:200]}...\" if len(post['text']) > 200 else f\"  Text: {post['text']}\")\n",
    "        print(f\"  Created: {post['created']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Main execution - Single JSON output with both methods\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Reddit Data Collection - Single JSON Output\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test Reddit connection\n",
    "    try:\n",
    "        print(\"üîå Testing Reddit connection...\")\n",
    "        test_sub = reddit.subreddit('python')\n",
    "        print(f\"‚úÖ Connected! Test subreddit: {test_sub.display_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Reddit connection failed: {e}\")\n",
    "        print(\"Check your Reddit API credentials!\")\n",
    "        exit()\n",
    "    \n",
    "    # Collect from multiple sources using both methods\n",
    "    all_posts = []\n",
    "    \n",
    "    # Configuration\n",
    "    subreddits_to_check = ['politics', 'PoliticalDiscussion', 'Ask_Politics', 'neutralpolitics']\n",
    "    methods_to_try = ['search', 'hot', 'new']\n",
    "    \n",
    "    print(\"\\nüéØ Starting comprehensive data collection...\")\n",
    "    \n",
    "    # Method 1: Search for politics-related content\n",
    "    for subreddit in subreddits_to_check:\n",
    "        print(f\"\\nüìç Processing r/{subreddit}...\")\n",
    "        \n",
    "        # Try search method first\n",
    "        if 'search' in methods_to_try:\n",
    "            search_posts = collect_reddit_posts(subreddit, 'politics', limit=200, method=\"search\")\n",
    "            if search_posts:\n",
    "                all_posts.extend(search_posts)\n",
    "                print(f\"   Added {len(search_posts)} posts from search\")\n",
    "        \n",
    "        # Try hot posts method\n",
    "        if 'hot' in methods_to_try:\n",
    "            time.sleep(1)  # Rate limiting\n",
    "            hot_posts = collect_reddit_posts(subreddit, None, limit=100, method=\"hot\")\n",
    "            if hot_posts:\n",
    "                all_posts.extend(hot_posts)\n",
    "                print(f\"   Added {len(hot_posts)} posts from hot\")\n",
    "        \n",
    "        # Try new posts method\n",
    "        if 'new' in methods_to_try:\n",
    "            time.sleep(1)  # Rate limiting\n",
    "            new_posts = collect_reddit_posts(subreddit, None, limit=100, method=\"new\")\n",
    "            if new_posts:\n",
    "                all_posts.extend(new_posts)\n",
    "                print(f\"   Added {len(new_posts)} posts from new\")\n",
    "        \n",
    "        time.sleep(2)  # Rate limiting between subreddits\n",
    "    \n",
    "    # Process and save results\n",
    "    if all_posts:\n",
    "        print(f\"\\nüìä Processing {len(all_posts)} total posts...\")\n",
    "        \n",
    "        # Remove duplicates\n",
    "        unique_posts = remove_duplicates(all_posts)\n",
    "        print(f\"After removing duplicates: {len(unique_posts)} unique posts\")\n",
    "        \n",
    "        # Filter out posts with no meaningful text content if desired\n",
    "        posts_with_text = []\n",
    "        posts_without_text = []\n",
    "        \n",
    "        for post in unique_posts:\n",
    "            if post['text'] and not post['text'].startswith('[Title only]') and not post['text'].startswith('Link post:'):\n",
    "                posts_with_text.append(post)\n",
    "            else:\n",
    "                posts_without_text.append(post)\n",
    "        \n",
    "        print(f\"Posts with text content: {len(posts_with_text)}\")\n",
    "        print(f\"Posts without text content: {len(posts_without_text)}\")\n",
    "        \n",
    "        # Save single JSON file with all data\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        final_json = f\"reddit_political_data_complete_{timestamp}.json\"\n",
    "        \n",
    "        save_single_json(unique_posts, final_json)\n",
    "        \n",
    "        # Display sample with text content\n",
    "        display_sample_with_text(unique_posts, 5)\n",
    "        \n",
    "        # Show statistics\n",
    "        print(f\"\\nüìà Final Statistics:\")\n",
    "        print(f\"Total unique posts: {len(unique_posts)}\")\n",
    "        print(f\"Posts with actual text: {len(posts_with_text)}\")\n",
    "        print(f\"Subreddits covered: {len(set([post['subreddit'] for post in unique_posts]))}\")\n",
    "        print(f\"Methods used: {set([post['method_collected'] for post in unique_posts])}\")\n",
    "        \n",
    "        # Show text content statistics\n",
    "        text_lengths = [len(post['text']) for post in posts_with_text]\n",
    "        if text_lengths:\n",
    "            print(f\"Average text length: {sum(text_lengths) / len(text_lengths):.0f} characters\")\n",
    "            print(f\"Longest text: {max(text_lengths)} characters\")\n",
    "        \n",
    "        print(f\"\\nüéâ Complete dataset saved to: {final_json}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ùå No data collected. Possible issues:\")\n",
    "        print(\"- Reddit API credentials invalid\")\n",
    "        print(\"- Network connectivity problems\") \n",
    "        print(\"- Rate limiting (try again later)\")\n",
    "        print(\"- Subreddit access restrictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e3a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
